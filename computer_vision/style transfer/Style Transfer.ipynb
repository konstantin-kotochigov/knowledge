{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The task is to acquire a new image that has content of the image X and the style of referral image Y.\n",
    "\n",
    "\n",
    "- **Content** is a high-level semantic content of an image (what's on the picture)\n",
    "\n",
    "\n",
    "- **Style** is low-level texture of an image (how is it drawn)\n",
    "\n",
    "<img src=\"img/setting.png\" width=500>\n",
    "\n",
    "Neural Style Transfer (NPS) - subclass of ST methods that utilize deep networks to accomplish this\n",
    "\n",
    "\n",
    "- Texture syntesis - generation of textures. Naturally it's a part of style transfer task. \n",
    "\n",
    "\n",
    "- Image analogy - we want to get image A' based on image A so that A relates to A' the same way that B relates to B'\n",
    "\n",
    "\n",
    "- Photorealistic textures - \n",
    "\n",
    "\n",
    "- Non-photorealistic tetures -\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Good reference:\n",
    "- https://arxiv.org/pdf/1705.04058.pdf\n",
    "\n",
    "\n",
    "According to the taxonomy, methods are grouped into two classes:\n",
    "- online - target image is being iteratively optimized from the original one\n",
    "- offline - target image is being generated from scratch\n",
    "\n",
    "<img src=\"img/taxonomy.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Algorithm of Artistic Style\n",
    "\n",
    "[[arxiv]](https://arxiv.org/pdf/1508.06576.pdf)\n",
    "\n",
    "This is a central work released in 2015 (Max Plank University) by Gatys et al that defined the new standard for a neural transfer architecture.\n",
    "\n",
    "There is one backbone network (for example VGG)\n",
    "\n",
    "\n",
    "С точки зрения сети - есть 3 входа. Один - оптимизируемый, наша целевая картинка (исходного размера). И есть 2 константы - картинка, содержание которой мы хотим скопировать (content), и картинка, стиль которой мы хотим скопировать (style).\n",
    "\n",
    "<img src=\"img/architecture.png\" width=500>\n",
    "\n",
    "\n",
    "We separatly define Loss in content and loss in style:\n",
    "\n",
    "### How we define content?\n",
    "It's just a feature map from a layer that is close to the output\n",
    "* Таким макаром можно семантические синонимы искать среди картинок\n",
    "\n",
    "### How we define Loss(content)\n",
    "Lcontent is a per pixel sum of squared error between pair of feature maps\n",
    "\n",
    "Note that here i - feature, j - pixel, l - layer.\n",
    "\n",
    "### How we define style\n",
    "We define style as a covariance matrix between features from some intermediate layer (or several intermediate layers).\n",
    "\n",
    "Those matrices are also called Gram matrices since they can be represented by a scalar product between all pairs of features. If we flatten each 2D feature map to a 1D vector, correlation becomes a scalar product between pair of vectors. \n",
    "\n",
    "#### How we define Loss(style)\n",
    "Lstyle - sum of squared errors when comparing two Gram matrices - of original and generated image:\n",
    "\n",
    "where G и A - Gram matrice of original / A - of target (i, j - features, k - pixel, l - layer)\n",
    "\n",
    "Lstyle is usually summed over multiple layers:\n",
    "\n",
    "\n",
    "Почему имеем право сравнивать матрицы ковариаций от разных картинок?\n",
    "Since the network is the same for original and reference images, the order of channels is determined. So, if images have the same style, the correlations of features must be high.\n",
    "Потому что сеть одна и та же. Поэтому порядок каналов одинаковый и каждый канал отвечает за один и тот же паттерн на обоих картинках. Поэтому можем сравнивать.\n",
    "\n",
    "Почему матрица ковариаций описывает стилистику?\n",
    "Что может указывать на наличие уникального стиля картинки\n",
    "повышенная встречаемость какого-то паттерна (например, контрастные линии), на это указывает высокое значение диагонального элемента матрицы Грама\n",
    "частая одновременная встречаемость паттернов (например, горизонтальные красные линии плюс диагонаьные желтые линии)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else can we do to add more accuracy to the syntesized image?\n",
    "\n",
    "## Laplacian-steered neural style transfer\n",
    "\n",
    "[[arxiv]](https://arxiv.org/pdf/1707.01253.pdf) (2017)\n",
    "\n",
    "-----\n",
    "\n",
    "Laplacian is a sum of second order derivatives.\n",
    "$$L = \\sum_i \\frac{\\partial^2 f}{\\partial x_i^2} $$\n",
    "\n",
    "It measures the level of curvature at the point.\n",
    "\n",
    "For example for a 2D gaussian the laplacian will look like this:\n",
    "<img src=\"img/laplacian.png\" width=500>\n",
    "\n",
    "That is, \n",
    "- the lowest curvature will be at the center of the Gaussian - the derivative there is almost linear, slowly starting to decrease. \n",
    "- the highest curvature is at the foundation of the Gaussian.\n",
    "\n",
    "------\n",
    "\n",
    "In Computer Vision laplacians are usually used as edge detectors. Examples of discrete laplacian filters\n",
    "\n",
    "<img src=\"img/laplacian_filter_examples.png\" width=200>\n",
    "\n",
    "Li et al added squared error between laplacians of original and generated image as additional loss. \n",
    "\n",
    "$$L_{Lap} =  \\sum \\big( L(x) - L(x_c) \\big) ^2$$\n",
    "\n",
    "where x_c - content image, x - currently optimized image\n",
    "\n",
    "Thus they made the model retain more edges from the original image.\n",
    "\n",
    "So the total loss becomes\n",
    "\n",
    "$$L_{total} = \\alpha L_{content} + \\beta L_{style} + \\gamma L_{Lap} $$\n",
    "\n",
    "Also they proposed to use laplacians from different resolutions (similar to style matrices)\n",
    "\n",
    "$$L_{total} = \\alpha L_{content} + \\beta L_{style} + \\sum_k \\gamma_k L^k_{Lap} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T04:02:08.239981Z",
     "start_time": "2021-01-11T04:02:08.236181Z"
    }
   },
   "source": [
    "## Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses\n",
    "\n",
    "\n",
    "[[arxiv]](https://arxiv.org/abs/1701.08893)(2017), Virginia university\n",
    "\n",
    "The authors investingated Gatys approach and ran into several problems.\n",
    "\n",
    "<img src=\"img/gatys_drawbacks.png\" width = 500>\n",
    "\n",
    "Among them:\n",
    "- unstable histogram (left)\n",
    "- ghosting artifacts (right)\n",
    "\n",
    "In order to achieve more stable results they proposed two augmentations to the model:\n",
    "- to add variance loss\n",
    "- to add histogram loss\n",
    "\n",
    "#### Total variation\n",
    "\n",
    "[Total variation](https://en.wikipedia.org/wiki/Total_variation_denoising) is a popular in CV measure that assesses the amount of noise on the picture.\n",
    "\n",
    "<img src=\"img/tv.svg\">\n",
    "\n",
    "It simply computes the total amount of \"jittering\" on the image. So the loss becomes\n",
    "\n",
    "$$L_{total} = L_{style} + L_{tv}$$\n",
    "\n",
    "#### Histogram loss\n",
    "To make $x$ look more like $x_s$ they added the comparison of the distributions (for each feature separately).\n",
    "\n",
    "Directly comparing two histograms is not a good idea, since histograms are coarse structures and change a litle => unsuitable for fitting the model.\n",
    "\n",
    "Instead:\n",
    "1. They remaped all features to the corresponsing original histograms\n",
    "2. Computed the amount of mapping that was done\n",
    "\n",
    "So they define the histogram loss as:\n",
    "$$L_{hist} = \\sum_i \\gamma_i || F_i - R(F_i) ||^2$$\n",
    "where \n",
    "- $F_i$ - i-th feature map, \n",
    "- $R(F_i)$ - remapped to original histogram feature map\n",
    "- $\\gamma_i$ - weight for each feature map\n",
    "\n",
    "\n",
    "So the total loss becomes\n",
    "\n",
    "$$L_{total} = L_{style} + L_{tv} + L_{histogram}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
