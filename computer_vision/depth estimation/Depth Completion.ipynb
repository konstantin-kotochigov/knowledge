{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth completion\n",
    "\n",
    "Depth cameras and LiDaRs are a good way to assess depth but they are inperfect. \n",
    "- They are often sparse (not every pixel can be estimated)\n",
    "- They leave lots of uncertainty zones (becasuse of refelctive surfaces ot too bright areas)\n",
    "\n",
    "So there is a reason is for a new task => to improve estimation by image using alogrithms. This is called Depth Completion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Depth completion of a single RGB image\n",
    "[[arxiv]](https://arxiv.org/abs/1803.09326), Princeton, 2018 \n",
    "\n",
    "One of the first works that introduced the term **Depth completion**.\n",
    "\n",
    "*The goal of our work is to complete the depth channel of\n",
    "an RGB-D image. Commodity-grade depth cameras often\n",
    "fail to sense depth for shiny, bright, transparent, and distant\n",
    "surfaces. To address this problem, we train a deep network\n",
    "that takes an RGB image as input and predicts dense surface normals and occlusion boundaries. Those predictions\n",
    "are then combined with raw depth observations provided by\n",
    "the RGB-D camera to solve for depths for all pixels, including those missing in the original observation*\n",
    "\n",
    "RGB-D - image that contains depth estimate along with stadard color channels.\n",
    "\n",
    "Modern day depth cameras (such as Microsoft Kinect) ahcieved progress in depth estimation, but still they work bad in cases of too <u>bright</u> or <u>glossy</u> surfaces or when objects are <u>too far away</u>.\n",
    "\n",
    "Related tasks:\n",
    "- depth estimation\n",
    "- depth inpainting (similar to image inpainting)\n",
    "- depth super-resolution\n",
    "- depth reconstruction from sparse signal\n",
    "\n",
    "#### System design\n",
    "1. They train two neural networks that predict:\n",
    "    1. Scene Normals\n",
    "    2. Scene Boundaries\n",
    "2. They solve optimization problem to corresctly blend two data inputs: \n",
    "    1. coarse depth from sensor \n",
    "    2. depth rendered from normals\n",
    "\n",
    "<img src=\"img/depth_completion1.png\" width=700>\n",
    "\n",
    "Networks are VGG16 fully convolutional encoder-decoder networks with shortcuts.\n",
    "\n",
    "#### Normal/Boundaries Training data\n",
    "They used a set of 3D models retireved by Matterport3D mutiview scans along with correponding images.\n",
    "\n",
    "So the dataset looks like\n",
    "- X = single camera shot\n",
    "    - depth channel\n",
    "    - RGB channel\n",
    "- Y = depth from reconstructed 3D model\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "Optimization function constists of three terms:\n",
    "\n",
    "<img src=\"img/depth_completion2.png\" width=300>\n",
    "\n",
    "Here we have:\n",
    "- $N$ - image\n",
    "- $T_{obs}$ - observed depth estimates\n",
    "- $E_D$ - new Depth values must correspond to our first coarse estimate\n",
    "- $E_N$ - \n",
    "- $E_S$ - smoothness regularizer: there sould not be too much depth fluctiation\n",
    "- $B(p)$ - reduces weight when pixel is near the boundary\n",
    "\n",
    "They solve system of linear equations using Cholesky decomposition.\n",
    "\n",
    "#### Results\n",
    "It turned out that it's better to predict normals without depth channel (from RGB only) and then optimize together with coarse signal (see scheme above).\n",
    "\n",
    "- X = single camera shot\n",
    "    - RGB channel\n",
    "- Y = depth from reconstructed 3D model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Measure\n",
    "\n",
    "Delta measure rate of pixels on image that have correct depth estimation\n",
    "$$\\delta_t = max \\bigg( \\frac{D(p)} {D_0(p)},\\frac{D_0(p)} {D(p)} \\bigg) < t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indoor Depth Completion with Boundary Consistency and Self-Attention\n",
    "\n",
    "[[arxiv]](https://arxiv.org/pdf/1908.08344.pdf), Taiwan University, 2019\n",
    "\n",
    "Drawbacks of previous approach:\n",
    "1. Second step, optimization with Cholesky decomposition is slow\n",
    "2. Two-stage computation is not cool\n",
    "3. They rejected using neural net, because it tended to make too insatisfactory predictions for blank areas\n",
    "\n",
    "So the authors proposed:\n",
    "1. To continue using normals and boundaries as main depth descriptors\n",
    "2. To replace the 2-stage process with a sinlge end-to-end neural net\n",
    "3. To make **2 key improvemenets**:\n",
    "    - Add Self-Attention Network (blue dotted line)\n",
    "    - Add Boundary Consistency Network (green dotted line)\n",
    "\n",
    "*End-to-End Network - the one that can be run in on step. As opposed to multy stage compautation.\n",
    "\n",
    "<img src=\"img/depth_completion_selfattention.png\">\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "Self-attention implemented using popular in <u>image-inpainting</u> technique called <u>gated convolution</u> in order to improve network prediction on blank regions.\n",
    "\n",
    "**Motivation:** to let the network highlight the areas to which it should pay more attention.\n",
    "\n",
    "<img src=\"img/gated_convolution.jpg\" width=700>\n",
    "\n",
    "### Boundary Consistency\n",
    "\n",
    "\n",
    "#### Loss function\n",
    "$$L = L_{SA} âˆ’ \\lambda_S L_S + \\lambda_{BC} L_{BC} + \\lambda_N L_N + \\lambda_B L_B$$\n",
    "\n",
    "- $L_{SA}$ - error for depth (reconstruction loss)\n",
    "- $L_N$ - error for normals\n",
    "- $L_B$ - error for boundaries\n",
    "- $L_S$ - structural similarity index for depth (SSID)\n",
    "- $L_{BC}$ - boundary constistency loss\n",
    "\n",
    "\n",
    "[Structural Similarity Index](https://en.wikipedia.org/wiki/Structural_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lerning guided convolutional netwroks for epth Complemetion\n",
    "[[arxiv]](https://arxiv.org/abs/1908.01238)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "In order to process different areas on the image differently there are some recent methods:\n",
    "RefineNet decoder.\n",
    "SPADE layers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The convolution treats valid and invalid pixels equally.\n",
    "Partial Convolution\n",
    "\n",
    "The continuous analogue is Gated Convolution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
