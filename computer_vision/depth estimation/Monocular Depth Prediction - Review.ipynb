{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Monocular depth estimation - deduction of image depth based on a single image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disparity = shift of pixel between 2 images. It depends on point depth.\n",
    "\n",
    "<img src=\"depth_binocular.png\" width = 500>\n",
    "Note that here $x_l$ and $x_r$ are coordinates for left and right coordinate systems respectively. That's why we when we calculate disparity, we subtract $x_r$ and add $x_l$ to T.\n",
    "\n",
    "T is also called B - Baseline. Thus disparity will be:\n",
    "\n",
    "$$ Z = \\frac{Bf}{d} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:52:41.727879Z",
     "start_time": "2021-01-16T09:52:41.715556Z"
    }
   },
   "source": [
    "## Learning Depth from Single Monocular Images\n",
    "\n",
    "Saxenna, 2005, Stanford\n",
    "\n",
    "[[arxiv]](http://www.cs.cornell.edu/~asaxena/learningdepth/NIPS_LearningDepth.pdf)\n",
    "\n",
    "They modeled image depth with MRFs:\n",
    "- Y (depths) as a 2D field\n",
    "- X (local features) as a 2D field\n",
    "\n",
    "Each image is divided on patches. Each patch is described by a number of features (650 features).\n",
    "\n",
    "Feature vector was constructed manually\n",
    "- 17 filters\n",
    "- neighboring information (at 3 different scales)\n",
    "- for each pair of patches they estimate their differences in histograms (for each of 17 filters)\n",
    "\n",
    "Their first model (Gaussian) maximizes the probability:\n",
    "<img src=\"predeeplearning_mrf_1.png\" width=500>\n",
    "\n",
    "First term represents error of depth prediction. Second term regularizes big differences in neighboring patches.\n",
    "\n",
    "It's called Gaussian since model probability is represented by multivariate gaussian distribution.\n",
    "\n",
    "Their second model (Lapalcian) maximizes the probability:\n",
    "<img src=\"predeeplearning_mrf_2.png\" width=500>\n",
    "\n",
    "It's pretty much the same as Gaussian but here they use Laplacian distribution <img src = \"laplacian.svg\">\n",
    "\n",
    "The solutions to the problem were achieved by standard Likelihood maximization techniques.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "They collceted their own dataset of ~400 images using 3D-scanners.\n",
    "\n",
    "<img src=\"predeeplearning_results.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:31:54.601560Z",
     "start_time": "2021-01-16T09:31:54.593226Z"
    }
   },
   "source": [
    "## Depth Map Prediction from a Single Image using a Multi-Scale Deep Network\n",
    "\n",
    "Eigen, 2014, NYU\n",
    "\n",
    "[[arxiv]](https://arxiv.org/pdf/1406.2283.pdf)\n",
    "\n",
    "One of the first works on DU using Deep Learning.\n",
    "\n",
    "Network consists of two branches\n",
    "1. coarse estimator - regular cNN that regresses the depth\n",
    "2. fine etimator - another regular cNN but additionally it uses global information from coarse prediction of the first network\n",
    "\n",
    "<img src=\"eigen_architecture.png\" width=750>\n",
    "\n",
    "Also the authors enhanced standard MSE error $\\sum_i \\big( log y_i - log y^*_i \\big) ^ 2$ and added summand:\n",
    "\n",
    "$$\\sum \\big( log y_i - log y^*_i + \\frac{1}{n} \\sum_i (log y^*_i - log y_i) ^ 2 \\big) $$\n",
    "\n",
    "They motivated as large part of error can be attributed to wrong scale perception not. So they added a mean shift between prediction and ground truth so that model on rellative preedcitions.\n",
    "\n",
    "### Datsets\n",
    "\n",
    "#### NYUDepth\n",
    "Dataset with indoor images\n",
    "\n",
    "https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\n",
    "\n",
    "#### KITTI\n",
    "Dataset with images taken from autonomous vehicles with corresponding depths\n",
    "\n",
    "http://www.cvlibs.net/datasets/kitti/\n",
    "\n",
    "### Results\n",
    "\n",
    "Here are some examples for NYUDepth:\n",
    "1. image\n",
    "2. coarse prediction\n",
    "3. fine-grained prediction\n",
    "4. ground-truth\n",
    "\n",
    "<img src=\"dl_results.png\" width=700>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue\n",
    "\n",
    "[[arxiv]](https://arxiv.org/pdf/1603.04992.pdf), 2016, Adelaide\n",
    "\n",
    "The authors propose an (almost) unsupervised method that requires only a set of stereopairs or images with known movement.\n",
    "\n",
    "Getting a set of stereopaired images is easier than getting a set of labelled images.\n",
    "\n",
    "Scheme of the algorithm:\n",
    "1. Encoder network computes a depth map for the left image\n",
    "2. Right image is warped (transformed) taking into account the estimated depth map and known movement\n",
    "3. Model compares original and reconstruvted image to assess the reconstruction error\n",
    "\n",
    "<img src=\"unsupervised_scheme.png\" width=750>\n",
    "\n",
    "During the warping step\n",
    "\n",
    "$$I_2(x) := I_2(x + \\frac{fB}{d(x)}) $$\n",
    "\n",
    "each pixel on the right image is moved according to its estimated depth from first image (close pixels move more, far pixels move less).\n",
    "\n",
    "The encoder part of the trained network then considered a depth estimator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning of Depth and Ego-Motion from Video\n",
    "\n",
    "[arxiv](https://arxiv.org/abs/1704.07813), 2017, Google\n",
    "\n",
    "This approach generalizes the previous one. It implements the same idea but instead a stereo image pair as an input the model uses short video sequence (several consequtive frames).\n",
    "\n",
    "The movement of the camera is not fixed anymore, now it is modeled as an arbitrary transformation T(p).\n",
    "\n",
    "Ego-motion = visual odometry - determining the current location of a robot based on sequence of images from its viewpoint. Motion modeling is a well studied problem itself.\n",
    "\n",
    "The model consists of 2 branches:\n",
    "1. Depth cNN (encoder that calculates image depth)\n",
    "2. Pose cNN (decoder that estimates image transformation)\n",
    "\n",
    "The view transformation $T$ learnt by PoseCNN is then applied to source images $I_{t-1}, I_{t+1}$ using output of the first network D(p)\n",
    "\n",
    "<img src= \"google_architecture.png\" width=500>\n",
    "\n",
    "The model uses standard photmetric loss between 2 images - the source image $I_s$ and its reconstructed versions $I^r_1$,$I^r_1$:\n",
    "\n",
    "$L = \\sum_s \\sum_p | I_s(p) - I_r(p) |$, where \n",
    "\n",
    "- $p$ - pixel\n",
    "- $r$ - index of reference image (for example $t-1$, $t+1$)\n",
    "- $I_t$ - source image, \n",
    "- $I_s$ - reconstructed reference image.\n",
    "\n",
    "#### Network structure\n",
    "Both networks architecure is quite straightforward:\n",
    "\n",
    "<img src=\"google_dispnet.png\" width=750>\n",
    "\n",
    "#### How does the view transformation look like\n",
    "$$p_s := K \\cdot T_{t \\rightarrow s} \\cdot D_t(p_t) \\cdot K^{âˆ’1} \\cdot t$$\n",
    "\n",
    "where\n",
    "- $K$ = camera instrinsics matrix\n",
    "- $T_{t \\rightarrow s}$ = view transformation matrix\n",
    "- $D_t$ = predicted depth\n",
    "- $p_t$ = coordinates\n",
    "\n",
    "Since new coordinates after transformation are continous, they use bilinear approximation. Which is differentialble => can be used in a network fitting.\n",
    "\n",
    "#### What regulizers are used\n",
    "\n",
    "To smoothen the depth mask they add a regularization term as $L_1$ of gradients:\n",
    "\n",
    "$$L = \\sum_s \\sum_p | I_s(p) - I_r(p) | +  \\lambda_s \\cdot L_{smooth}$$\n",
    "\n",
    "Model performs poorly when there is motion on the scene. To add robustness to the model they introduce a third network that predicts an explainability mask. It highlights the most difiicult parts of the scene that contain motion / occlusions or else. This mask $E(p)$ is then added as a weight to the view syntesis loss:\n",
    "\n",
    "$$L = \\sum_s \\sum_p E(p) \\cdot | I_s(p) - I_r(p) | + \\lambda_s \\cdot L_{smooth}$$\n",
    "\n",
    "To prevent model from always setting difficulty mask to zero thet had to add regulizer for the difficulty mask too:\n",
    "\n",
    "$$L = \\sum_s \\sum_p | I_s(p) - I_r(p) | +  \\lambda_s \\cdot L_{smooth} + \\lambda_e \\cdot \\sum L_{reg}(E_s^l)$$\n",
    "\n",
    "Here are some examples of explainability masks\n",
    "\n",
    "<img src = \"google_exp.png\" width=700>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Monocular Depth Estimation with Left-Right Consistency\n",
    "\n",
    "[[arxiv]](https://arxiv.org/abs/1609.03677), 2017, UCL\n",
    "\n",
    "The authors emphasize the importance of tackling moving objects in the scene. That's why they "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
