{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In earlier models context was considered textual only. Multimodal presume context can be of many modalities - for example image, audio, video etc\n",
    "\n",
    "2 approaches:\n",
    "- model retains its LLM architecture but adds cross-image attention (like in LLAMA 3.2)\n",
    "- whole model is retrained on image-text pairs (like in mini-gpt 4)\n",
    "\n",
    "- Encoder = maps image to a set of embeddings (VIT), 1 patch = 1 embedding\n",
    "- Adapter = maps image embeddings into text token space, 1 patch = 1 point in semantic space\n",
    "\n",
    "Training strategies:\n",
    "- __internVL:__ encoder & LLM are freezed, only adapter is being trained\n",
    "- __cogLMV2:__ every 3 components are trained\n",
    "- __MM1.5:__ every 3 components are trained\n",
    "\n",
    "Data types for training:\n",
    "- image-text pairs (annotations)<br>trains model to properly annotate an image\n",
    "- image-in-text (web docs)\n",
    "- text only<br>\n",
    "  to prevent diverting from text mode too much\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAVA (2023)\n",
    "\n",
    "One of the first open-source multumodal LLM models\n",
    "\n",
    "<img src=\"img/\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
