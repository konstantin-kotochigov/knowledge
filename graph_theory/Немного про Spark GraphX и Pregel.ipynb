{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как хранятся графы?**\n",
    "\n",
    "В виде матрицы смежности A хранить граф глупо, так как данные почти всегда разреженные. Поэтому всегда хранится в виде sparse массива - только ненулевые элементы. Это то же самое, что хранить просто в виде таблицы:\n",
    "\n",
    "|Source Vertex|Destination Vertex|\n",
    "|---|---|\n",
    "|1|2|\n",
    "|1|3|\n",
    "|2|4|\n",
    "|3|5|\n",
    "|4|6|\n",
    "\n",
    "Деревья (специальный вид графов) так же хранятся.\n",
    "\n",
    "У реальных графов всегда есть атрибуты вершины и атрибуты ребер\n",
    "\n",
    "|Source Vertex|Dest Vertex|Edge Properties|\n",
    "|---|---|---|\n",
    "|1|2|(relation='is a friend of', recency=5)|\n",
    "|1|3|(relation='follows', recency=10)|\n",
    "|3|5|(relation='foolows', recency=2)|\n",
    "\n",
    "\n",
    "Кроме того нужна структура для хранения атрибутов вершин. Для этого ведется отдельная таблица:\n",
    "\n",
    "|Vertex|Vertex Properties|\n",
    "|---|---|\n",
    "|1|(name='Alex', surname='Pompeo')|\n",
    "|2|(name='John', surname='Bombaleilo')|\n",
    "|3|(name='Katrin', surname='Zeta')|\n",
    "\n",
    "## Spark GraphX\n",
    "\n",
    "#### Как подключить\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "Специальные структуры для работы с графами не требуются, достаточно обычного RDD.\n",
    "\n",
    "#### Примеры задач, которые можно решать анализируя корпус текстов Wikipedia:\n",
    "- *PageRank* - наиболее значимые страницы\n",
    "- *Topic Definition* - выделение тематик\n",
    "- *User Communities* - выявление сообществ\n",
    "- *Community Topics* - определение тематик сообществ\n",
    "<img src=\"img/graphx_tasks.png\" width=500>\n",
    "\n",
    "#### Иерархия базовых классов в GraphX\n",
    "\n",
    "val graph = Graph(VertexRDD, EdgeRDD)\n",
    "\n",
    "*EdgeRDD* = RDD[Edge] с возможной перегруппировкой по партициям и 3 встроенными функциями\n",
    "\n",
    "*VertexRDD* = RDD[(VertexID,VD)] с ограничением на уникальность VertexID и несколькими встроенными функциями\n",
    "\n",
    "|method|desc|\n",
    "|:--|:--|\n",
    "|edges()|RDD с ребрами|\n",
    "|vertices()|RDD с вершинами|\n",
    "|numEdges()|кол-во ребер|\n",
    "|numVertices()|кол-во вершин|\n",
    "|||\n",
    "|mapVertices(f)|обработать вершины|\n",
    "|mapEdges(f)| обработать ребра|\n",
    "|mapTriplets(f)| обработать обогащенные ребра|\n",
    "|||\n",
    "|aggregateMessages| считает показатель для каждой связи и агрегирует его по dest ребра |\n",
    "|mapReducetriplets| то же самое, но в другой нотации |\n",
    "|collectNeighborIds| возвращает RDD со списком соседей для каждой вершины|\n",
    "|collectNeighbors| возвращает RDD со списком и атирбутами соседей для каждой вершинам|\n",
    "|||\n",
    "\n",
    "Класс EdgeTriplet - просто ребро со всеми атрибутами (соединенный Edge с Vertex)\n",
    "\n",
    "<img src=\"img/graphx_classes.png\">\n",
    "\n",
    "aggregateMessages(send_f,merge_f) - .\n",
    "\n",
    "\n",
    "#### Storage\n",
    "\n",
    "Как бить граф по нодам кластера?\n",
    "\n",
    "Можно выбрать стратегию партичионирования с помощью partitionBy().\n",
    "- 2D parition\n",
    "\n",
    "Ниже приведен пример для 2х партиций. Каждое ребро в своей партиции. Некоторые вершины дублируются.\n",
    "<img src=\"img/graphx_partition.png\" width=450>\n",
    "\n",
    "#### Какие есть готовые алгоритмы\n",
    "- In-Degrees\n",
    "- PageRank\n",
    "- Triangles\n",
    "- Connected Components\n",
    "\n",
    "#### Как генерируются графы в GraphX?\n",
    "- FromEdgeList\n",
    "- Lognormal\n",
    "\n",
    "#### Резюме\n",
    "Максмимально базовая библиотека, из готового нет почти ничего. Единственное преимущество - возможность распределения вычислений.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregel\n",
    "В 2010 году Google разработал Pregel - парадигму вычислений на графах. В некотором смысле она является альтернативой MapReduce. Позднее концепция перекочевала в Apache Giraf (который использовал Facebook) и собственно Apache Spark GraphX.\n",
    "\n",
    "**Идея:** вычисления выполняются в параллель для каждой вершины графа. Каждая такая итерация называется superstep. \n",
    "\n",
    "Выход итерации: обновленный граф (state) + **сообщения** с неким промежуточным результатом (messages), которые отсылаются в соседние вершины и затем там агрегируются.\n",
    "\n",
    "Схема представлена ниже:\n",
    "\n",
    "<img src=\"img/graphx_pregel.png\" width=500>\n",
    "\n",
    "#### Примеры\n",
    "1. Посчитать максимум по всем нодам\n",
    "    - отправляем свое текщее значение соседям\n",
    "    - если в полученных от соседей сосбщениях есть значение больше, то обновить свое и отправить его соседям еще раз\n",
    "    - если все значения меньше, делаем halt\n",
    "    <img src=\"img/graphx_max.png\" width=250>\n",
    "2. Посчитать средний возраст подписчиков (из примеров SparkX)\n",
    "    - функция map отправляет сообщение c 1 по направлению текущей связи\n",
    "    - функция reduce суммирует по каждой ноде все сообщения пришедшие в нее\n",
    "    - за одну итерацию посчитали\n",
    "3. PageRank\n",
    "    - то же самое, но в цикле\n",
    "\n",
    "\n",
    "плюс любые алгоритмы, связанные с переходом по графу (практически все).\n",
    "\n",
    "\n",
    "Основное отличие от MapReduce - там между партициями должен синхронизироваться результат (данные), а в Pregel пересылаются только сообщения, сами данные сохраняются по партициям.\n",
    "\n",
    "В GraphX pregel-сообщение - это просто итератор с двумя параметрами (dstId, attrs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
