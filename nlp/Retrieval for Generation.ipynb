{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Retrieval\n",
    "---\n",
    "Information Retrieval (of Text documents) is often associated with three tasks: \n",
    "- Search\n",
    "- Recommendations\n",
    "- Generation\n",
    "\n",
    "Search and Retrieval usually do not require any substantial post-processing of retrieved documents. Generation does require it (sumamrize, find answers etc), but in most cases rely on the original document => can use standard retrieval\n",
    "\n",
    "# Retrieval for Generation\n",
    "---\n",
    "\n",
    "First LLM models relied entirely on their own weights. Problem: the knowledge of first ChatGPT was 2 years behind the actual time. To overcome this limitation you had to continuosly retrain the whole model, which is unfeasible.\n",
    "\n",
    "There came RAG = Retrieval Augmented Generation. Idea: relevant fresh data is attached directly to the LLM context on-the-run (real time)\n",
    "\n",
    "Classic RAG system:\n",
    "1. Retriever component fetches candidate documents (focus on recall)<br>\n",
    "   - using query matching\n",
    "   - using embedding matching\n",
    "2. LLM uses them as a context\n",
    "\n",
    "<img src=\"img/rag1.png\" width=500>\n",
    "\n",
    "Important problem, not only in the domain of LLM generation, but in the IR generally = original query might be not sufficient for accurate retireval\n",
    "\n",
    "# User query misspecification\n",
    "---\n",
    "One of the main challenges in Information Retrieval (known before 1980s) is that users often cannot articulate their needs precisely because they don’t fully understand the problem. IR practitioneers of that time ([paper](https://arxiv.org/pdf/2503.00223)) formulated the following target - a good IR system must be context-dependent, personalized, able to maintain dialogue with user\n",
    "\n",
    "Examples of poor specification:\n",
    "\n",
    "- Ambiguous Queries / Polysemy<br>\n",
    "   ```jaguar, apple history```<br><br>\n",
    "- Synonymy / Different Vocabulary<br>\n",
    "   ```heart attack symptoms```<br><br>\n",
    "- Contextual or Temporal Mismatch<br>\n",
    "   ```president bush```<br><br>\n",
    "- Mismatch Due to Domain Jargon<br>\n",
    "   ```How do I fix my computer’s blue screen?```<br><br>\n",
    "- Overspecification (too narrow queries)<br>\n",
    "   ```best restaurants for vegan ramen in San Francisco open late```<br><br>\n",
    "- User Under-Specification (too wide queries)<br>\n",
    "   ```python tutorial```\n",
    "\n",
    "Main takeaway = Retrieval requires some kind of query preprocessing / adjustment. Classic sparse retrievers like TF-IDF / BM25 might not be enough\n",
    "\n",
    "# Sparse Retrieval\n",
    "---\n",
    "\"Sparse Retrieval\" is an old approach where documents are represetnted as vocabulary-size vector and the process lookups inverted index using the terms from the query. Resulting document chains are then merged, filtered and transfered for further ranking\n",
    "\n",
    "Matching algorithms\n",
    "- TF-IDF\n",
    "- BM25\n",
    "\n",
    "# Dense Retrieval\n",
    "---\n",
    "Dense Retrievers is a family of algorithms where queries and documents are mapped into the same latent vector-space by some sort of Encoder. Query/document proximity is then calculated by a cosine / dot-product similarity\n",
    "\n",
    "Dense Extraction can be implemented by either performing a \"full-scan\" (suitable for small indices) or \"approximate fetch\" (suitable for large indices but requires additional filtering of the result)\n",
    "\n",
    "# Hybrid Retrieval\n",
    "---\n",
    "Hybrid Retrievers use dense encoding as an intermediary representaion, but still use sparse retrieval from inverted index. They tend to take best from two worlds - semantic richness of Dense representation and efficiency of Sparse vectors. Often refered to as neural sparse retrieval.\n",
    "\n",
    "The output is a sparse vocabulary-sized vector of tokens that constitute the document/query, but instead of deterministic TFs weights we use more expressive, context-dependent weights\n",
    "\n",
    "Methods include:\n",
    "- DeepImpact\n",
    "- DeepCT\n",
    "- COIL\n",
    "- uniCOIL\n",
    "- TILDE\n",
    "- SPLADE / SPLADE v2\n",
    "\n",
    "# Approximate Retrieval\n",
    "---\n",
    "Retrieving documents from a large database, whether using sparse or dense keys, requires full-scan of this database, which is <u>unsatisfactory</u> for most production-level systems\n",
    "\n",
    "This means we need to finr a way to accelerate retrieval. Puting hardware accelreration (sharding, caching etc) aside, the best option is to store documents in a manner optimized for fast retireval using proximity-based queries\n",
    "\n",
    "Most dense retrievers are approximate (at least at the candidate generation phase). Approximate nearest neighbours alogorithms that are worth mentioning include:\n",
    "- k-D trees<br>\n",
    "- FAISS (Meta)<br>\n",
    "- Annoy (Spotify)<br>we build k decision trees, navigate to analyzed observation in $log(n)$ and neighbours from the neighbouring nodes\n",
    "- ScaNN (Google)<br>\n",
    "- HNWS (Yandex)<br>build a proximity graph, aggregate it several times to make it hierarchical and navigate to find neighbouring obervations\n",
    "\n",
    "See the notebook for more detailed explanation\n",
    "\n",
    "# Two-stage Retrieval\n",
    "---\n",
    "Most modern systems implement two-stage retrieval process:\n",
    "1. fast but unfiltered candidate generation<br>- sparse matching: tf-idf / bm25 weighted matching through inverted index<br>- approximate dense matching (for moderate size systems): approximate algorithms like FAISS, Annoy, HNSW\n",
    "2. slow but precise filtering & reranking\n",
    "\n",
    "\n",
    "# Retriever as a Model\n",
    "---\n",
    "Retrievers can be trainable\n",
    "\n",
    "Methods:\n",
    "- REALM\n",
    "- colBERT\n",
    "- DPR\n",
    "- RAG\n",
    "- DPR-CTL\n",
    "- S3\n",
    "\n",
    "# Rocchio (1971)\n",
    "---\n",
    "Rocchio algorithm defined the first personalized version of document Retrieval. What it does - it shifts query vector (from the document-query vector space) in the direction towards documents previously positively evaluated by this user\n",
    "\n",
    "# Retireval Augmentation\n",
    "---\n",
    "Query Augmentation = rewriting OR augmentation of user query to reflect all sides of user's intent\n",
    "Document Augmentation = enhancing documents stored in index\n",
    "\n",
    "Algorithms include:\n",
    "- Doc2Query\n",
    "- AxaRanker\n",
    "- DeepRetrieval\n",
    "- Search-R1\n",
    "- ConQRR\n",
    "\n",
    "## Zero-shot retrieval\n",
    "Out-of-the box pretrained Encoders do not know how to rank by relevance. They need to be fine-tuned on some relevance dataset. Usually contrastive losses are used. But labeling is expensive => there is a challenge of __zero-shot retrieval__ - fetching without previous training on human labeled data\n",
    "\n",
    "# PRF (2021)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2108.11044)<br>PRF = Pseudo-Relevance Feedback. It is the algorithm to enhance the original query with related / synonim terms. Synonymic terms = most frequent terms that appear in the list of documents returned on the first retrieval\n",
    "\n",
    "<img src=\"img/prf.png\" width=400>\n",
    "\n",
    "# REALM (2020)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2002.08909)<br>\n",
    "Retrieval‑Augmented Language Model Pre‑Training = one of the first implementations of Dense Retrieval for Generation where the Retriever model is trainable and is trainable together with Generator LLM\n",
    "\n",
    "Retriever is an early-linkage two-tower model\n",
    "\n",
    "Model architecture:\n",
    "- consists of \"Retriever\" and \"Reader\"\n",
    "- Retriever is a two-tower network with the same BERT model and [CLS] output\n",
    "- embedings are combined with a dot-product\n",
    "- for all retrieved candidates apply Reader model to generate answer\n",
    "- average answers from \n",
    "\n",
    "Training procedure\n",
    "1. pretraining: some text corpora is masked (MLM) => we get a labeled QA dataset for self-suprrevised training\n",
    "2. fine-tuning: trained on QA dataset\n",
    "\n",
    "<img src=\"img/realm.png\" width=750>\n",
    "\n",
    "# ColBERT (2020)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2004.12832)<br>\n",
    "ColBERT is a tower network of two Encoders (BERT) - one for query encoding and one for document encoding. Each tower outputs token embeddings, which in turn are combined using MaxSim pooling to get a scalar \"relevance\" score\n",
    "\n",
    "Model architecture:\n",
    "- each tower uses the same BERT model with full output\n",
    "- a projection linear layer is additionally attached to reduce the dimensionality of the outputs (748 -> 128)\n",
    "- we compare sets of output embeddings in a \"cross-attentional\" manner\n",
    "- MaxSim (Maximum Simlarity) = each embedding from query is multiplied to such embedding from document that gives maximum product\n",
    "- all individual scores are summed into a global \"relevance\" score\n",
    "\n",
    "Training process:\n",
    "- use constrastive learning - query is compared to one positive and one negative example\n",
    "- ranking loss evaluates how far these two scores are\n",
    "- all weights of the model are fine-tuned (embedding layer, BERT layers, linear projection)\n",
    "\n",
    "Inference:\n",
    "- for each document $D$ from a database compute the \"relevance\" score $s(Q,D)$\n",
    "- select top-K documents with highest score\n",
    "\n",
    "Acceleration (optional):\n",
    "- precompute outputs for all documents\n",
    "- store those outputs in an index\n",
    "\n",
    "<img src=\"img/colbert.png\" width=750>\n",
    "\n",
    "## MaxSim\n",
    "MaxSim is a \"crossattention-like\" method of aggregating scores\n",
    "\n",
    "Suppose query encoder retruned $Q = [q_1, q_2, ..., q_m]$ and document encoder returned $D = [d_1, d_2, ..., d_n]$\n",
    "\n",
    "Then for each query token we get the most relevant pair $\\text{score}(Q, D) = \\sum_{i=1}^{m} \\max_{j=1}^{n} \\langle q_i, d_j \\rangle$ \n",
    "\n",
    "\n",
    "\n",
    "# DPR (2020)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2004.04906)<br>DPR = Dense Passage Model is an implementation of Dense Retrieval offered by Meta. It uses two BERT models to encode query and documents and dot-product as similarity measure. Encoders are trained over contrastive loss (one positive and one negative example)\n",
    "\n",
    "Architecture:\n",
    "- two-tower model with two <u>different</u> BERT encoders (called Question encoder and Passage encoder) with a single [CLS] output\n",
    "- relevance score is a dot-product of towers output\n",
    "\n",
    "Training process:\n",
    "- use contrastive learning: we gonna compare a query $Q$ with one positive $D^+$ and one negative example $D^-$\n",
    "- compute predicted \"relevance\" score for all documents\n",
    "- normalize output scores to get a probability<br>probability of event \"$D^+$ is relevant to $Q$\"\n",
    "- use negative loglikelihood as a loss function<br>[why not triplet loss?]\n",
    "\n",
    "Index construction:\n",
    "- apply trained Passage Encoder to all documents (passages)\n",
    "- store output embeddings in some ANN index\n",
    "\n",
    "Inference\n",
    "- output embedding\n",
    "- retrieve top-K documents\n",
    "- optionaly send to further processing like \"Reader\" ot \"Reranker\" model\n",
    "\n",
    "\n",
    "\n",
    "# DPR-CTL (2020)\n",
    "---\n",
    "A neural retrieval method that is trained in self-supervised fashion - neighboring chunks of text are considered positive exmaples, random chunks - negative ones. It achieves comparable to fine-tuned alternatives performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RAG (2020)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2005.11401)<br>Retrieval Augmented Generation = the first implementation of the RAG approach when the term was coined. It uses DPR model for retrieval together with some generation model (BART in the original paper)\n",
    "\n",
    "<img src=\"img/rag.png\" width=600>\n",
    "\n",
    "# ExaRanker (2023)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2402.06334)<br>\n",
    "Before training the Retriever on some relevance dataset let's use a strong LLM (like Chatgpt) to generate a textual \"explanation\" for each example in this dataset (\"this document is relevant because ...\")\n",
    "\n",
    "During training phase make the Retriever model not only predict the correct label, but also reconstruct this explanation<br>This develops model's reasoning ability about relevance, avoids prediction hacking. Distance to ground-truth explanation is measured using standard text (sequence-to-sequence) loss\n",
    "\n",
    "<img src=\"img/exarank.png\" width=400>\n",
    "\n",
    "# CONQRR (2022)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2112.08558)<br>\n",
    "Retrieval for conversational (dialog) systems is more challenging since the query might be distributed along the previous conversation (\"What about his birthplace?\"). CONQRR summarizes all necessary information for the query to be effectively processed. It is retriever agnostic and relies only on query rewriting\n",
    "<img src=\"img/conqrr.png\" width=400>\n",
    "\n",
    "# Contextual Clues Sampling (2022)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2210.07093)<br>\n",
    "The authors suggest using some strong LLM (ChatGPT) to enhance original query by generating a list of related terms - they call them \"contextual clues\"<br>Not sure what exact prompt do they use \"Model, generate me related terms\"?\n",
    "\n",
    "Retriever model runs multiple fetches - one for each enhancement and extracts a list of documents which are next fused into one large list.\n",
    "\n",
    "Diversity is achieved by multiple generations. They are followed by deduplication - identic or similar \"clues\" are grouped into clusters<br>Precision is achieved by first ranking the clues and then retrieved documents according to their generation probabilities. Only top-K are used.\n",
    "\n",
    "<img src=\"img/contextual_clues.png\" width=400>\n",
    "\n",
    "# DeepRetrieval (2025)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2503.00223)<br>Enhances user query by rewriting it with a (reasoning) LLM model\n",
    "\n",
    "LLM is trainable and is updated using RL (PPO). Reward consists of two pieces: query consistency (how good new prompt is formatted) + Recall-based reward (how relevant are the documents we fetched)\n",
    "\n",
    "Requires some kind of pre-labeled dataset to be able to evaluate the relevance reward\n",
    "\n",
    "<img src=\"img/deep_retrieval.png\" width=400>\n",
    "\n",
    "# Search-R1 (2025)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2503.09516)<br>Treats retrieval as a multi-step iterative enhancement process. Model inserts API calls during reasoning and fetches new data. \n",
    "\n",
    "Retrieval here is a part of generator model. The fetch itself is not trainable, just an API call. Reasoning can be adjusted. \n",
    "\n",
    "The model is updated using DPO/GRPO. Reward is determined by the correctness of the final answer (ExactMatch). Requires some pre-labeled dataset\n",
    "\n",
    "# S3\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2505.14146)<br>\n",
    "S3 = Search, select, serve. When Generator is not trainable, focus on fine-tuning the Retriever model\n",
    "\n",
    "In s3  we train the Retrieval model using RL. \n",
    "\n",
    "A reward is an uplift compared to some baseline (RAG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GraphRAG (2024)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2404.16130)<br>\n",
    "\n",
    "In case of \"broad\" queries (that require some aggregation of knowledge) regular RAG tends to give too fragmented answers<br>Instead of doing an exhausting full-scan over all documents in a corpus, let's make the knowledge hierarchical and query it in a tree-like fasion. \n",
    "\n",
    "Example of a broad query<br>\"What are the main research themes and their interconnections in the latest COVID-19 scientific literature?\"\n",
    "\n",
    "Graph = named entities linked by their relatshionships. Communities = clusters of similar nodes. They might have different levels of aggregation (large communities consisting of smaller communities)\n",
    "\n",
    "__Algorithm__\n",
    "- Graph building\n",
    "    - split documents into managebale chunks\n",
    "    - detect Named Entities and Relationships\n",
    "    - build a graph\n",
    "    - create an hierarchy\n",
    "    - generate a summarization - first on low level, then on high level\n",
    "- Aggregate in map-reduce style\n",
    "- Generate answer\n",
    "\n",
    "<img src=\"img/graphRAG.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LightRAG (2024)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2410.05779)<br>\n",
    "LightRAG = a separate parallel implementation of the similar idea, but with focus on <u>fast</u> indexing and retrieval\n",
    "\n",
    "LightRAG is a <u>Hybrid</u> approach - it is intended to work with both specific queries (through vector retrieval) and broad queries (thriygh graph retrieval)\n",
    "\n",
    "Examples of specific / broad queries:<br>\n",
    "“Who wrote ’Pride and Prejudice’?”<br>\n",
    "“How does artificial intelligence influence modern education?”\n",
    "\n",
    "__Algorithm__\n",
    "1. Graph building\n",
    "    - split documents into managebale chunks\n",
    "    - encode each chunk with an embedding (for example using Sentence-BERT)\n",
    "    - extract Entities and Relations from each chunk using \"LLM Profiling\"\n",
    "    - build a graph \n",
    "        - nodes \n",
    "            - chunks \n",
    "            - entities \n",
    "        - edges \n",
    "            - embedding proximity \n",
    "            - having entities \n",
    "            - relationship between entities\n",
    "- Retrieval\n",
    "    - find nodes using a) query proximity b) entity matching\n",
    "    - expand using neighbors + \n",
    "    - rerank and filter documents\n",
    "- Generate answer\n",
    "\n",
    "<img src=\"img/light_rag.png\" width=1000>\n",
    "\n",
    "They compare their performance with GraphRAG and declare LightRAG winning while being way more efficient\n",
    "\n",
    "\n",
    "# PathRAG (2025)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2502.14902)<br>\n",
    "PathRAG = Graph based RAG but instead of retrieving all relevant communities / subgraphs detect only crucial dependency paths in these graphs and rewrite by summarizing them\n",
    "\n",
    "__Algorithm__\n",
    "1. Graph building\n",
    "    - Nodes = entity or text-chunk nodes extracted from the corpus.\n",
    "    - Edges represent relations (e.g., co-occurrence or semantic links).\n",
    "2. Retrieval\n",
    "    - select anchor nodes (by embedding proximity or entity matching)\n",
    "    - select paths connecting anchor nodes to other relevant nodes (multi-hop graph traversal).\n",
    "    - rank paths by total “resource” score and prune low-value or redundant paths.\n",
    "    - add other path features (e.g., length, connectivity).\n",
    "    - order paths by reliability\n",
    "    - format them as prompt bullets for the LLM\n",
    "3. Generation\n",
    "    - Feed the structured prompt into the LLM to generate a logical, coherent response using the curated paths.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/path_rag.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SPLADE (2021)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2107.05720)<br>\n",
    "SPLADE = Sparse Lexical and Expansion Model for Information Retrieval\n",
    "\n",
    "SPLADE is a two-tower Encoder (BERT) model, used to rank documents by relevance with query augmentation \n",
    "\n",
    "Towers has additional projection layer at the output - it maps output embedding to token distribution.\n",
    "The purpose of this distribution is to encode tokens \"related\" to each input token. It works as query enhancement<br>\n",
    "$z_i = W \\cdot h_i + b, \\quad z_i \\in \\mathbb{R}^{|V|}$\n",
    "\n",
    "<img src=\"img/splade2.png\" width=750>\n",
    "\n",
    "Document-level aggregation of embeddings is done by MaxPooling of token probability - maximal seen signal for token goes to the output<br>\n",
    "$z = \\max_{i=1..n}(z_i), \\quad z \\in \\mathbb{R}^{|V|}$\n",
    "\n",
    "Sparsification is enforced by Lasso ($L_1$) regularization + smoothed Loss function<br>\n",
    "$\\mathcal{L}_\\text{total} = \\mathcal{L} + \\lambda \\cdot (\\|v_q\\|_1 + \\|v_{d^+}\\|_1 + \\|v_{d^-}\\|_1)$\n",
    "\n",
    "Smoothed Loss function guarantees less flucatuation around zero values<br>\n",
    "$v_x = \\log(1 + \\text{ReLU}(z)), \\quad v_x \\in \\mathbb{R}^{|V|}$\n",
    "\n",
    "At inference time sparsified distribution and fetch candidate documents from inverted index\n",
    "\n",
    "<img src=\"img/splade.png\" width=500>\n",
    "\n",
    "Model is trained on labeled relevance dataset (i.e. MS MARCO) using contrastive loss function with regularization<br>\n",
    "$\\mathcal{L} = \\max(0, \\text{margin} - S(q, d^+) + S(q, d^-))$\n",
    "\n",
    "Where model output is predicted relevance score:<br>\n",
    "$S(q, d) = \\langle v_q, v_d \\rangle$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SPLADE++ (2021)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2109.10086)<br>\n",
    "Same idea as SPLADE, but several advancements in comparison to v1:\n",
    "1. they propose more aggregation strategies (max, sum, avg, weighted)\n",
    "2. instead of two-tower architecture they distill from cross-encoder\n",
    "3. they added weighting in regularization based no token frequency in batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COIL (2021)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2104.07186)<br>\n",
    "COIL = Contextualized Inverted List Retrieval<br>It is an example of Hybrid retrieval: sparse, but with neural enhancements<br>\n",
    "\n",
    "__Idea:__ we rely on classic sparse matching, but instead of computing standard (non-contextual) TFs, we use <u>multidimensional embeddings</u> \n",
    "\n",
    "Inverted index still stores documents with non-zero occurences => the fetch process remains the same. But document ranking is different  - it is done by summing dot-products of token embeddings vs documents embeddings\n",
    "\n",
    "Interpretation:<br>\n",
    "Per-document token embeddings reflect their importance to this particular document. Query token embeddings reflects its importance to query being processed. Their combination models query-document relevance in terms of this token. Total relevance is modeled as a sum of all token matches\n",
    "\n",
    "<img src=\"img/COIL.png\" width=750>\n",
    "\n",
    "# uniCOIL (2022)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2106.14807)<br>\n",
    "uniCOIL is a newer and simplified version of COIL - it drops complexity to make retrieval faster<br>\n",
    "\n",
    "Two main simplifications:\n",
    "1) we compute scalar value as a score, instead of computing multidimensional embedding as COIL does<br>it is done by appending a single MLP head to the Encoder\n",
    "2) unlike COIL, we do not compute scores for query, only for documents - for queries we stay with one-hot encodings (constant values)\n",
    "Apart from this, methodology is the same = we fetch posting lists from inverted index and compute per-token dot-products to rerank\n",
    "\n",
    "Algorithm:\n",
    "For each token $t$ in a document $D$ we predict scalar score $s(D,t)$ using BERT model with one scalar head (token importance score). All documents with non-zero scores are appended to postings list of that token. For query encoding at inference time use standard one-hot encoding. Alternatively, we can also apply BERT with scalar head. Both variants are applicable: first is faster, second is (a little bit) more accurate. Relevance score is a dot-product\n",
    "\n",
    "Second simplification is optional - we can compute context-depending embeddings for query:\n",
    "<img src=\"img/uniCOIL.png\" width=750>\n",
    "\n",
    "\n",
    "# DeepImpact\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2104.12016)<br>\n",
    "DeepImpact\n",
    "\n",
    "Model architecture:\n",
    "- encode input sequence with BERT model\n",
    "- additional MLP layer that outputs a scalar \"impact score\"<br>interpretation = how important this token is to the input\n",
    "\n",
    "Training process:\n",
    "- contrastive loss is used: query and 2 candidate documents\n",
    "- we compute two scores for positive and negative documents\n",
    "- aggregate scores to get 2 predicted \"relevances\"\n",
    "\n",
    "Index construction:\n",
    "- iterate over all documents from a database\n",
    "- append document tokens to an inverted index with its score\n",
    "\n",
    "Inference:\n",
    "- get the input\n",
    "- fetch the listings from the inverted index\n",
    "- for each document from a listing compute the relevance by summing pre-stored scores\n",
    "\n",
    "<img src=\"img/DEEPIMPACT.png\" width=750>\n",
    "\n",
    "\n",
    "# DeepCT\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/1910.10687)<br>\n",
    "DeepImpact\n",
    "\n",
    "<img src=\"img/DEEPCT.png\" width=500>\n",
    "\n",
    "Model architecture:\n",
    "- encode input sequence with BERT model\n",
    "- additional MLP layer that outputs a scalar \"impact score\"<br>interpretation = how important this token is to the input\n",
    "- enforce sparsity \n",
    "\n",
    "Training process:\n",
    "- done in a self-supervised fashion: for each dociuemtn we make model predict the occurence of token in the input<br>interpretation = likelihood of the token in that context\n",
    "- logistic loss is used\n",
    "\n",
    "Index construction:\n",
    "- iterate over all documents from a database\n",
    "- append document tokens to an inverted index with its score\n",
    "\n",
    "Inference:\n",
    "- get the input\n",
    "- fetch the listings from the inverted index\n",
    "- for each document from a listing compute the relevance by summing pre-stored scores\n",
    "\n",
    "\n",
    "# TILDE\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2108.08513)<br>\n",
    "TILDE =  Term Independent Likelihood moDEl for Passage Re‑ranking\n",
    "\n",
    "Model architecture:\n",
    "- encode input sequence with BERT model, but use only [CLS] output\n",
    "- a small MLP decoder maps this embedding back to token space, returning the probabilities vector $[P_i]$<br>here P = probability that x occures in X\n",
    "- enforce sparsity either by applying threshold OR top-k selection\n",
    "- output = one vector of token probabilities\n",
    "\n",
    "<img src=\"img/TILDE.png\" width=750>\n",
    "\n",
    "Training process:\n",
    "- done in self-supervised fashion: for each document MLP should predict whether each token occurs in the input\n",
    "- logistic loss is used\n",
    "\n",
    "Index construction:\n",
    "- iterate over all documents from a database\n",
    "- append selected tokens to an inverted index with their probabilities\n",
    "\n",
    "Inference:\n",
    "- get the input\n",
    "- fetch the listings from the inverted index\n",
    "- for each document from a listing compute the relevance by summing pre-stored probabilities\n",
    "\n",
    "Model architecture is very similar to Autoencoders: Encoder learns to extract semantics, Decoder learns to extract tokens\n",
    "\n",
    "Advancements in TILDEv2:\n",
    "- instead of binary occurence (0,1) Encoder is being trained on TF-IDF\n",
    "- model is distilled from more powerful models like SPLADE<br>SPLADE is more powerfil beacuse it is token-level, not document-level like TILDE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Doc2Query (2019)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/1904.08375)<br>\n",
    "__Idea:__ instead of enhancing user queries let's __enhance documents__ by generating potential queries and appending them to document text\n",
    "\n",
    "Training process: Take some sequence-to-sequence model like T5 and train on a labeled dataset MS MARCO, but in reversed fashion (prompt = document, answer = question)\n",
    "\n",
    "Such document enhancement will increase the recall of the retirve \n",
    "\n",
    "<img src=\"img/doc2query.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
