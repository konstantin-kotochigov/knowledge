{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First LLM models relied entirely on their own knowledge. Problem: the knowledge of first ChatGPT was 2 years behind the actual time. To overcome this you had to continuosly retrain the whole model which is unfeasible.\n",
    "\n",
    "Classic RAG system:\n",
    "1. Retriever component fetches candidate documents (focus on recall)<br>\n",
    "   - using query matching\n",
    "   - using embedding matching\n",
    "2. LLM uses them as a context\n",
    "\n",
    "Challenge, not only in the domain of LLM generation but in the IR generally = original user query might be not enough for retireval of all relevant documents\n",
    "\n",
    "# User query insufficiency (1982)\n",
    "---\n",
    "[[paper](https://arxiv.org/pdf/2503.00223)]<br> Users often cannot articulate their needs precisely because they don’t fully understand the problem => IR systems must be context-dependent, personalized, dialogue maintaining\n",
    "\n",
    "# Rocchio (1971)\n",
    "---\n",
    "Rocchio algorithm defined the first personalized version of document Retrieval. What it does - it shifts query vector (from the document-query vector space) in the direction towards documents previously positively evaluated by this user\n",
    "\n",
    "__Query Augmentation__ = rewriting user query to reflect all sides of user's intent\n",
    "\n",
    "# PRF (2021)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2108.11044)<br>PRF = Pseudo-Relevance Feedback. It is the algorithm to enhance the original query with related / synonim terms. Synonymic terms = most frequent terms that appear in the list of documents returned on the first retrieval\n",
    "\n",
    "<img src=\"img/prf.png\" width=400>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dense Retrieval\n",
    "---\n",
    "Dense Retrieval is a family of algorithms where queries and documents are mapped into the same latent vector-space by some sort of Encoder. Query/document proximity is then calculated by a cosine / dot-product similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# REALM (2020)\n",
    "---\n",
    "[[paper]]()<br>\n",
    "Retrieval‑Augmented Language Model Pre‑Training = one of the first implementations of Dense Retrieval where the Retriever model is trainable and is trainable together with Generator LLM\n",
    "\n",
    "Retriever is an early-linkage two-tower model\n",
    "\n",
    "Training procedure\n",
    "1. pretraining: some text corpora is masked (MLM) => we get a labeled QA dataset for self-suprrevised training\n",
    "2. fine-tuning: trained on QA dataset\n",
    "\n",
    "<img src=\"img/realm.png\" width=750>\n",
    "\n",
    "\n",
    "\n",
    "# ColBERT (2020)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2004.12832)<br>\n",
    "In late 2010s transformer-based Encoders (BERT) narrowed the gap by learning to extract the latent semantics of the query and documents. Late linkage models like ColBERT made the process efficient\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Embedding are per-token => all-vs-all comparison with Max aggregation<br>\n",
    "Linkage is late<br>\n",
    "Used only for ranking, not generation\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/colbert.png\" width=750>\n",
    "\n",
    "# Zero-shot retrieval\n",
    "---\n",
    "Out-of-the box pretrained Encoders do not know how to rank by relevance. They need to be fine-tuned on some relevance dataset. Usually contrastive losses are used. But labeling is expensive => there is a challenge of __zero-shot retrieval__ - fetching without previous training on human labeled data\n",
    "\n",
    "# DPR-CTL (2020)\n",
    "---\n",
    "A neural retrieval method that is trained in self-supervised fashion - neighboring chunks of text are considered positive exmaples, random chunks - negative ones. It achieves comparable to fine-tuned alternatives performance \n",
    "\n",
    "\n",
    "\n",
    "# DPR (2020)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2004.04906)<br>DPR = Dense Passage Model is an implementation of Dense Retrieval offered by Meta. It uses two BERT models to encode query and documents and dot-product as similarity measure. Encoders are trained over contrastive loss (one positive and one negative example)\n",
    "\n",
    "# RAG (2020)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2005.11401)<br>Retrieval Augmented Generation = the first implementation of the RAG approach when the term was coined. It uses DPR retrieval combined with some generation model (BART in the original paper)\n",
    "\n",
    "<img src=\"img/rag.png\" width=600>\n",
    "\n",
    "# ExaRanker (2023)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2402.06334)<br>\n",
    "Before training the Retriever on some relevance dataset let's use a strong LLM (like Chatgpt) to generate a textual \"explanation\" for each example in this dataset (\"this document is relevant because ...\")\n",
    "\n",
    "During training phase make the Retriever model not only predict the correct label, but also reconstruct this explanation<br>This develops model's reasoning ability about relevance, avoids prediction hacking. Distance to ground-truth explanation is measured using standard text (sequence-to-sequence) loss\n",
    "\n",
    "<img src=\"img/exarank.png\" width=400>\n",
    "\n",
    "# CONQRR (2022)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2112.08558)<br>\n",
    "Retrieval for conversational (dialog) systems is more challenging since the query might be distributed along the previous conversation (\"What about his birthplace?\"). CONQRR summarizes all necessary information for the query to be effectively processed. It is retriever agnostic and relies only on query rewriting\n",
    "<img src=\"img/conqrr.png\" width=400>\n",
    "\n",
    "# Contextual Clues Sampling (2022)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2210.07093)<br>\n",
    "The authors suggest using some strong LLM (ChatGPT) to enhance original query by generating a list of related terms - they call them \"contextual clues\"<br>Not sure what exact prompt do they use \"Model, generate me related terms\"?\n",
    "\n",
    "Retriever model runs multiple fetches - one for each enhancement and extracts a list of documents which are next fused into one large list.\n",
    "\n",
    "Diversity is achieved by multiple generations. They are followed by deduplication - identic or similar \"clues\" are grouped into clusters<br>Precision is achieved by first ranking the clues and then retrieved documents according to their generation probabilities. Only top-K are used.\n",
    "\n",
    "<img src=\"img/contextual_clues.png\" width=400>\n",
    "\n",
    "# DeepRetrieval (2025)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2503.00223)<br>Enhances user query by rewriting it with a (reasoning) LLM model\n",
    "\n",
    "LLM is trainable and is updated using RL (PPO). Reward consists of two pieces: query consistency (how good new prompt is formatted) + Recall-based reward (how relevant are the documents we fetched)\n",
    "\n",
    "Requires some kind of pre-labeled dataset to be able to evaluate the relevance reward\n",
    "\n",
    "<img src=\"img/deep_retrieval.png\" width=400>\n",
    "\n",
    "# Search-R1 (2025)\n",
    "---\n",
    "[[paper]](https://arxiv.org/pdf/2503.09516)<br>Treats retrieval as a multi-step iterative enhancement process. Model inserts API calls during reasoning and fetches new data. \n",
    "\n",
    "Retrieval here is a part of generator model. The fetch itself is not trainable, just an API call. Reasoning can be adjusted. \n",
    "\n",
    "The model is updated using DPO/GRPO. Reward is determined by the correctness of the final answer (ExactMatch). Requires some pre-labeled dataset\n",
    "\n",
    "# S3\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2505.14146)<br>\n",
    "S3 = Search, select, serve. When Generator is not trainable, focus on fine-tuning the Retriever model\n",
    "\n",
    "In s3  we train the Retrieval model using RL. \n",
    "\n",
    "A reward is an uplift compared to some baseline (RAG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GraphRAG (2024)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2404.16130)<br>\n",
    "\n",
    "In case of \"broad\" queries (that require some aggregation of knowledge) regular RAG tends to give too fragmented answers<br>Instead of doing an exhausting full-scan over all documents in a corpus, let's make the knowledge hierarchical and query it in a tree-like fasion. \n",
    "\n",
    "Example of a broad query<br>\"What are the main research themes and their interconnections in the latest COVID-19 scientific literature?\"\n",
    "\n",
    "Graph = named entities linked by their relatshionships. Communities = clusters of similar nodes. They might have different levels of aggregation (large communities consisting of smaller communities)\n",
    "\n",
    "__Algorithm__\n",
    "- Graph building\n",
    "    - split documents into managebale chunks\n",
    "    - detect Named Entities and Relationships\n",
    "    - build a graph\n",
    "    - create an hierarchy\n",
    "    - generate a summarization - first on low level, then on high level\n",
    "- Aggregate in map-reduce style\n",
    "- Generate answer\n",
    "\n",
    "<img src=\"img/graphRAG.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LightRAG (2024)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2410.05779)<br>\n",
    "LightRAG = a separate parallel implementation of the similar idea, but with focus on <u>fast</u> indexing and retrieval\n",
    "\n",
    "LightRAG is a <u>Hybrid</u> approach - it is intended to work with both specific queries (through vector retrieval) and broad queries (thriygh graph retrieval)\n",
    "\n",
    "Examples of specific / broad queries:<br>\n",
    "“Who wrote ’Pride and Prejudice’?”<br>\n",
    "“How does artificial intelligence influence modern education?”\n",
    "\n",
    "__Algorithm__\n",
    "1. Graph building\n",
    "    - split documents into managebale chunks\n",
    "    - encode each chunk with an embedding (for example using Sentence-BERT)\n",
    "    - extract Entities and Relations from each chunk using \"LLM Profiling\"\n",
    "    - build a graph \n",
    "        - nodes \n",
    "            - chunks \n",
    "            - entities \n",
    "        - edges \n",
    "            - embedding proximity \n",
    "            - having entities \n",
    "            - relationship between entities\n",
    "- Retrieval\n",
    "    - find nodes using a) query proximity b) entity matching\n",
    "    - expand using neighbors + \n",
    "    - rerank and filter documents\n",
    "- Generate answer\n",
    "\n",
    "<img src=\"img/light_rag.png\" width=1000>\n",
    "\n",
    "They compare their performance with GraphRAG and declare LightRAG winning while being way more efficient\n",
    "\n",
    "\n",
    "# PathRAG (2025)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2502.14902)<br>\n",
    "PathRAG = Graph based RAG but instead of retrieving all relevant communities / subgraphs detect only crucial dependency paths in these graphs and rewrite by summarizing them\n",
    "\n",
    "__Algorithm__\n",
    "1. Graph building\n",
    "    - Nodes = entity or text-chunk nodes extracted from the corpus.\n",
    "    - Edges represent relations (e.g., co-occurrence or semantic links).\n",
    "2. Retrieval\n",
    "    - select anchor nodes (by embedding proximity or entity matching)\n",
    "    - select paths connecting anchor nodes to other relevant nodes (multi-hop graph traversal).\n",
    "    - rank paths by total “resource” score and prune low-value or redundant paths.\n",
    "    - add other path features (e.g., length, connectivity).\n",
    "    - order paths by reliability\n",
    "    - format them as prompt bullets for the LLM\n",
    "3. Generation\n",
    "    - Feed the structured prompt into the LLM to generate a logical, coherent response using the curated paths.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/path_rag.png\" width=500>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
