{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5df59e0-3288-4d6a-b55b-a9b5854e7581",
   "metadata": {},
   "source": [
    "There is a number of notable papers\n",
    "\n",
    "Most grpahs are plotted on a log-log scale\n",
    "\n",
    "## Maths of Power law\n",
    "---\n",
    "\n",
    "Exponential<br>\n",
    "$f(x)=a^{x}$\n",
    "\n",
    "General power law<br>\n",
    "$f(x) = k \\cdot x^\\alpha$\n",
    "\n",
    "Or shortly: f is proportional to x<br>\n",
    "$f \\propto x^\\alpha$\n",
    "\n",
    "On a Log-Log scale the expression<br>\n",
    "$\\log{f} = \\log{k} + \\alpha \\cdot \\log{x}$\n",
    "<br>becomes a straight line\n",
    "\n",
    "Fun fact: there are no axes on the Log-Log plots since logarithm is never zero (instead you infinitely converge towards zero along the axis)\n",
    "\n",
    "Power law functions $k \\cdot x^\\alpha$ are defined by two parameters: \n",
    "- change dynamics $\\alpha$\n",
    "- scale factor $k$\n",
    "\n",
    "If $\\alpha < 1$ we say function is sublinear and it exposes the diminishing returns<br>\n",
    "If $\\alpha < 0$ the function is decreasing\n",
    "\n",
    "On a Log-Log scale: \n",
    "- power function becomes a straight line \n",
    "- $\\alpha$ defines the tangent of the straight line\n",
    "- $k$ defines the bias\n",
    "\n",
    "$\\alpha$ is enough to evaluate the scaling rate of parameters. $k$ is reuired if we want to predict the concrete values\n",
    "\n",
    "Here are the examples of different scaling factors - they help match available data points\n",
    "\n",
    "<img src=\"img/powerlaw1.png\" width=750>\n",
    "\n",
    "And here we can see different powers - they help model the dependency kind\n",
    "\n",
    "<img src=\"img/powerlaw2.png\" width=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ff7e6-e2d2-4c7e-bd54-567de3ac1645",
   "metadata": {},
   "source": [
    "# Kaplan 2020\n",
    "---\n",
    "\n",
    "Authors from OpenAI explore two main characteristics of the model:\n",
    "- N = model size (in parameters)\n",
    "- D = training time (in tokens)\n",
    "\n",
    "*N does not include embeddings as they are not very informative (not very controllable)\n",
    "\n",
    "The task is to predict the behavior of model performance:\n",
    "- L = performance <br>validation loss, aka cross-entropy of the corresponding language modeling task\n",
    "\n",
    "Third important parameter\n",
    "- C = compute (in FLOPs)\n",
    "\n",
    "C is a combination of the two other parameters<br> $C = 6ND$. It optimization to a narrow set of points holding same compute\n",
    "\n",
    "#### Why parameter C is importnant?\n",
    "In practice C is often your only input and parameters N and D are under control\n",
    "\n",
    "#### Where does 6ND come from?\n",
    "Compute can be estimated as 6 FLOPs per one network weight = 1-2 on forward pass, 2 on backward, 2 on ...\n",
    "\n",
    "Exact formulas also exist. For a standard Transformer model where L=input, d=dimensionality:<br>\n",
    "$$\\underset{projections}{3Ld^2} + \\underset{attention}{L^2d}+\\underset{output}{Ld^2} = 4Ld^2 + L^2d$$\n",
    "\n",
    "But those are not as convenient, beacuase they depend on a particular architecture of the network, and we want our estimates to be as robust as possible\n",
    "\n",
    "For a batch processing there is an alternative variant<br>$C=6NDB$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Digest of Key findings\n",
    "---\n",
    "- __Performance depends strongly on scale, weakly on model shape__<br>Model performance increases with model scale (N,D,C) and does not depend much on the architecture<br><br>\n",
    "- __Smooth power laws:__<br>Performance depends on scale parameters as power law $N^{-1}, D^{-1}, C^{-1}$, given that other parameters are set free<br><br>\n",
    "- __Universality of overfitting__<br>this power law scaling is not additive, you have to scale both N and D simultaneously (ideal combination D = N^{0.75})<br><br>\n",
    "- __Universality of training__<br>learning curves are universal and do not depend on N => one can use them to extrapolate <br>it contradicts an image<br><br>\n",
    "- __Transfer improves with test performance__<br>distribution shift in data generates a constant penalty preserves the shape<br><br>\n",
    "- __Sample efficiency__<br>arger models reach the same loss faster than small models<br><br>\n",
    "- __Convergence is inefficient__<br><br>\n",
    "- __Optimal batch size__<br><br>\n",
    "\n",
    "\n",
    "# Main Laws\n",
    "---\n",
    "\n",
    "First, they explore individual laws (one argument)\n",
    "\n",
    "### Model size\n",
    "$$L(N) = L_0 + \\bigg(\\frac{N_c}{N}\\bigg)^{0.076} \\quad \\text{where } N_c ∼ 8.8 × 10^{13}$$\n",
    "\n",
    "Where did we get D?<br>\n",
    "For each $N$ we train the model virtually \"infinitely\" $D \\rightarrow \\infty$. The limit that $L(N)$ converges to is assigned to each N<br>According to Kaplan there is always some $D^*$ training time after which loss function stabilizes on a plateau - the model \"learned\" everything it could have learned\n",
    "\n",
    "Conclusion = The performance of a model of size N is distributed accoring to scaling law\n",
    "\n",
    "The process is illustrated by the learning curves (see the limits on the right handside)\n",
    "\n",
    "<img src=\"img/learning1.png\" width=300>\n",
    "\n",
    "Facts:\n",
    "- Smaller models reach the plateau faster<br>there is less potential for learning<br><br>\n",
    "- Larger models are always trained faster<br>given two models trained for the equal amount of time, the larger model will always outperform the smaller one<br><br>\n",
    "- Final model performance (validational) = plateau height $L(N)$<br>defines how much we can potentially squeezer from thr model of that size<br><br>\n",
    "- Model loss is diminishing according to power law presented above $L(N) \\propto N^{-0.076}$<br><br>\n",
    "- learning curves resemble sigmoids on a Log-Log scale<br>heating, active learning, plateau<br><br>\n",
    "\n",
    "The irreducible loss bias $L_0$ is sometimes omited for simplicity\n",
    "\n",
    "### Dataset size\n",
    "D denotes the dataset size\n",
    "$$L(D) = \\bigg(\\frac{Dc}{D}\\bigg)^{0.095} \\text{ where } D_c ∼ 5.4 × 10^{13}$$\n",
    "For each value of $D$ we train the model (potentially multiple epochs) on this dataset until it converges\n",
    "\n",
    "Conclusion = The performance that we can squeeze from the dataset of size D is according to power law\n",
    "\n",
    "<img src=\"img/kaplan_d.png\" width=400>\n",
    "\n",
    "Note that here D is not a Training time! To get a training time refer to the Model size learning curves (plotted above)<br>And to get asymptotic training times refer to the yellow line from that graph\n",
    "\n",
    "### Compute size\n",
    "\n",
    "C is number of FLOPs\n",
    "\n",
    "$$L(C) = \\bigg(\\frac{C_c}{C}\\bigg)^{0.05} \\text{where } C_c = 1.3 × 10^{22} \\text{ FLOPs per hour}$$\n",
    "\n",
    "For each compute $C$ check all $(N,D)$ pairs and assign corresponding loss values. One (N,D) pair is always optimal - it corresponds to minimal validation loss\n",
    "\n",
    "*Loss is computed in Nats - this just means they use natural lograithms in cross-entropy calculation\n",
    "\n",
    "<img src=\"img/learning2.png\" width=450>\n",
    "\n",
    "## Optimal frontier\n",
    "If we send <u>more</u> data than optimal, we encounter <u>underfit</u><br>the model have not enough time to update its weights properly; we are higher than the forntier line\n",
    "\n",
    "If we send <u>less</u> data than optimal, we encounter <u>overfit</u><br>\n",
    "; we are higher than the forntier line\n",
    "\n",
    "Если подать больше данных, чем оптимально => underfit\n",
    "Если подать меньше данных, чем оптимально => overfit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Предположим, \n",
    "1) у нас есть оптимальная обученная модель\n",
    "2) надо её обновить/улучшить и на следующий месяц нам выделили повышенную квоту на compute => можем обучить еще раз\n",
    "Что выгоднее: увеличить размер модели и обучать столько же или размер оставить и обучать дольше?\n",
    "\n",
    "Ответ авторов очевиден - важнее размер модели, при этом обучать надо тоже дольше (но меньше).\n",
    "\n",
    "Что такое оптимально обученая LLM модель?\n",
    "это модель при заданном лимите на Compute (ND) достигающая наибольшей точности\n",
    "\n",
    "Авторы замеряют только pre-training LLM модели, fine-tuning или reinforcement learning не цчитываются\n",
    "\n",
    "\n",
    "В 2022 авторы Chinchilla показали, что Kaplan были не совсем правы и недооценивали важность D\n",
    "Причина была в том, что они не проверяли на действительно больших объемах, где закономерности немного меняются\n",
    "К тому же показали, что GPT-3 была недообучна (можно было её улучшить)\n",
    "\n",
    "Мы считаем, что ресурсы используются эффективно - то есть Compute распределен между N и D так, чтобы получить наилучший performance. А именно:\n",
    "$$N = D^{0.67}$4\n",
    "\n",
    "Откуда соотношение N и D?\n",
    "\n",
    "Теория\n",
    "Зная вид функции потерь, можно подставить произвольную изокосту и найти минимум на этой кривой. Он довольно просто выражается\n",
    "$$D = k N^{\\gamma=\\frac{\\alpha}{\\beta}}$$\n",
    "$\\alpha$ и $\\beta$ ранее также оценивались $\\alpha=0.076$, $\\beta=1.25$, получаем формулу\n",
    "Вывод чуть сложнее, чем финальное выражение\n",
    "Выберем C_0 и выпишем L\n",
    "\n",
    "Практика\n",
    "Однако, конкретное значение они подбирали эмпирически\n",
    "\n",
    "## Overview of laws\n",
    "Here is the list of all described laws\n",
    "\n",
    "<img src=\"img/kaplan_overview.png\" width=750>\n",
    "\n",
    "# Architecture tweaks\n",
    "\n",
    "Approach: they fix total number of parameters, dataset size and vary some aspect of the model\n",
    "\n",
    "### Feedforward Ratio\n",
    "\n",
    "Feedforward Ratio = inner dimensionality of the second FFN layer (as a rate of input/output layer dimensionality). Varies in [0.5, 10]\n",
    "\n",
    "How FFN shape affects performance\n",
    "\n",
    "<img src=\"img/arch1.png\" width=300>\n",
    "\n",
    "Increasing dimensions in FFN means they have to deduct weights from somewhere else to maintain the overall amount. \n",
    "They deduct from Attention layer\n",
    "\n",
    "Two strategies how to do it:\n",
    "- number of attention heads are fixed, but their dimensionality varies\n",
    "- dimensionality fixed, but the number of heads varies\n",
    "\n",
    "Conclusion: it is better to make FFN thinner\n",
    "\n",
    "*all evaluations were run on rather small models\n",
    "\n",
    "__Aspect Ratio__\n",
    "\n",
    "Aspect Ratio = model dimensionality /  model layers\n",
    "\n",
    "<img src=\"img/arch2.png\" width=300>\n",
    "\n",
    "There is clearly some in-between balance. Too wide and too narrow networks make performance degrade. But not too much (2-4%) - even poor model can be trained to perform well if there is enough compute\n",
    "\n",
    "Conclusion: dimensionality should be 10-100 times larger than number of layers\n",
    "\n",
    "Evaluations were run at three scales, the pattern persists\n",
    "\n",
    "### LSTM comparison\n",
    "\n",
    "Transformers are comparable with LSTMs for small contexts, but win a lot for larger ones\n",
    "per-token loss = an averaged loss when predicting the next token on a position P\n",
    "\n",
    "<img src=\"img/lstm1.png\" width=350>\n",
    "\n",
    "<img src=\"img/lstm2.png\" width=350>\n",
    "\n",
    "### Including embeddings\n",
    "Почему они не включают эмбединги в кол-во параметров N?\n",
    "В маленьких моделях они могут составлять значительную часть всей модели (5-30%), а польза от них статичная\n",
    "Графики это иллюстрируют - на них мальенкие модели (100M), добавление слоев к узкой модели сильно ее улучшает. Добавление слоев к нормальной модели не сильно ее улдушает. \n",
    "Даже двуслойные модели можно неплохо обучить (удивительно)\n",
    "\n",
    "<img src=\"img/arch4.png\" width=300>\n",
    "\n",
    "<img src=\"img/arch5.png\" width=300>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b64b5-16c4-4e96-9740-8f1d522f05ba",
   "metadata": {},
   "source": [
    "# Examples\n",
    "---\n",
    "\n",
    "For a 1B model<br>\n",
    "$\\begin{cases}\n",
    "  A100: \\,\\, 1 \\text{ Exa-FLOPs per hour} = 10^{18} \\text{ FLOPs per hour} \\\\\n",
    "  6ND: \\,\\, 6 * 10^9 * 10^8 \\text{ FLOPS required to get a properly trained 1B model}\n",
    "\\end{cases}$\n",
    "\n",
    "Time = 1600 hours (~6000$)\n",
    "\n",
    "Kaplan only states that $D \\propto N^{0.76}$ $\\big( D = k N^{0.76}\\big)$ if you need to be efficient<br>The community estimated the concrete training factor k = 20:<br>\n",
    "$D = 20 \\cdot N^{0.76}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0721ef-3528-44e9-9c18-4d47c30addac",
   "metadata": {},
   "source": [
    "# Chinchilla Scaling (2020)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/2203.15556)\n",
    "\n",
    "Two years later DeepMind challenged the conclusions made by OpenAI and made a statement that Kaplan et al significantly <u>underestimated</u> the importance of dataset size D\n",
    "\n",
    "The correct combination is <br> $D \\propto N^{1.02}$\n",
    "\n",
    "They explain with small model sizes (1B maximum)\n",
    "Причина была в том, что они не проверяли на действительно больших объемах, где закономерности немного меняются\n",
    "К тому же показали, что GPT-3 была недообучна (можно было её улучшить)\n",
    "\n",
    "Мы считаем, что ресурсы используются эффективно - то есть Compute распределен между N и D так, чтобы получить наилучший performance. А именно:\n",
    "$$N = D^{0.67}$$\n",
    "\n",
    "### Approach 1\n",
    "- Pre-build a loss surface\n",
    "- For a chosen C make a slice and return optimal value\n",
    "### Approach 2\n",
    "- For a chosen C loop through the constraint and fill with model loss\n",
    "- Return optimal value\n",
    "### Apporach 3\n",
    "- Find an analytical solution\n",
    "\n",
    "Hypothetical example:\n",
    "\n",
    "<img src='img/syntetic_chinchilla.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23f990-5682-4ce8-916a-48dfa57bf595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e134a95-70c2-460a-ba3d-5b68bf28e2f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43d4f3-8bdc-4e29-8cc0-2cf9006ab53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509ef55-f1a2-484b-b48e-84162ce5f5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
