{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5931bd4-a779-4a93-8d8a-b1d1be1d143c",
   "metadata": {},
   "source": [
    "__Problem setting:__ in Retrieval tasks (search, recommendations, RAG) we seek for records in a huge dataset, that are the most relevant in some sense to our query (i.e. have the smallest $L_2$ distance). \n",
    "\n",
    "Full-scan of the database will require linear complexity $O(n)$, which is OK for toy examples but not acceptable for large-scale products => we need to find a way to do it faster. \n",
    "\n",
    "Solution = store records in a format optimized for proximity-based retrieval, that would provide less than $O(n)$ complexity.\n",
    "\n",
    "# FAISS\n",
    "---\n",
    "FAISS = \"Facebook AI Similarity Search\" by Meta. This library provides several ANN algorithms\n",
    "\n",
    "## IndexFlat (Brute-Force)\n",
    "Index construction:<br>- store observations as-is in a plain table\n",
    "\n",
    "Search<br>- compute $L_2$ distances from query to all records and select top-n\n",
    "\n",
    "## IndexIVF (Inverted File Index)\n",
    "\n",
    "Index construction:<br>- do K-means clustering and describe each cluster with its centroid<br>- store all members of each cluster in a separate bucket<br>$\\,\\,\\,$(during search those K buckets will act as an inverted index: cluster_id $\\rightarrow$ members list).\n",
    "\n",
    "\n",
    "Search<br>- compare query vector with all centroids in $O(n)$ and select top-N clusters<br>- do linear search among all members of those n clusters\n",
    "\n",
    "<img src=\"img/ivf.png\" width=500>\n",
    "\n",
    "Red partitions = top-2 selected clusters<br>we seek for top-5 neighbours in those clusters. Out of 5 points: 1 missed / 1 is falsely selected<br>\n",
    "\n",
    "It is not perfect. If clusters are not unifrom (see example above), we can get trapped in proximal but small clusters and overlook many neighbouring points <br>\n",
    "\n",
    "__Example__<br>\n",
    "Supppose we have $10^6$ document vectors in a $\\mathbb{R}^{64}$ Document-vector space. If we cluster it into 1024 clusters and retrieve top-3 clusters we get the 250x FLOPs reduction:$$\\frac{64 \\cdot 10^6}{64 \\cdot 10^3 + 3 \\cdot 64 \\cdot 10^3} = \\frac{64 \\cdot 10^6}{256 \\cdot 10^3} = 250$$\n",
    "\n",
    "\n",
    "\n",
    "## PQ (Product Quantization)\n",
    "Index construction:<br>- split dimensions into m bands<br>- do K-means clustering (1024 clusters) for each band and describe each cluster with its centroid<br>- store all cluster members in a separate bucket B(b,c)<br>- store as cluster index (8-bit)\n",
    "\n",
    "Search<br>- compare query vector with all centroids and select top-n $O(n)$<br>- do linear search among n clusters\n",
    "\n",
    "<img src=\"img/pq.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2d7bf-1559-4784-a3dc-4bbab818d583",
   "metadata": {},
   "source": [
    "# Annoy\n",
    "---\n",
    "Annoy = \"Approximate Nearest Neighbors Oh Yeah\" by Spotify\n",
    "\n",
    "Index contruction<br>- Build a binary tree using random space partitioning<br>$\\,\\,\\,\\,$close points have more chance to appear in the same partition<br>- Repeat to get multiple trees\n",
    "\n",
    "Search<br>- Traverse all trees to reach the resulting node<br>- Intersect those nodes to get a partition with candidate points<br>- Evaluate candidates in $O(n)$ and rank them using $L_2$ distance\n",
    "\n",
    "<img src=\"img/annoy.png\" width=500>\n",
    "\n",
    "Because of the \"random\" nature both index construction and search are lightning fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec259708-9471-42d7-87bf-0e94b75409a9",
   "metadata": {},
   "source": [
    "## ScaNN\n",
    "---\n",
    "ScaNN = \"Scalable Nearest Neighbors\" by Google\n",
    "\n",
    "### Algorithm: Hybrid Tree + Asymmetric Hashing + Reordering\n",
    "\n",
    "1. Partitioning (Tree Search):\n",
    "   - Data clustered into centroids (leaves).\n",
    "   - Only top-N leaves visited during search.\n",
    "\n",
    "2. Asymmetric Hashing:\n",
    "   - Compresses database vectors.\n",
    "   - Approximates dot/cosine similarity using quantized representations.\n",
    "\n",
    "3. Reordering:\n",
    "   - Top-K candidates re-evaluated with full-precision vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4dd92-e7e6-4726-8148-f26e185fdaa5",
   "metadata": {},
   "source": [
    "# HNSW\n",
    "---\n",
    "HNWS = \"Hierarchical Navigable Small World\", endorsed by Yandex\n",
    "\n",
    "Index construction\n",
    "<br>- store the data as a multi-layer graph\n",
    "<br>- top layers are sparse (few points), bottom layers are dense (all points)\n",
    "<br>- each node links to its K closest neighbours on the same layer\n",
    "<br>- each node links to its instance on the lower layer\n",
    "\n",
    "Search\n",
    "<br>- start from the top level of the hierarchy and navigate in search for the closest node\n",
    "<br>- move to the lower layer and repeat the process\n",
    "<br>- do linear search on the bottom layer to find the exact solutions\n",
    "\n",
    "In Graph theory a \"Small World\" graph = highly connected graph, when a distance (# of edges) between every pair of nodes is relatively small. This propery is beneficial for fast navigation and search\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/hnsw.png\" width=750>\n",
    "\n",
    "## Summary Comparison\n",
    "\n",
    "| Library   | Core Algorithm                       | Index Type              | Speed     | Accuracy | Memory Use   | Best Use                          |\n",
    "|-----------|---------------------------------------|--------------------------|-----------|----------|--------------|-----------------------------------|\n",
    "| **FAISS** | IVF + PQ / HNSW / Flat                | Quantization / Graph     | ✅✅✅ (GPU) | ✅✅✅    | Medium–Low   | Large-scale, GPU-based search     |\n",
    "| **Annoy** | Random Projection Trees               | Tree-based               | ✅✅       | ✅✅      | Low          | Lightweight, disk-based retrieval |\n",
    "| **ScaNN** | Tree + Hashing + Reordering           | Hybrid                   | ✅✅✅      | ✅✅✅    | Medium       | ML-serving, dot/cosine retrieval  |\n",
    "| **HNSW**  | Multi-layer Navigable Graph           | Graph-based              | ✅✅✅      | ✅✅✅    | Medium–High  | Real-time, high-recall search     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e436c8e-008a-4ed9-938e-dc6b6e785466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f9af4-fbe8-4cfa-9574-8b581857ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Sample item embeddings (100 items, 64-dim)\n",
    "item_embeddings = np.random.rand(100, 64).astype('float32')\n",
    "\n",
    "# Build index for Inner Product similarity (cosine if normalized)\n",
    "index = faiss.IndexFlatIP(64)  # or faiss.IndexFlatL2(64)\n",
    "index.add(item_embeddings)\n",
    "\n",
    "# Query: 1 user embedding\n",
    "query = np.random.rand(1, 64).astype('float32')\n",
    "scores, ids = index.search(query, k=5)\n",
    "\n",
    "print(\"FAISS:\", ids, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba85e70d-b7fa-49d9-9687-2c59bb95f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 1e+03 ns, total: 4 μs\n",
      "Wall time: 5.01 μs\n",
      "Annoy: ([28, 598, 407, 114, 266], [0.5223425626754761, 0.5260087251663208, 0.56670081615448, 0.567012369632721, 0.5683410167694092])\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "\n",
    "f = 64  # dimension\n",
    "index = AnnoyIndex(f, 'angular')\n",
    "\n",
    "# Add 1000000 items to index\n",
    "for i in range(1000):\n",
    "    vec = np.random.rand(f)\n",
    "    index.add_item(i, vec.tolist())\n",
    "\n",
    "index.build(10)  # 10 trees\n",
    "\n",
    "# Query\n",
    "query = np.random.rand(f)\n",
    "\n",
    "%time\n",
    "ids = index.get_nns_by_vector(query.tolist(), 5, include_distances=True)\n",
    "\n",
    "print(\"Annoy:\", ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dc9562-4501-483e-8410-ae1418c162ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hnswlib\n",
      "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from hnswlib) (2.3.0)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp313-cp313-macosx_10_13_universal2.whl size=419349 sha256=a66111578fc206f4eeb6f120684acc6e244ee311893156ed33c504bcb6247095\n",
      "  Stored in directory: /Users/konstantin/Library/Caches/pip/wheels/35/04/88/b31765a4b9957705e18065db4657e61fc8da54f50e3ef0b67e\n",
      "Successfully built hnswlib\n",
      "Installing collected packages: hnswlib\n",
      "Successfully installed hnswlib-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip3.13 install hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280fb4c-a93e-4139-8e4b-b798b24afbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scann\n",
    "\n",
    "# Sample data\n",
    "item_embeddings = np.random.rand(100, 64).astype('float32')\n",
    "query = np.random.rand(1, 64).astype('float32')\n",
    "\n",
    "# Build ScaNN index\n",
    "searcher = scann.scann_ops_pybind.builder(item_embeddings, 5, \"dot_product\").tree(\n",
    "    num_leaves=100, num_leaves_to_search=10, training_sample_size=1000).score_ah(\n",
    "    2, anisotropic_quantization_threshold=0.2).reorder(5).build()\n",
    "\n",
    "# Query\n",
    "neighbors, distances = searcher.search_batched(query)\n",
    "\n",
    "print(\"ScaNN:\", neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c11f15a4-8205-4ff2-a75a-4043ffe3a0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 0 ns, total: 2 μs\n",
      "Wall time: 4.77 μs\n",
      "HNSW: [[778  55 597 391 866]] [[0.13336885 0.15304852 0.1588034  0.16153485 0.16769516]]\n"
     ]
    }
   ],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = 64\n",
    "num_elements = 1000\n",
    "\n",
    "# Initialize index\n",
    "index = hnswlib.Index(space='cosine', dim=dim)\n",
    "index.init_index(max_elements=num_elements, ef_construction=100, M=16)\n",
    "\n",
    "# Add item vectors\n",
    "data = np.random.rand(num_elements, dim).astype('float32')\n",
    "index.add_items(data)\n",
    "\n",
    "# Query\n",
    "query = np.random.rand(1, dim).astype('float32')\n",
    "\n",
    "%time\n",
    "labels, distances = index.knn_query(query, k=5)\n",
    "\n",
    "print(\"HNSW:\", labels, distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5a5111-9940-4180-b33f-20c31702c069",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Configurations\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "# Configurations\n",
    "d = 128         # vector dimension\n",
    "nb = 100_000    # number of database vectors\n",
    "nq = 100        # number of query vectors\n",
    "k = 10          # number of nearest neighbors to search\n",
    "\n",
    "# Generate random vectors\n",
    "np.random.seed(123)\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "xq = np.random.random((nq, d)).astype('float32')\n",
    "\n",
    "# ----------------------\n",
    "# 1. Exact: IndexFlatL2\n",
    "# ----------------------\n",
    "index_flat = faiss.IndexFlatL2(d)\n",
    "index_flat.add(xb)\n",
    "\n",
    "start_flat = time.time()\n",
    "D_flat, I_flat = index_flat.search(xq, k)\n",
    "end_flat = time.time()\n",
    "\n",
    "print(f\"IndexFlatL2 search time: {end_flat - start_flat:.4f} seconds\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Approximate: IndexIVFPQ\n",
    "# -------------------------\n",
    "nlist = 100      # number of clusters\n",
    "m = 16           # number of PQ subquantizers (d should be divisible by m)\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(d)  # quantizer used for k-means\n",
    "index_ivfpq = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)  # 8 bits per subquantizer\n",
    "\n",
    "# Training (needed for IVFPQ)\n",
    "index_ivfpq.train(xb)\n",
    "index_ivfpq.add(xb)\n",
    "\n",
    "# Set number of clusters to search (tradeoff between recall and speed)\n",
    "index_ivfpq.nprobe = 10\n",
    "\n",
    "start_ivfpq = time.time()\n",
    "D_ivfpq, I_ivfpq = index_ivfpq.search(xq, k)\n",
    "end_ivfpq = time.time()\n",
    "\n",
    "print(f\"IndexIVFPQ search time: {end_ivfpq - start_ivfpq:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
