{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохой разбор здесь<br>\n",
    "https://www.youtube.com/watch?v=lMShR1vjbUo&t=2364s\n",
    "\n",
    "Мы максимизируем Marginal Loglikelihood\n",
    "$$log P(X|\\theta) \\rightarrow \\max$$\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Definitions\n",
    "\n",
    "#### Маржинальное распределение\n",
    "Агрегированное по некоторой переменной, в данной случае по неизвестной нам $Z$\n",
    "\n",
    "<img src=\"img/marginal.png\" width=300>\n",
    "\n",
    "Интерпретация: размазали масло по плоскости (complete distribution), а потом сгребли его к одному краю (marginal). На выходе тоже распределение (суммируется в 1.0), но уже по одной переменной вместо двух\n",
    "\n",
    "#### Likelihood\n",
    "Правдоподобие = вероятность реализации выборки $x$ в предположении, что выбрана модель $\\theta$\n",
    "\n",
    "$$L(\\theta) = P(x|\\theta)$$\n",
    "\n",
    "Используется также в Байесовской интерпретации, когда параметр модели воспринимается как случайная величина. В этом случае мы можем не только найти argmax L, но и посчитать апостериорное распределение параметра\n",
    "\n",
    "$$P(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{P(x)} = \\frac{likelihood \\cdot prior}{marginal}$$\n",
    "\n",
    "#### Definition MLE\n",
    "Maximum Likelihood Estimation - чтобы выбрать модель, вероятнее всего сгенерировавшую наблюдаемые данные, ищут $$\\theta = argmax \\{ L(\\theta) \\}$$\n",
    "\n",
    "#### Definition Marginal Likelihood\n",
    "Маржинальное правдоподобие - термин из Байесовской статистики. Это скаляр, равный правдоподобию $L(\\theta)$, взвешенному по распределению параметра $\\theta$ (в Байесовской постановке модель - это распредлеение, а не значение)\n",
    "\n",
    "$$L = P(x) = \\sum_z P(z) \\cdot P(x|z,\\theta)$$\n",
    "\n",
    "Здесь $P(x,z)$ агрегируем до $P(x)$\n",
    "\n",
    "Почему максимизацируем именно его? Потом что больше нечего\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM: Подробный вывод\n",
    "\n",
    "Основное свойство маржинального распределения P(x) - для любых $q(z)$ и $\\theta$ его всегда можно оценить снизу:\n",
    "\n",
    "$$p(x|\\theta) \\geqslant \\underbrace{\\sum q(z) \\log \\frac{p(x,z)}{q(z)}}_{L(q, \\theta)}$$\n",
    "\n",
    "Эту штуку принято называть $L(q,\\theta)$ или ELBO (evidence lower bound). \n",
    "\n",
    "План такой - будем поочередно выбирать $q(z)$ и $\\theta$ так, чтобы пошагово приближаться к оптимуму $p(x|\\theta)$\n",
    "- сначала выбираем q, который максимизирует правдоподобие при заданном $\\theta$ (спойлер - это делается аналитически)\n",
    "- выбираем модель, \n",
    "\n",
    "Для начала докажем это свойство. По определению маржинального правдоподобия:\n",
    "\n",
    "$$\\log P(x|\\theta) = \\log \\bigg( \\sum_{z} P(x,z|\\theta) \\bigg)$$\n",
    "\n",
    "Взвесим на q(z) и применим неравенство Йенсена:\n",
    "\n",
    "$$\\log P(x|\\theta) = \\log \\bigg( \\sum_{z} q(z) \\cdot \\frac{P(x,z|\\theta)}{q(z)} \\bigg) \\geqslant  \\sum_{z} q(z) \\cdot \\log \\bigg( \\frac{P(x,z|\\theta)}{q(z)} \\bigg)$$\n",
    "\n",
    "Свйоство доказано.\n",
    "\n",
    "Покажем, что есть один q(z), который максимизирует наш ELBO,а именно $q^{*} = p(z|x)$. \n",
    "\n",
    "Для этого перейдем от $p(x,z)$ в знаменателе к $p(z|x)$:\n",
    "\n",
    "$$p(x,z) = \\sum p(z|x) \\cdot p(x)$$\n",
    "\n",
    "Оценка разложится на два слагаемых:\n",
    "\n",
    "$$L(q,\\theta) = \\underbrace{\\sum_{z} q(z) \\cdot \\log \\bigg( \\frac{P(z|x,\\theta)}{q(z)} \\bigg)}_{-D_{KL}} + \\underbrace{\\sum_{z} q(z) \\cdot \\log  p(x|\\theta)}_{\\log p(x|\\theta)}$$\n",
    "\n",
    "Второе слагаемое - просто взвешивание константы (там нет z), поэтому оно равно $\\log p(x|\\theta)$\n",
    "\n",
    "Тогда можем переписать:\n",
    "\n",
    "$$L(q, \\theta) = \\log p(x) - D_{KL} $$\n",
    "\n",
    "$$\\log p(x) = L(q, \\theta) + D_{KL} $$\n",
    "\n",
    "Мы помним, что всякая $D_{KL} \\geqslant 0$, а это значит зазор между оценкой $L(q,\\theta)$ и $\\log p(x)$ как раз содержится в дивергенции $D_{KL}$\n",
    "\n",
    "Это значит, что максимум достигается при $D_{KL} = 0$, то есть $q^{*} = p(z|x)$\n",
    "\n",
    "Мы нашли оценку неизвестной перемнной $q(z)$, при которой в рамках модели \\theta данные наиболее правдоподобны\n",
    "\n",
    "Нет гарантии, что $\\theta$ оптимальный выбор модели. Второй шаг делаем по переменной $\\theta$ при фиксированном знании $q*$\n",
    "\n",
    "$$\\theta^{*} = \\underset{\\theta}{\\mathrm{argmax}} \\big\\{ L(q^{*}, \\theta) \\big\\} = \\underset{\\theta}{\\mathrm{argmax}} \\bigg\\{ \\sum_z q^{*} \\cdot \\log \\frac{p(x,z)}{q^{*}} \\bigg\\} = $$\n",
    "\n",
    "Знаменатель не зависит от $\\theta$, его выкидываем:\n",
    "\n",
    "$$\\theta = \\underset{\\theta}{\\mathrm{argmax}} \\bigg\\{ \\sum q^{*} \\cdot \\log p(x,z|\\theta) \\bigg\\}\\ = \\underset{\\theta}{\\mathrm{argmax}} E_{q^{*}} \\big[ \\log p(x,z|\\theta) \\big] $$\n",
    "\n",
    "Мы нашли новый параметр $\\theta$, который улучшает нашу нижнюю границу $L(q, \\theta)$, а значит и само правдоподобие $\\log p(x|\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С расчетом возникает сложность, если $P(X|\\Theta)$ зависит не только от параметров модели, но и от скрытых (латентных) переменных: $P(X|\\Theta) = P(X|Z, \\Theta)$\n",
    "\n",
    "В этом случае нам нужно взвешивать (формулой полной веротяности) по возможным значениям Z $$P(X|\\Theta) = \\sum_{Z} P(X|Z,\\Theta) P (Z|\\Theta)$$\n",
    "\n",
    "## Алгоритм\n",
    "1) для выбранной модели $\\theta$ подбираем распределение z такое, чтобы ELBO лучше всего приближала искомую $p(x|\\theta)$: \n",
    "\n",
    "$$Q(\\Theta|\\Theta_{current}) = E_{Z|X,\\Theta_{current}} logP(X|Z,\\Theta)$$\n",
    "\n",
    "2) для выбранного распределения z подбираем лучше всего описывающую пару (x,z) модель: \n",
    "\n",
    "$$\\Theta_{new} = \\underset{\\theta}{\\mathrm{argmax}} Q(\\Theta)$$\n",
    "\n",
    "Повторяем до сходимости $$\\Theta_0, \\Theta_1 \\cdots \\Theta_n \\rightarrow \\Theta_{opt}$$\n",
    "\n",
    "\n",
    "## Алгоритм (\"на пальцах\"):\n",
    "1. Фиксируем параметры модели $\\Theta_{current}$ и ищем наиболее вероятный набор Z\n",
    "2. Фиксируем значения $Z$ и подбираем наиболее веротяные значения параметра $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приложения\n",
    "Всё, где есть латентные переменные. Грубо - все генеративные модели\n",
    "- K-means\n",
    "- Gaussian Mix Models\n",
    "- Topic Modeling\n",
    "- Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическая обоснованность алгоритма\n",
    "\n",
    "Функцию правдоподобия можно оценить снизу:\n",
    "\n",
    "\n",
    "$$L(\\theta) = logP(X|\\theta) \\ge E_{Z|X} logP(X,Z) + H(X) = ELBO$$ \n",
    "\n",
    "ELBO = Evidence Lower Bound\n",
    "\n",
    "Схема работы алгоритма показана на картинке ниже:\n",
    "- красная линия - искомая функция правдоподобия (нам не видна)\n",
    "- синяя - ELBO для первой оценки\n",
    "- зеленая - ELBO для второй оценки\n",
    "<img src=\"img/em.jpeg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иллюстрация на примере K-means\n",
    "\n",
    "### Дано \n",
    "Некоторая выборка на плоскости, состоящая из N точек\n",
    "\n",
    "<img src=\"img/kmeans_0.png\">\n",
    "    \n",
    "Принадлежность точки классу определяется латентной переменной $Z=(z_1 \\cdots z_N)$. Считаем, что в ней представлены данные 3 классов и $z \\in \\{'red','blue','green'\\}$. То есть вектор **Z** - это раскраска множества точек выборки в 3 цвета. \n",
    "    \n",
    "Параметры $\\Theta = (\\theta_1, \\theta_2, \\theta_3)$ характеризуют центры распределения каждого из классов $\\{x|x \\in X_1\\},\\{x|x \\in X_2\\},\\{x|x \\in X_3\\}$. Раскраска в k-means однозначно опредлеяется параметрами $\\Theta$.\n",
    "\n",
    "### Задача\n",
    "Определить оптимальный набор $\\Theta$, который наиболее правдоподобно описывает данные. Раскраска (Z) тогда будет рассчитана автоматически и должна выглядеть примерно так:\n",
    "\n",
    "<img src = \"img/kmeans_3.png\">\n",
    "\n",
    "### Алгоритм\n",
    "\n",
    "#### 0. Initialization\n",
    "\n",
    "Берем некий начальный параметр $\\Theta_0$. Значения латентной переменной **Z** зависят от $\\Theta$: чем ближе точка к центру кластера, тем больше веротяность этого кластера $P(Z|\\Theta)$, \n",
    "\n",
    "<img src=\"img/kmeans_1.png\">\n",
    "\n",
    "\n",
    "#### 1 Expectation\n",
    "\n",
    "$E_{Z|X,\\Theta}logP(X,Z) = \\sum P(Z|X,\\Theta_0)logP(X|Z,\\Theta)$\n",
    "\n",
    "- $Z|X\\Theta$\n",
    "\n",
    "    Смотрим условное распределение вероятности латентных переменных Z при фиксированном $\\Theta$: $$P(Z|X,\\Theta)$$\n",
    "\n",
    "    Для начала посчитаем $P(z_i|x_i,\\Theta)$ для отдельной точки - она могла быть сгенерирована из любого из трех классов, поэтому ее приндлежность - апостериорная веротяность, которая рассчитывается по теореме Байеса, как взвешенная сумма правдоподобий каждого класса.\n",
    "\n",
    "    Далее $P(Z|X,\\Theta)$ считается как произведение $P(z_i|x_i,\\Theta)$\n",
    "\n",
    "    Соджержательно это ответ на вопрос - Как распределены значения латентных переменных (разделение на классы) при заданном $\\Theta$\n",
    "    \n",
    "\n",
    "- $log P(X|Z,\\Theta)$\n",
    "\n",
    "    Смотрим правдоподобие выборки $X$ при данных $Z$ и $\\Theta$. То есть при заданной раскраске $Z$ и заданных распределениях $\\Theta$, какова веротяность наблюдать данную выборку.\n",
    "\n",
    "- $E(\\Theta)$\n",
    "\n",
    "    \n",
    "\n",
    "Далее, так как k-means это у нас пример hard-clustering, то мы дальше мы работаем не с распределением, а берем наиболее вероятный набор значений. Он легко определяется без особой математики - всё  что ближе к центру кластера X, относится к кластеру X.\n",
    "\n",
    "<img src=\"img/kmeans_2.png\">\n",
    "\n",
    "\n",
    "#### 2. Maximization \n",
    "\n",
    "Теперь фиксируем Z, который мы нашли на предыдущем шаге, и корректируем параметр $\\Theta$. \n",
    "\n",
    "Выбираем на плоскости такие центры распределения, которые лучше всего описывают текущие значения Z. Лучше всего = правдоподобие $P(X|Z,\\Theta)$ наибольшее.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иллюстрация на примере GMM (gaussian mix model)\n",
    "\n",
    "Он же soft-clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
