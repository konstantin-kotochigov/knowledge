{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation\n",
    "\n",
    "Assume distribution of X depends on $\\theta$.\n",
    "\n",
    "There is some estimate. It could be for example maximum likelihood estimate - for each **X** we seek for the $\\theta$ with the highest likelihood. Realization of X is random and has some variation. Hence the estimate also has some variation. Variance of the estimate of $\\theta$ is a mean squared error.\n",
    "\n",
    "Intuitevly this error is higher the lower the difference in the neighborhood (in terms of corresponsing X distributions) of $\\theta$. Because in that case we miss more often. That's exactly what Rao-Cramer therorem states.\n",
    "\n",
    "$$var(\\theta)=E(\\theta - \\hat{\\theta})^ 2 \\le \\frac{1}{I(\\theta)}$$\n",
    "where $I(\\theta)$ - fisher information\n",
    "$$I(\\theta) = E_x \\bigg[ \\frac{\\partial^2 L}{\\partial \\theta^2} \\bigg] $$\n",
    "\n",
    "Here is the illustration. At the peak the second derivatives are low. On the sides they are high - there the likelihood functions change rapidly.\n",
    "\n",
    "<img src=\"rao_cramer.png\" width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The same in more detail\n",
    "\n",
    "Without loss of generality we will depict samples **X** as if they were scalars x.\n",
    "\n",
    "Each **X**  is a random variable and has its own distribution. This distribution is dependent on $\\theta$.\n",
    "\n",
    "The task of estimator $\\hat{\\theta}$ is to predict $\\theta$ based on observed X.\n",
    "\n",
    "Every estimator misses and distribution of its error $\\hat{\\theta} - \\theta$ (across different implementations of **X**) is chartacterized by its bias and variance.\n",
    "\n",
    "Second derivative is taken with respect of parameter $\\theta$! For each sample **X** It shows how likelihood changes in the neighborhood of $\\theta$ if current value of parameter is $\\theta$.\n",
    "\n",
    "Then we average over all samples **X** to get second derivative for value of $\\theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Case\n",
    "\n",
    "In general case there are multiple parameters that define data distribution. \n",
    "It could be for example Normal distribution.\n",
    "\n",
    "In that case we deal with Fisher Information matrices rather than Fisher Information scalars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "It's an importnant element of estimation theory. It helps to describe properies of linear regression estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
