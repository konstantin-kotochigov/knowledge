{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions\n",
    "\n",
    "Purpose:\n",
    "- adds non-linearity\n",
    "- controls output scale\n",
    "\n",
    "In case of linear activation (LU) network reduces to single linear mapping => no difference from linear regression\n",
    "\n",
    "Activations are usually chosen to be __differentiable__ almost everywhere (usually except fracture points like 0). Optimization algorithms overcome this by using __subgradients__ or setting derivative to arbitrary value for those points\n",
    "\n",
    "Other useful properties include convectivity and smoothness\n",
    "\n",
    "Typical scales:\n",
    "- LU $\\rightarrow (-\\infty; +\\infty)$<br><br>\n",
    "- ReLU $\\rightarrow(0; +\\infty)$<br>when we require output signal to be strictly positive<br><br>\n",
    "- Tahn $\\rightarrow(-1,1)$<br>when we require output signal to be limited<br><br>\n",
    "- Sigmoid / Softmax $\\rightarrow(0,1)$<br>when we model output signal as probabilties<br>\n",
    "\n",
    "Some functions require setting additional hyperparameters (like pReLU or HardTanh), but most do not\n",
    "\n",
    "Most functions are applied element-wise, some depend on the whole vector (like Softmax)\n",
    "\n",
    "In Pytorch activations are implemented as separate Modules (with input and forward() function as in layers)<br>\n",
    "(<a href=\"https://github.com/pytorch/pytorch/blob/260d1dcef4d82d0a2181d516707f6cdf2a054413/torch/nn/modules/activation.py#L97\">Module class</a> and <a href=\"https://github.com/pytorch/pytorch/blob/260d1dcef4d82d0a2181d516707f6cdf2a054413/torch/_refs/nn/functional/__init__.py#L257\">function</a>)\n",
    "\n",
    "| ProbDistribution | Accuracy ||\n",
    "| --- | --- | --- |\n",
    "|Basic functions|\n",
    "| <img src=\"img/sigmoid.png\" width=500> | <img src=\"img/Sigmoid_formula.png\" width=200>||\n",
    "| <img src=\"img/tahn.png\" width=500> | <img src=\"img/Tahn_formula.png\" width=300> |\n",
    "| <img src=\"img/softmax.png\" width=500> | <img src=\"img/Softmax_formula.png\" width=150> |\n",
    "|Rectified Linear functions|\n",
    "| <img src=\"img/ReLU.png\" width=500> | <img src=\"img/ReLU_formula.png\" width=200>|Rectified Linear Unit|\n",
    "| <img src=\"img/pReLU.png\" width=500> | <img src=\"img/PReLU_formula.png\" width=200> |\n",
    "| <img src=\"img/SeLU.png\" width=500> | <img src=\"img/SeLU_formula.png\" width=500> |\n",
    "| <img src=\"img/ELU.png\" width=500> | <img src=\"img/ELU_formula.png\" width=300>|Rectified Linear Unit|\n",
    "| <img src=\"img/CELU.png\" width=500> | <img src=\"img/CELU_formula.png\" width=300> |\n",
    "| <img src=\"img/GELU.png\" width=500> | <img src=\"img/GELU_formula.png\" width=150> |\n",
    "| <img src=\"img/RRELU.png\" width=500> | <img src=\"img/RRELU_formula.png\" width=200>|Rectified Linear Unit|\n",
    "| <img src=\"img/ReLU6.png\" width=500> | <img src=\"img/ReLU6_formula.png\" width=200> |\n",
    "| <img src=\"img/SiLU.png\" width=500> | <img src=\"img/SiLU_formula.png\" width=500> |\n",
    "| <img src=\"img/LogSigmoid.png\" width=500> | <img src=\"img/LogSigmoid_formula.png\" width=200>||\n",
    "| <img src=\"img/Mish.png\" width=500> | <img src=\"img/Mish.png\" width=300>|Rectified Linear Unit|\n",
    "| <img src=\"img/softtahn.png\" width=500> | <img src=\"img/softtahn_formula.png\" width=300> |\n",
    "| <img src=\"img/Hardtanh.png\" width=500> | <img src=\"img/hardtahn_formula.png\" width=150> |piecewise linear approximation of Tanh|\n",
    "| <img src=\"img/hardsigmoid.png\" width=500> | <img src=\"img/hardSigmoid_formula.png\" width=300>|Rectified Linear Unit|\n",
    "| <img src=\"img/hardswish.png\" width=500> | <img src=\"img/Hardswish_formula.png\" width=300>|piecewise approximation of Swish $x \\sigma(x)$ as in <a href=\"https://arxiv.org/abs/1905.02244\">MobileNet</a> - allows formulation though popular ReLU|\n",
    "| <img src=\"img/hardsigmoid.png\" width=500> | <img src=\"img/hardsigmoid_formula.png\" width=250> |linear approximation of sigmoid (no parameters)|\n",
    "| <img src=\"img/hardshrink.png\" width=500> | <img src=\"img/hardshrink_formula.png\" width=250> |maps zero neighborhood to precise zero|\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
