{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f112329c9b4269a5fba0c5ab14704e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84df2f4d705e48fcb42d91db5c42dcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf01dd92c0f34810ba826333895ea47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c6c594e8144c80ba559086beaf084a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/konstantin/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1595629430416/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len =  60000\n",
      "target vector =  tensor([9, 0, 0,  ..., 3, 0, 5])\n",
      "\n",
      "class names:\n",
      "0 T-shirt/top\n",
      "1 Trouser\n",
      "2 Pullover\n",
      "3 Dress\n",
      "4 Coat\n",
      "5 Sandal\n",
      "6 Shirt\n",
      "7 Sneaker\n",
      "8 Bag\n",
      "9 Ankle boot\n",
      "\n",
      "class assignment:\n",
      "0 T-shirt/top\n",
      "1 Trouser\n",
      "2 Pullover\n",
      "3 Dress\n",
      "4 Coat\n",
      "5 Sandal\n",
      "6 Shirt\n",
      "7 Sneaker\n",
      "8 Bag\n",
      "9 Ankle boot\n",
      "\n",
      "train_data type =  <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-374e6f4a267a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\ntrain_data type = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nx[0] = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(\"len = \", len(training_data))\n",
    "\n",
    "print(\"target vector = \", training_data.targets)\n",
    "\n",
    "print(\"\\nclass names:\")\n",
    "for i, e in enumerate(training_data.classes):\n",
    "    print(i, e)\n",
    "\n",
    "print(\"\\nclass assignment:\")\n",
    "for k,v in training_data.class_to_idx.items():\n",
    "    print(v,k)\n",
    "\n",
    "print(\"\\ntrain_data type = \",type(training_data.train_data))\n",
    "\n",
    "print(\"\\nx[0] = \", training_data[0], training_data[0])\n",
    "\n",
    "dir(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten()\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    #train mode\n",
    "    model.train()\n",
    "    \n",
    "    # results\n",
    "    loss_log = []\n",
    "    accuracy_log = []\n",
    "    \n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader)):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # batch evaluation\n",
    "        loss_log.append(loss.item())\n",
    "        accuracy_log.append((pred.argmax(1) == y).type(torch.float).sum().item())\n",
    "    \n",
    "    return loss_log, accuracy_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    loss_log = []\n",
    "    accuracy_log = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            # batch evaluation\n",
    "            loss_log.append(loss.item())\n",
    "            accuracy_log.append((pred.argmax(1) == y).type(torch.float).sum().item())\n",
    "    \n",
    "    return loss_log, accuracy_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_run(model, loss_fn, optimizer):\n",
    "  epochs = 100\n",
    "  train_loss, test_loss, train_acc, test_acc = [],[],[],[]\n",
    "  \n",
    "  for t in range(epochs):\n",
    "      \n",
    "      loss, acc = train(train_dataloader, model, loss_fn, optimizer)\n",
    "      train_loss += loss\n",
    "      train_acc += acc      \n",
    "      \n",
    "      loss, acc = test(test_dataloader, model, loss_fn)\n",
    "      test_loss += loss\n",
    "      test_acc += acc\n",
    "  \n",
    "  return train_loss, train_acc, test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [00:06, 136.55it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 175.05it/s]\n",
      "938it [00:07, 132.70it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 191.44it/s]\n",
      "938it [00:07, 128.98it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 173.69it/s]\n",
      "938it [00:07, 120.58it/s]\n",
      "100%|██████████| 157/157 [00:01<00:00, 133.90it/s]\n",
      "938it [00:08, 105.52it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 176.03it/s]\n",
      "938it [00:06, 134.73it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 181.58it/s]\n",
      "938it [00:07, 124.99it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 202.17it/s]\n",
      "938it [00:06, 136.97it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 183.96it/s]\n",
      "938it [00:06, 142.27it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 201.89it/s]\n",
      "938it [00:06, 141.46it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 190.63it/s]\n",
      "938it [00:06, 143.12it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 205.87it/s]\n",
      "938it [00:06, 141.46it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 190.62it/s]\n",
      "938it [00:06, 144.92it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 201.43it/s]\n",
      "938it [00:06, 145.22it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 205.39it/s]\n",
      "938it [00:06, 144.61it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 202.49it/s]\n",
      "938it [00:06, 142.21it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 190.11it/s]\n",
      "938it [00:06, 144.11it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 207.00it/s]\n",
      "938it [00:06, 139.73it/s]\n",
      "100%|██████████| 157/157 [00:01<00:00, 119.95it/s]\n",
      "938it [00:06, 139.86it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 194.39it/s]\n",
      "938it [00:06, 140.47it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 200.43it/s]\n",
      "938it [00:08, 108.58it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 161.37it/s]\n",
      "938it [00:11, 81.85it/s] \n",
      "100%|██████████| 157/157 [00:00<00:00, 180.94it/s]\n",
      "938it [00:07, 133.54it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 182.11it/s]\n",
      "938it [00:06, 145.41it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 194.19it/s]\n",
      "938it [00:07, 130.79it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 199.21it/s]\n",
      "938it [00:06, 143.33it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 201.05it/s]\n",
      "938it [00:06, 139.18it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 196.66it/s]\n",
      "938it [00:06, 146.18it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 198.86it/s]\n",
      "938it [00:07, 132.45it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 182.75it/s]\n",
      "938it [00:07, 131.40it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 195.94it/s]\n",
      "938it [00:06, 142.28it/s]\n",
      "100%|██████████| 157/157 [00:01<00:00, 153.11it/s]\n",
      "938it [00:07, 132.00it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 174.81it/s]\n",
      "938it [00:06, 134.54it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 194.75it/s]\n",
      "938it [00:06, 144.80it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 198.54it/s]\n",
      "938it [00:06, 140.72it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 193.16it/s]\n",
      "938it [00:06, 144.42it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 197.82it/s]\n",
      "938it [00:06, 145.66it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 205.40it/s]\n",
      "938it [00:06, 143.62it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 195.18it/s]\n",
      "938it [00:06, 143.22it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 204.66it/s]\n",
      "938it [00:06, 147.44it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 193.41it/s]\n",
      "938it [00:06, 148.42it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 196.79it/s]\n",
      "938it [00:09, 100.35it/s]\n",
      "100%|██████████| 157/157 [00:04<00:00, 38.17it/s]\n",
      "938it [00:08, 111.75it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 184.22it/s]\n",
      "938it [00:06, 135.47it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 190.95it/s]\n",
      "938it [00:06, 142.19it/s]\n",
      "100%|██████████| 157/157 [00:11<00:00, 13.43it/s]\n",
      "938it [00:12, 74.22it/s] \n",
      "100%|██████████| 157/157 [00:00<00:00, 193.90it/s]\n",
      "938it [00:07, 125.76it/s]\n",
      "100%|██████████| 157/157 [00:01<00:00, 149.48it/s]\n",
      "938it [00:08, 112.55it/s]\n",
      "100%|██████████| 157/157 [00:01<00:00, 143.52it/s]\n",
      "938it [00:07, 132.89it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 172.79it/s]\n",
      "938it [00:08, 111.86it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 189.49it/s]\n",
      "938it [00:06, 136.15it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 191.40it/s]\n",
      "938it [00:06, 141.88it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 201.65it/s]\n",
      "938it [00:06, 140.30it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 178.56it/s]\n",
      "938it [00:06, 134.34it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 186.31it/s]\n",
      "938it [00:07, 132.75it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 190.96it/s]\n",
      "938it [00:07, 128.22it/s]\n",
      "100%|██████████| 157/157 [00:01<00:00, 144.24it/s]\n",
      "938it [00:08, 111.83it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 178.40it/s]\n",
      "938it [00:07, 128.61it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 191.42it/s]\n",
      "938it [00:06, 134.03it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 182.71it/s]\n",
      "938it [00:06, 138.77it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 165.10it/s]\n",
      "938it [00:07, 127.51it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 192.13it/s]\n",
      "938it [00:07, 127.78it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 187.23it/s]\n",
      "938it [00:07, 125.17it/s]\n",
      "100%|██████████| 157/157 [00:01<00:00, 128.09it/s]\n",
      "938it [00:07, 127.31it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 195.38it/s]\n",
      "938it [00:06, 138.21it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 162.14it/s]\n",
      "938it [00:06, 136.39it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 196.85it/s]\n",
      "938it [00:08, 111.44it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 192.98it/s]\n",
      "938it [00:07, 133.21it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 198.48it/s]\n",
      "938it [00:08, 116.00it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 170.84it/s]\n",
      "938it [00:08, 115.68it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 164.50it/s]\n",
      "938it [00:07, 119.13it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 187.35it/s]\n",
      "938it [00:07, 132.08it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 189.81it/s]\n",
      "938it [00:07, 122.68it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 195.76it/s]\n",
      "938it [00:06, 134.97it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 189.92it/s]\n",
      "938it [00:07, 126.99it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 177.73it/s]\n",
      "938it [00:06, 136.92it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 190.06it/s]\n",
      "938it [00:08, 115.82it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 191.57it/s]\n",
      "938it [00:07, 130.26it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 198.22it/s]\n",
      "938it [00:06, 136.86it/s]\n",
      "100%|██████████| 157/157 [00:01<00:00, 156.32it/s]\n",
      "938it [00:07, 132.06it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 177.84it/s]\n",
      "938it [00:07, 132.59it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 186.02it/s]\n",
      "938it [00:07, 129.51it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 191.18it/s]\n",
      "938it [00:06, 141.23it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 180.43it/s]\n",
      "938it [00:07, 128.95it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 197.56it/s]\n",
      "938it [00:06, 136.54it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 203.85it/s]\n",
      "938it [00:06, 134.53it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 198.31it/s]\n",
      "938it [00:07, 133.17it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 195.55it/s]\n",
      "938it [00:06, 142.45it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 186.17it/s]\n",
      "938it [00:07, 125.81it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 194.82it/s]\n",
      "938it [00:08, 113.39it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 172.49it/s]\n",
      "938it [00:06, 134.22it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 172.08it/s]\n",
      "938it [00:07, 126.03it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 199.60it/s]\n",
      "938it [00:06, 144.49it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 201.52it/s]\n",
      "938it [00:06, 140.40it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 194.57it/s]\n",
      "938it [00:06, 142.00it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 192.07it/s]\n",
      "938it [00:07, 132.24it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 196.08it/s]\n",
      "938it [00:07, 133.75it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 182.64it/s]\n",
      "938it [00:06, 135.11it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 171.85it/s]\n",
      "938it [00:07, 132.65it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 159.78it/s]\n",
      "938it [00:07, 127.01it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 195.65it/s]\n"
     ]
    }
   ],
   "source": [
    "run_params = [(model, loss_fn, optimizer)] * 1\n",
    "\n",
    "results = []\n",
    "for i, param in enumerate(run_params):\n",
    "    train_loss, train_acc, test_loss, test_acc = model_run(param[0], param[1], param[2])\n",
    "    results.append({'train_loss':train_loss,'train_acc':train_acc,'test_loss':test_loss,'test_acc':test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-c1b2b27e64c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"only recognize index or columns for orient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    433\u001b[0m             )\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         ]\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(results[0]['train_loss'])\n",
    "plt.plot(np.ma.average(results[0]['train_loss']))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "y = results[0]['train_loss']\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[1].plot(x, y, color='dodgerblue')\n",
    "ax[1].fill_between(x, upper, lower, color='crimson', alpha=0.2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5632306466753613"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ma.average(results[0]['train_loss'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa4c4887a50>]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results[0]['train_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39305034279823303,\n",
       " 0.30729934573173523,\n",
       " 0.44052261114120483,\n",
       " 0.5724703669548035,\n",
       " 0.5759528875350952,\n",
       " 0.4795520603656769,\n",
       " 0.4012663662433624,\n",
       " 0.4126700758934021,\n",
       " 0.5416015982627869,\n",
       " 0.5198168158531189,\n",
       " 0.4827893376350403,\n",
       " 0.33115947246551514,\n",
       " 0.3733053207397461,\n",
       " 0.47603461146354675,\n",
       " 0.4459421634674072,\n",
       " 0.5772793292999268,\n",
       " 0.4916297495365143,\n",
       " 0.6746588349342346,\n",
       " 0.5366076827049255,\n",
       " 0.45381709933280945,\n",
       " 0.5090374946594238,\n",
       " 0.4664851427078247,\n",
       " 0.45791125297546387,\n",
       " 0.45888128876686096,\n",
       " 0.3742842376232147,\n",
       " 0.41010013222694397,\n",
       " 0.42427656054496765,\n",
       " 0.39084622263908386,\n",
       " 0.7270249128341675,\n",
       " 0.39394471049308777,\n",
       " 0.49604660272598267,\n",
       " 0.38895341753959656,\n",
       " 0.476762056350708,\n",
       " 0.5226293206214905,\n",
       " 0.5185800194740295,\n",
       " 0.4207804799079895,\n",
       " 0.3009741008281708,\n",
       " 0.817721426486969,\n",
       " 0.40118470788002014,\n",
       " 0.46428972482681274,\n",
       " 0.46502357721328735,\n",
       " 0.5984079837799072,\n",
       " 0.5028018951416016,\n",
       " 0.716476321220398,\n",
       " 0.44852152466773987,\n",
       " 0.5786523222923279,\n",
       " 0.5999372601509094,\n",
       " 0.34010618925094604,\n",
       " 0.46621859073638916,\n",
       " 0.36349767446517944,\n",
       " 0.5904449224472046,\n",
       " 0.45863255858421326,\n",
       " 0.5706771016120911,\n",
       " 0.5164029598236084,\n",
       " 0.4148397743701935,\n",
       " 0.5879116654396057,\n",
       " 0.6269326210021973,\n",
       " 0.3508729934692383,\n",
       " 0.36299222707748413,\n",
       " 0.35362884402275085,\n",
       " 0.7751965522766113,\n",
       " 0.6992810368537903,\n",
       " 0.5059046745300293,\n",
       " 0.6356989145278931,\n",
       " 0.2447640597820282,\n",
       " 0.43022051453590393,\n",
       " 0.39310377836227417,\n",
       " 0.32417726516723633,\n",
       " 0.485198974609375,\n",
       " 0.448737233877182,\n",
       " 0.42763710021972656,\n",
       " 0.2682472765445709,\n",
       " 0.3287971615791321,\n",
       " 0.31268367171287537,\n",
       " 0.383413165807724,\n",
       " 0.34765926003456116,\n",
       " 0.5112209916114807,\n",
       " 0.42540132999420166,\n",
       " 0.4979117214679718,\n",
       " 0.5855283141136169,\n",
       " 0.5954616665840149,\n",
       " 0.5277301073074341,\n",
       " 0.5744565725326538,\n",
       " 0.6402425169944763,\n",
       " 0.5380922555923462,\n",
       " 0.5198229551315308,\n",
       " 0.48287147283554077,\n",
       " 0.4430597424507141,\n",
       " 0.6322048902511597,\n",
       " 0.39427652955055237,\n",
       " 0.4255619943141937,\n",
       " 0.40897971391677856,\n",
       " 0.5305564999580383,\n",
       " 0.45157286524772644,\n",
       " 0.3580929934978485,\n",
       " 0.4991995394229889,\n",
       " 0.5247105956077576,\n",
       " 0.4121894836425781,\n",
       " 0.4565700590610504,\n",
       " 0.5216163992881775,\n",
       " 0.5064216256141663,\n",
       " 0.6140838861465454,\n",
       " 0.44020262360572815,\n",
       " 0.5299127697944641,\n",
       " 0.46396908164024353,\n",
       " 0.39366939663887024,\n",
       " 0.5079464912414551,\n",
       " 0.43719273805618286,\n",
       " 0.40890809893608093,\n",
       " 0.5571925044059753,\n",
       " 0.4813341796398163,\n",
       " 0.4531607925891876,\n",
       " 0.5207076072692871,\n",
       " 0.4360133707523346,\n",
       " 0.622736394405365,\n",
       " 0.45684197545051575,\n",
       " 0.37985774874687195,\n",
       " 0.49951180815696716,\n",
       " 0.47762560844421387,\n",
       " 0.6115015149116516,\n",
       " 0.48788607120513916,\n",
       " 0.5628761649131775,\n",
       " 0.3707750737667084,\n",
       " 0.7004623413085938,\n",
       " 0.47830110788345337,\n",
       " 0.5072765946388245,\n",
       " 0.3587675988674164,\n",
       " 0.40364396572113037,\n",
       " 0.4873824119567871,\n",
       " 0.4235939383506775,\n",
       " 0.5449156761169434,\n",
       " 0.5184130668640137,\n",
       " 0.4418930113315582,\n",
       " 0.5194478631019592,\n",
       " 0.4293413460254669,\n",
       " 0.4658529460430145,\n",
       " 0.43619585037231445,\n",
       " 0.4598142206668854,\n",
       " 0.5457243323326111,\n",
       " 0.6999645829200745,\n",
       " 0.6691672801971436,\n",
       " 0.3664637506008148,\n",
       " 0.5112350583076477,\n",
       " 0.4771837890148163,\n",
       " 0.4634554386138916,\n",
       " 0.3516126871109009,\n",
       " 0.4845191240310669,\n",
       " 0.5659175515174866,\n",
       " 0.474447101354599,\n",
       " 0.5201032757759094,\n",
       " 0.5396295785903931,\n",
       " 0.5277544856071472,\n",
       " 0.5564399361610413,\n",
       " 0.6199665069580078,\n",
       " 0.44312626123428345,\n",
       " 0.3619682788848877,\n",
       " 0.4645080864429474,\n",
       " 0.4226064682006836,\n",
       " 0.22107815742492676,\n",
       " 0.5518702864646912,\n",
       " 0.5058925151824951,\n",
       " 0.42258989810943604,\n",
       " 0.507868230342865,\n",
       " 0.4846303164958954,\n",
       " 0.4096783697605133,\n",
       " 0.38680192828178406,\n",
       " 0.38520538806915283,\n",
       " 0.4769848883152008,\n",
       " 0.49325257539749146,\n",
       " 0.4424966871738434,\n",
       " 0.50697261095047,\n",
       " 0.2529919147491455,\n",
       " 0.5307643413543701,\n",
       " 0.4281277656555176,\n",
       " 0.5300182700157166,\n",
       " 0.4796346127986908,\n",
       " 0.36769670248031616,\n",
       " 0.4139499366283417,\n",
       " 0.6438943147659302,\n",
       " 0.29554373025894165,\n",
       " 0.6280127167701721,\n",
       " 0.44957682490348816,\n",
       " 0.5235241055488586,\n",
       " 0.540284276008606,\n",
       " 0.5163816809654236,\n",
       " 0.8009381294250488,\n",
       " 0.37031835317611694,\n",
       " 0.5241554379463196,\n",
       " 0.40188974142074585,\n",
       " 0.5907085537910461,\n",
       " 0.47831231355667114,\n",
       " 0.4771057069301605,\n",
       " 0.4395715296268463,\n",
       " 0.5244275331497192,\n",
       " 0.5225527286529541,\n",
       " 0.3303036391735077,\n",
       " 0.39264312386512756,\n",
       " 0.44126826524734497,\n",
       " 0.41750192642211914,\n",
       " 0.5544912815093994,\n",
       " 0.32866474986076355,\n",
       " 0.43635934591293335,\n",
       " 0.3695708215236664,\n",
       " 0.362409383058548,\n",
       " 0.49224913120269775,\n",
       " 0.46550458669662476,\n",
       " 0.45485612750053406,\n",
       " 0.3368404507637024,\n",
       " 0.4488610029220581,\n",
       " 0.5209522247314453,\n",
       " 0.5005602836608887,\n",
       " 0.49487626552581787,\n",
       " 0.6602442264556885,\n",
       " 0.4339173436164856,\n",
       " 0.36384162306785583,\n",
       " 0.4020901620388031,\n",
       " 0.4637840986251831,\n",
       " 0.4798186421394348,\n",
       " 0.5109937787055969,\n",
       " 0.6118372678756714,\n",
       " 0.4334772229194641,\n",
       " 0.5617563128471375,\n",
       " 0.39495614171028137,\n",
       " 0.4854161739349365,\n",
       " 0.367780476808548,\n",
       " 0.5238195657730103,\n",
       " 0.4815109968185425,\n",
       " 0.32210204005241394,\n",
       " 0.41352781653404236,\n",
       " 0.44088223576545715,\n",
       " 0.5135626196861267,\n",
       " 0.7036476135253906,\n",
       " 0.45636895298957825,\n",
       " 0.5246769785881042,\n",
       " 0.51605623960495,\n",
       " 0.5673336386680603,\n",
       " 0.44893336296081543,\n",
       " 0.42393937706947327,\n",
       " 0.5610228776931763,\n",
       " 0.5299597978591919,\n",
       " 0.4837726354598999,\n",
       " 0.6044993996620178,\n",
       " 0.49555692076683044,\n",
       " 0.4993945062160492,\n",
       " 0.4576701819896698,\n",
       " 0.44386231899261475,\n",
       " 0.5039470791816711,\n",
       " 0.22239352762699127,\n",
       " 0.48488184809684753,\n",
       " 0.6138301491737366,\n",
       " 0.6277023553848267,\n",
       " 0.34938180446624756,\n",
       " 0.35485395789146423,\n",
       " 0.36799943447113037,\n",
       " 0.5225629210472107,\n",
       " 0.5175760388374329,\n",
       " 0.43877533078193665,\n",
       " 0.35977333784103394,\n",
       " 0.646115243434906,\n",
       " 0.5817157030105591,\n",
       " 0.4559553563594818,\n",
       " 0.5542039275169373,\n",
       " 0.7429399490356445,\n",
       " 0.49623221158981323,\n",
       " 0.42133378982543945,\n",
       " 0.3688810169696808,\n",
       " 0.4853615462779999,\n",
       " 0.5075348615646362,\n",
       " 0.38792744278907776,\n",
       " 0.48145750164985657,\n",
       " 0.6788243651390076,\n",
       " 0.6311056017875671,\n",
       " 0.5563075542449951,\n",
       " 0.6302617192268372,\n",
       " 0.4297546446323395,\n",
       " 0.6113575100898743,\n",
       " 0.5108653903007507,\n",
       " 0.43217578530311584,\n",
       " 0.5280258059501648,\n",
       " 0.4982033967971802,\n",
       " 0.4573298394680023,\n",
       " 0.6094977259635925,\n",
       " 0.7038431167602539,\n",
       " 0.43346574902534485,\n",
       " 0.5460676550865173,\n",
       " 0.6296257972717285,\n",
       " 0.3043968677520752,\n",
       " 0.5722272992134094,\n",
       " 0.4753902852535248,\n",
       " 0.5796980261802673,\n",
       " 0.5035567879676819,\n",
       " 0.3400726318359375,\n",
       " 0.474715918302536,\n",
       " 0.4095647633075714,\n",
       " 0.48231935501098633,\n",
       " 0.4856892228126526,\n",
       " 0.4274023175239563,\n",
       " 0.5631330013275146,\n",
       " 0.7352151870727539,\n",
       " 0.39759546518325806,\n",
       " 0.5726232528686523,\n",
       " 0.5397542715072632,\n",
       " 0.5409923791885376,\n",
       " 0.47108349204063416,\n",
       " 0.5245845317840576,\n",
       " 0.5645917654037476,\n",
       " 0.3221162259578705,\n",
       " 0.37776684761047363,\n",
       " 0.3430803716182709,\n",
       " 0.5483967065811157,\n",
       " 0.5170215964317322,\n",
       " 0.667184054851532,\n",
       " 0.43781399726867676,\n",
       " 0.38977131247520447,\n",
       " 0.5992730259895325,\n",
       " 0.5618449449539185,\n",
       " 0.5010813474655151,\n",
       " 0.5146835446357727,\n",
       " 0.4427567720413208,\n",
       " 0.4747183322906494,\n",
       " 0.37675708532333374,\n",
       " 0.615854799747467,\n",
       " 0.5422664284706116,\n",
       " 0.6192213892936707,\n",
       " 0.38901886343955994,\n",
       " 0.6406282186508179,\n",
       " 0.42487236857414246,\n",
       " 0.35835912823677063,\n",
       " 0.49758055806159973,\n",
       " 0.5298001766204834,\n",
       " 0.35326674580574036,\n",
       " 0.3002491891384125,\n",
       " 0.5343644022941589,\n",
       " 0.5186408758163452,\n",
       " 0.5963103175163269,\n",
       " 0.5444173812866211,\n",
       " 0.30324485898017883,\n",
       " 0.41325679421424866,\n",
       " 0.3899131417274475,\n",
       " 0.447883665561676,\n",
       " 0.5287593007087708,\n",
       " 0.5203489065170288,\n",
       " 0.4442484676837921,\n",
       " 0.45745787024497986,\n",
       " 0.46506765484809875,\n",
       " 0.5660455822944641,\n",
       " 0.6672623753547668,\n",
       " 0.5417900681495667,\n",
       " 0.5055933594703674,\n",
       " 0.494489461183548,\n",
       " 0.2990201413631439,\n",
       " 0.6177815198898315,\n",
       " 0.7071898579597473,\n",
       " 0.5213611125946045,\n",
       " 0.4824458360671997,\n",
       " 0.4463276267051697,\n",
       " 0.4338315725326538,\n",
       " 0.39421921968460083,\n",
       " 0.7436155080795288,\n",
       " 0.4120708703994751,\n",
       " 0.5831609964370728,\n",
       " 0.3571513891220093,\n",
       " 0.5169536471366882,\n",
       " 0.4091956913471222,\n",
       " 0.49984481930732727,\n",
       " 0.49961668252944946,\n",
       " 0.5606198310852051,\n",
       " 0.4141145646572113,\n",
       " 0.5820754766464233,\n",
       " 0.47829997539520264,\n",
       " 0.4834713637828827,\n",
       " 0.3683730959892273,\n",
       " 0.32545918226242065,\n",
       " 0.41125568747520447,\n",
       " 0.4820510149002075,\n",
       " 0.7228918075561523,\n",
       " 0.4339977204799652,\n",
       " 0.42793452739715576,\n",
       " 0.4125204086303711,\n",
       " 0.5886709094047546,\n",
       " 0.5353283882141113,\n",
       " 0.4946621060371399,\n",
       " 0.4256407916545868,\n",
       " 0.7194201350212097,\n",
       " 0.5744526982307434,\n",
       " 0.4770525395870209,\n",
       " 0.47127169370651245,\n",
       " 0.6129313111305237,\n",
       " 0.47563624382019043,\n",
       " 0.3255237340927124,\n",
       " 0.5486029982566833,\n",
       " 0.27932414412498474,\n",
       " 0.47793546319007874,\n",
       " 0.36367788910865784,\n",
       " 0.4407733380794525,\n",
       " 0.3912521302700043,\n",
       " 0.43449267745018005,\n",
       " 0.5026442408561707,\n",
       " 0.4984061121940613,\n",
       " 0.6950536966323853,\n",
       " 0.5115074515342712,\n",
       " 0.3414478302001953,\n",
       " 0.5799279808998108,\n",
       " 0.35049086809158325,\n",
       " 0.4223763346672058,\n",
       " 0.42467156052589417,\n",
       " 0.4658013880252838,\n",
       " 0.49125275015830994,\n",
       " 0.45692288875579834,\n",
       " 0.7956604957580566,\n",
       " 0.6595057845115662,\n",
       " 0.3389817476272583,\n",
       " 0.5864095091819763,\n",
       " 0.44734230637550354,\n",
       " 0.3647443652153015,\n",
       " 0.2786520719528198,\n",
       " 0.34754785895347595,\n",
       " 0.4588397443294525,\n",
       " 0.47039446234703064,\n",
       " 0.36790212988853455,\n",
       " 0.5266995429992676,\n",
       " 0.35332345962524414,\n",
       " 0.36260348558425903,\n",
       " 0.5452376008033752,\n",
       " 0.547141969203949,\n",
       " 0.5088809728622437,\n",
       " 0.3297925889492035,\n",
       " 0.6945031881332397,\n",
       " 0.4974275231361389,\n",
       " 0.4790184199810028,\n",
       " 0.5055719017982483,\n",
       " 0.5459830164909363,\n",
       " 0.6557955741882324,\n",
       " 0.43200719356536865,\n",
       " 0.36306819319725037,\n",
       " 0.3856126666069031,\n",
       " 0.47221070528030396,\n",
       " 0.6622040271759033,\n",
       " 0.4094878137111664,\n",
       " 0.5457976460456848,\n",
       " 0.537591278553009,\n",
       " 0.6317370533943176,\n",
       " 0.44144272804260254,\n",
       " 0.5833161473274231,\n",
       " 0.3333356976509094,\n",
       " 0.6287989616394043,\n",
       " 0.36856886744499207,\n",
       " 0.4760568141937256,\n",
       " 0.4275364875793457,\n",
       " 0.45206546783447266,\n",
       " 0.48290908336639404,\n",
       " 0.552014172077179,\n",
       " 0.4900091290473938,\n",
       " 0.6298075318336487,\n",
       " 0.43224915862083435,\n",
       " 0.474138081073761,\n",
       " 0.46882694959640503,\n",
       " 0.5115846395492554,\n",
       " 0.46241575479507446,\n",
       " 0.5582976341247559,\n",
       " 0.7393835186958313,\n",
       " 0.5854877233505249,\n",
       " 0.7308252453804016,\n",
       " 0.35488390922546387,\n",
       " 0.5106497406959534,\n",
       " 0.5377414226531982,\n",
       " 0.4568774998188019,\n",
       " 0.35691481828689575,\n",
       " 0.36474695801734924,\n",
       " 0.5444178581237793,\n",
       " 0.606723427772522,\n",
       " 0.5095707178115845,\n",
       " 0.44275474548339844,\n",
       " 0.4404321312904358,\n",
       " 0.2862933278083801,\n",
       " 0.41171735525131226,\n",
       " 0.4005869925022125,\n",
       " 0.4490157961845398,\n",
       " 0.5113996267318726,\n",
       " 0.4504644274711609,\n",
       " 0.5637762546539307,\n",
       " 0.4140593111515045,\n",
       " 0.473567932844162,\n",
       " 0.5023690462112427,\n",
       " 0.4185484051704407,\n",
       " 0.3658551573753357,\n",
       " 0.5043107867240906,\n",
       " 0.42836326360702515,\n",
       " 0.5165241956710815,\n",
       " 0.48636019229888916,\n",
       " 0.3300377428531647,\n",
       " 0.45724377036094666,\n",
       " 0.5724586248397827,\n",
       " 0.31188279390335083,\n",
       " 0.47631722688674927,\n",
       " 0.5797807574272156,\n",
       " 0.5011109709739685,\n",
       " 0.3826296031475067,\n",
       " 0.39990583062171936,\n",
       " 0.3091506361961365,\n",
       " 0.5067511796951294,\n",
       " 0.569187343120575,\n",
       " 0.3941277265548706,\n",
       " 0.3549768924713135,\n",
       " 0.573614776134491,\n",
       " 0.3442952632904053,\n",
       " 0.5604662299156189,\n",
       " 0.5607670545578003,\n",
       " 0.3449418842792511,\n",
       " 0.32130181789398193,\n",
       " 0.33780524134635925,\n",
       " 0.5732248425483704,\n",
       " 0.5361781120300293,\n",
       " 0.4846652150154114,\n",
       " 0.4963615834712982,\n",
       " 0.4702480137348175,\n",
       " 0.371541291475296,\n",
       " 0.5128210186958313,\n",
       " 0.30624091625213623,\n",
       " 0.5374377965927124,\n",
       " 0.30810198187828064,\n",
       " 0.571828305721283,\n",
       " 0.5143163204193115,\n",
       " 0.7094537019729614,\n",
       " 0.5345267653465271,\n",
       " 0.4781380891799927,\n",
       " 0.5640968680381775,\n",
       " 0.7335131168365479,\n",
       " 0.5154701471328735,\n",
       " 0.37406831979751587,\n",
       " 0.6656996607780457,\n",
       " 0.4845006763935089,\n",
       " 0.6666682362556458,\n",
       " 0.3717981278896332,\n",
       " 0.655343770980835,\n",
       " 0.4093342125415802,\n",
       " 0.37680596113204956,\n",
       " 0.5698664784431458,\n",
       " 0.3853408694267273,\n",
       " 0.3655678629875183,\n",
       " 0.5215906500816345,\n",
       " 0.4038960337638855,\n",
       " 0.5440489649772644,\n",
       " 0.41820964217185974,\n",
       " 0.5703615546226501,\n",
       " 0.5073406100273132,\n",
       " 0.488523006439209,\n",
       " 0.3542638123035431,\n",
       " 0.3857002258300781,\n",
       " 0.4838520288467407,\n",
       " 0.34185224771499634,\n",
       " 0.6137974858283997,\n",
       " 0.5067750215530396,\n",
       " 0.33244186639785767,\n",
       " 0.5107055902481079,\n",
       " 0.6014596223831177,\n",
       " 0.4247830808162689,\n",
       " 0.69422847032547,\n",
       " 0.4243485629558563,\n",
       " 0.5671262145042419,\n",
       " 0.4211363196372986,\n",
       " 0.31346556544303894,\n",
       " 0.5703425407409668,\n",
       " 0.6150219440460205,\n",
       " 0.39214998483657837,\n",
       " 0.5507376194000244,\n",
       " 0.3973260223865509,\n",
       " 0.629082202911377,\n",
       " 0.5476996302604675,\n",
       " 0.5868304371833801,\n",
       " 0.49592429399490356,\n",
       " 0.46906280517578125,\n",
       " 0.6404608488082886,\n",
       " 0.462265282869339,\n",
       " 0.6407219767570496,\n",
       " 0.44474175572395325,\n",
       " 0.35370853543281555,\n",
       " 0.4532151222229004,\n",
       " 0.4803481996059418,\n",
       " 0.5634186863899231,\n",
       " 0.4307241141796112,\n",
       " 0.3537396192550659,\n",
       " 0.6368380188941956,\n",
       " 0.41516202688217163,\n",
       " 0.46469932794570923,\n",
       " 0.4447333812713623,\n",
       " 0.35040366649627686,\n",
       " 0.6054646968841553,\n",
       " 0.3812752068042755,\n",
       " 0.44078418612480164,\n",
       " 0.4297794997692108,\n",
       " 0.3645167648792267,\n",
       " 0.766503095626831,\n",
       " 0.3878409266471863,\n",
       " 0.68592768907547,\n",
       " 0.5529680252075195,\n",
       " 0.5111913084983826,\n",
       " 0.48649343848228455,\n",
       " 0.41331371665000916,\n",
       " 0.4281669855117798,\n",
       " 0.5101661086082458,\n",
       " 0.4329894781112671,\n",
       " 0.514079749584198,\n",
       " 0.4345385730266571,\n",
       " 0.4527687132358551,\n",
       " 0.4426828622817993,\n",
       " 0.574707567691803,\n",
       " 0.45780861377716064,\n",
       " 0.3800050616264343,\n",
       " 0.4175263047218323,\n",
       " 0.5332622528076172,\n",
       " 0.3432597219944,\n",
       " 0.6041436195373535,\n",
       " 0.5525495409965515,\n",
       " 0.455746591091156,\n",
       " 0.36117833852767944,\n",
       " 0.5181590914726257,\n",
       " 0.44254863262176514,\n",
       " 0.4469457268714905,\n",
       " 0.5670697689056396,\n",
       " 0.45014697313308716,\n",
       " 0.42141711711883545,\n",
       " 0.44111475348472595,\n",
       " 0.4503217935562134,\n",
       " 0.5062888264656067,\n",
       " 0.5146937370300293,\n",
       " 0.4407871663570404,\n",
       " 0.5583961009979248,\n",
       " 0.3575509488582611,\n",
       " 0.4841821491718292,\n",
       " 0.48509645462036133,\n",
       " 0.4570765495300293,\n",
       " 0.4345850348472595,\n",
       " 0.5795782208442688,\n",
       " 0.5273579955101013,\n",
       " 0.3463820219039917,\n",
       " 0.40367624163627625,\n",
       " 0.5014209151268005,\n",
       " 0.48928990960121155,\n",
       " 0.43541112542152405,\n",
       " 0.567064642906189,\n",
       " 0.512800931930542,\n",
       " 0.3600621223449707,\n",
       " 0.5338070392608643,\n",
       " 0.5355945229530334,\n",
       " 0.5681101679801941,\n",
       " 0.49562132358551025,\n",
       " 0.39302438497543335,\n",
       " 0.5521939992904663,\n",
       " 0.32414335012435913,\n",
       " 0.6128905415534973,\n",
       " 0.4992113411426544,\n",
       " 0.29660147428512573,\n",
       " 0.37357935309410095,\n",
       " 0.48773929476737976,\n",
       " 0.4284665286540985,\n",
       " 0.5240582823753357,\n",
       " 0.5185225009918213,\n",
       " 0.4086920917034149,\n",
       " 0.37556371092796326,\n",
       " 0.4837350845336914,\n",
       " 0.5146380662918091,\n",
       " 0.5045692324638367,\n",
       " 0.3902055621147156,\n",
       " 0.5529839992523193,\n",
       " 0.3714330196380615,\n",
       " 0.45266732573509216,\n",
       " 0.572087824344635,\n",
       " 0.5525994300842285,\n",
       " 0.4261147677898407,\n",
       " 0.4649515151977539,\n",
       " 0.5810307264328003,\n",
       " 0.42330992221832275,\n",
       " 0.46795159578323364,\n",
       " 0.620208203792572,\n",
       " 0.36453330516815186,\n",
       " 0.6135960221290588,\n",
       " 0.46737903356552124,\n",
       " 0.5127886533737183,\n",
       " 0.3606736660003662,\n",
       " 0.42969703674316406,\n",
       " 0.3828348219394684,\n",
       " 0.4460301995277405,\n",
       " 0.37014076113700867,\n",
       " 0.6042566299438477,\n",
       " 0.45125502347946167,\n",
       " 0.4011911153793335,\n",
       " 0.3751318156719208,\n",
       " 0.44018781185150146,\n",
       " 0.6195250749588013,\n",
       " 0.42913731932640076,\n",
       " 0.6845526099205017,\n",
       " 0.4941870868206024,\n",
       " 0.580068826675415,\n",
       " 0.48406529426574707,\n",
       " 0.33610087633132935,\n",
       " 0.3978126049041748,\n",
       " 0.3658856749534607,\n",
       " 0.3626880645751953,\n",
       " 0.6467792391777039,\n",
       " 0.6740327477455139,\n",
       " 0.5626190304756165,\n",
       " 0.5869983434677124,\n",
       " 0.49434441328048706,\n",
       " 0.5969392657279968,\n",
       " 0.498004287481308,\n",
       " 0.6503063440322876,\n",
       " 0.552101731300354,\n",
       " 0.6416367888450623,\n",
       " 0.3485372066497803,\n",
       " 0.3982074558734894,\n",
       " 0.48465174436569214,\n",
       " 0.4886910021305084,\n",
       " 0.5667884945869446,\n",
       " 0.5182270407676697,\n",
       " 0.556560754776001,\n",
       " 0.4023389220237732,\n",
       " 0.4790104329586029,\n",
       " 0.42077547311782837,\n",
       " 0.3826180100440979,\n",
       " 0.6120514869689941,\n",
       " 0.39342641830444336,\n",
       " 0.4631095230579376,\n",
       " 0.5112987756729126,\n",
       " 0.516045868396759,\n",
       " 0.4123908281326294,\n",
       " 0.3091356158256531,\n",
       " 0.4413085877895355,\n",
       " 0.5649053454399109,\n",
       " 0.3665773570537567,\n",
       " 0.46076565980911255,\n",
       " 0.5436713695526123,\n",
       " 0.5686772465705872,\n",
       " 0.4963303804397583,\n",
       " 0.3699915409088135,\n",
       " 0.49559903144836426,\n",
       " 0.5443205237388611,\n",
       " 0.6121431589126587,\n",
       " 0.6223723292350769,\n",
       " 0.4850688576698303,\n",
       " 0.4664953052997589,\n",
       " 0.46410226821899414,\n",
       " 0.6389996409416199,\n",
       " 0.6615121364593506,\n",
       " 0.48613643646240234,\n",
       " 0.2719896137714386,\n",
       " 0.3992326855659485,\n",
       " 0.3629639148712158,\n",
       " 0.4324389398097992,\n",
       " 0.47722184658050537,\n",
       " 0.42174577713012695,\n",
       " 0.286300927400589,\n",
       " 0.47614458203315735,\n",
       " 0.4026944637298584,\n",
       " 0.5541040897369385,\n",
       " 0.40045520663261414,\n",
       " 0.32511791586875916,\n",
       " 0.36967065930366516,\n",
       " 0.43303996324539185,\n",
       " 0.4686436355113983,\n",
       " 0.5440062284469604,\n",
       " 0.6840395927429199,\n",
       " 0.4281998872756958,\n",
       " 0.38039344549179077,\n",
       " 0.4529475271701813,\n",
       " 0.408443421125412,\n",
       " 0.5896077752113342,\n",
       " 0.45710811018943787,\n",
       " 0.313091516494751,\n",
       " 0.4395320415496826,\n",
       " 0.5418466925621033,\n",
       " 0.3738343417644501,\n",
       " 0.5343104600906372,\n",
       " 0.37737321853637695,\n",
       " 0.5138221383094788,\n",
       " 0.40689486265182495,\n",
       " 0.5949280261993408,\n",
       " 0.45625758171081543,\n",
       " 0.365973562002182,\n",
       " 0.550150454044342,\n",
       " 0.6443988084793091,\n",
       " 0.49458253383636475,\n",
       " 0.5020133256912231,\n",
       " 0.5330950021743774,\n",
       " 0.471879243850708,\n",
       " 0.5310588479042053,\n",
       " 0.527550995349884,\n",
       " 0.36717689037323,\n",
       " 0.3687688410282135,\n",
       " 0.5403913855552673,\n",
       " 0.502622663974762,\n",
       " 0.6643248796463013,\n",
       " 0.5062710642814636,\n",
       " 0.49505844712257385,\n",
       " 0.5169611573219299,\n",
       " 0.37876713275909424,\n",
       " 0.4553883373737335,\n",
       " 0.3005541265010834,\n",
       " 0.49982520937919617,\n",
       " 0.5377339124679565,\n",
       " 0.6039736270904541,\n",
       " 0.4513782560825348,\n",
       " 0.3795076608657837,\n",
       " 0.5404289364814758,\n",
       " 0.5457022190093994,\n",
       " 0.3183385133743286,\n",
       " 0.5401498079299927,\n",
       " 0.4726942181587219,\n",
       " 0.5044503808021545,\n",
       " 0.34055671095848083,\n",
       " 0.5188065767288208,\n",
       " 0.544939398765564,\n",
       " 0.2758907079696655,\n",
       " 0.36993208527565,\n",
       " 0.5902609825134277,\n",
       " 0.3222181797027588,\n",
       " 0.49355706572532654,\n",
       " 0.7330254316329956,\n",
       " 0.366614431142807,\n",
       " 0.44381260871887207,\n",
       " 0.5050932765007019,\n",
       " 0.6235359311103821,\n",
       " 0.5741547346115112,\n",
       " 0.4552144408226013,\n",
       " 0.6219666600227356,\n",
       " 0.42244604229927063,\n",
       " 0.3952455520629883,\n",
       " 0.5873668789863586,\n",
       " 0.5021404027938843,\n",
       " 0.47593870759010315,\n",
       " 0.60133957862854,\n",
       " 0.4622946083545685,\n",
       " 0.643612802028656,\n",
       " 0.46257245540618896,\n",
       " 0.5323859453201294,\n",
       " 0.5640857219696045,\n",
       " 0.6197994351387024,\n",
       " 0.6744579672813416,\n",
       " 0.5702652335166931,\n",
       " 0.4901038706302643,\n",
       " 0.37990331649780273,\n",
       " 0.734129786491394,\n",
       " 0.41428709030151367,\n",
       " 0.6378252506256104,\n",
       " 0.39698028564453125,\n",
       " 0.6151213645935059,\n",
       " 0.5963637232780457,\n",
       " 0.5968160033226013,\n",
       " 0.5033719539642334,\n",
       " 0.35815131664276123,\n",
       " 0.5628662705421448,\n",
       " 0.5675444006919861,\n",
       " 0.619104266166687,\n",
       " 0.4538188576698303,\n",
       " 0.44879817962646484,\n",
       " 0.3056941330432892,\n",
       " 0.6884006261825562,\n",
       " 0.5015074014663696,\n",
       " 0.5198397040367126,\n",
       " 0.5570706725120544,\n",
       " 0.3083552420139313,\n",
       " 0.3416418433189392,\n",
       " 0.33180883526802063,\n",
       " 0.5276483297348022,\n",
       " 0.6108646988868713,\n",
       " 0.42401859164237976,\n",
       " 0.5160424113273621,\n",
       " 0.4407668113708496,\n",
       " 0.41319960355758667,\n",
       " 0.5303632020950317,\n",
       " 0.4605806767940521,\n",
       " 0.40886804461479187,\n",
       " 0.3749459981918335,\n",
       " 0.581721305847168,\n",
       " 0.39938223361968994,\n",
       " 0.6065878868103027,\n",
       " 0.4004143476486206,\n",
       " 0.5661070346832275,\n",
       " 0.36249810457229614,\n",
       " 0.4549902677536011,\n",
       " 0.4794885516166687,\n",
       " 0.4849458336830139,\n",
       " 0.5013995170593262,\n",
       " 0.48219817876815796,\n",
       " 0.5271888375282288,\n",
       " 0.49269238114356995,\n",
       " 0.42114779353141785,\n",
       " 0.4517812430858612,\n",
       " 0.6957788467407227,\n",
       " 0.4721742570400238,\n",
       " 0.5657141208648682,\n",
       " 0.5731568336486816,\n",
       " 0.5675286054611206,\n",
       " 0.7995383739471436,\n",
       " 0.2981487214565277,\n",
       " 0.3212307095527649,\n",
       " 0.46568867564201355,\n",
       " 0.2664477527141571,\n",
       " 0.5482481718063354,\n",
       " 0.4113962948322296,\n",
       " 0.4690627455711365,\n",
       " 0.4406130909919739,\n",
       " 0.49252814054489136,\n",
       " 0.3035338521003723,\n",
       " 0.5079105496406555,\n",
       " 0.39697277545928955,\n",
       " 0.48358699679374695,\n",
       " 0.5714964866638184,\n",
       " 0.4779522120952606,\n",
       " 0.56048983335495,\n",
       " 0.3369540572166443,\n",
       " 0.57133549451828,\n",
       " 0.6303783059120178,\n",
       " 0.45823022723197937,\n",
       " 0.3719894289970398,\n",
       " 0.5190213322639465,\n",
       " 0.4450981616973877,\n",
       " 0.38567787408828735,\n",
       " 0.5753567218780518,\n",
       " 0.3320164382457733,\n",
       " 0.5092523694038391,\n",
       " 0.34539738297462463,\n",
       " 0.4486556053161621,\n",
       " 0.38736528158187866,\n",
       " 0.5959349870681763,\n",
       " 0.42203131318092346,\n",
       " 0.641222357749939,\n",
       " 0.41916096210479736,\n",
       " 0.38919347524642944,\n",
       " 0.4567662179470062,\n",
       " 0.5717602372169495,\n",
       " 0.426963210105896,\n",
       " 0.4796726107597351,\n",
       " 0.4481469988822937,\n",
       " 0.33952662348747253,\n",
       " 0.5107961893081665,\n",
       " 0.37843355536460876,\n",
       " 0.5831871032714844]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'weights_only' is an invalid keyword argument for Unpickler()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-279bcfa29f39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'weights_only' is an invalid keyword argument for Unpickler()"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_unimplemented',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'flatten',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'linear_relu_stack',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_unimplemented',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'ignore_index',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'reduction',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
