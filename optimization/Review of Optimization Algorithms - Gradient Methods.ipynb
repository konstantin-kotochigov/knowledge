{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization Algorithms\n",
    "\n",
    "- Gradient descent methods\n",
    "- Gradient descent methods\n",
    "- Newton and Quasi-newton methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T04:44:27.178670Z",
     "start_time": "2021-01-05T04:44:27.169268Z"
    }
   },
   "source": [
    "# Gradient methods\n",
    "\n",
    "In ML the default approach to optimization is gradient descent.\n",
    "\n",
    "Described by Cauchy 1847\n",
    "\n",
    "Good analogy - fog in the mountains. You need to reach the summit, but you see only in radius of 10 meters.\n",
    "\n",
    "In simple terms GD algorithm does the following: \n",
    "1. chooses the starting point\n",
    "2. computes gradient - the direction of the steepest descent\n",
    "3. updates current location\n",
    "4. if update is insignificantly small => stop\n",
    "5. goto 2\n",
    "\n",
    "Suppose we want to minimize $f(\\theta)$, where $\\theta = (\\theta_1, \\theta_2 \\cdots \\theta_n)$. We choose some initial  approximation $\\theta_0$ and repeat the update process:\n",
    "\n",
    "$$\\theta_{t} = \\theta_{t-1} + \\Delta_t$$\n",
    "where\n",
    "$$\\Delta_t = - \\eta \\cdot grad(\\theta_{t-1})$$\n",
    "\n",
    "NOTE: if we need to maximize $f(\\theta)$ instead of minimization, we will <u>add</u> the gradient $\\Delta_t = \\eta \\cdot grad(\\theta_{t-1})$\n",
    "\n",
    "### Variations of GD\n",
    "\n",
    "There are 3 flavors of the Gradient Descend algorithm:\n",
    "\n",
    "|||\n",
    "| --- | --- |\n",
    "|**Batch**| calculates gradients for each data sample and combines them into one big update |\n",
    "|**Stochastic**| updates for each data sample |\n",
    "|**Minibatch**| constructs small minibatch updates and apply them |\n",
    "\n",
    "Here is the illustrsation of convergence of these 3 alogrithms:\n",
    "<img src= \"gd_variants.png\" width=500>\n",
    "\n",
    "## Modifications of GD\n",
    "\n",
    "### SGD with Momentum\n",
    "\n",
    "Suppose we do standard Gradient Descent:\n",
    "$$\\theta_{t} = \\theta_{t-1} + \\Delta$$\n",
    "\n",
    "We want our updates \n",
    "\n",
    "$$\\Delta = - \\eta \\cdot grad(\\theta_{t})$$\n",
    "\n",
    "to be less susceptible to small arbitrary changes in direction. That's why we add momentum - some rate ($\\gamma$) of previous movement.\n",
    "\n",
    "$$\\Delta = \\gamma grad(\\theta_{t-1}) + \\eta grad(\\theta_{t})$$\n",
    "\n",
    "Physical analogy - moving objects do not stop immediately when they need to change the direction, they continue movment for some time.\n",
    "\n",
    "Advantages of momentum can be easily seen in case of \"ravines\" when locally large gradients give wrong direction. Here momentum gives more weight to another dimension whose gradients are more stable => convergence is faster.\n",
    "\n",
    "<img src = \"momentum.png\" width = 500>\n",
    "\n",
    "### Nesterov Accelerated Momentum\n",
    "We do SGD with momentum but slightly modify the order of execution:\n",
    "1. first we follow the momentum from previous step\n",
    "2. then we add current gradient as a correction\n",
    "\n",
    "This gives us the following formula:\n",
    "$$\\theta_{t} = \\theta_{t-1} + \\gamma \\cdot grad(\\theta_{t-1}) + \\eta \\cdot grad(\\theta_{t-1} - \\gamma \\cdot grad_{t-1})$$\n",
    "\n",
    "[arxiv (2013)](http://proceedings.mlr.press/v28/sutskever13.pdf)\n",
    "\n",
    "### Adagrad\n",
    "We would like to define some schedule of decrease for learning rate $\\eta$. \n",
    "Here we do it by simply adding a normalizer that divides learning rate by root of sum of accumulated squared gradients\n",
    "$$\\theta_{t} = \\theta_{t-1} + \\frac{\\eta}{\\sqrt{\\sum_t {grad}^2_t + \\epsilon}} \\cdot grad(\\theta_t)$$\n",
    "\n",
    "Note that denominators are different for each agrument. So in formula above *grad* is actually a diagonal matrix.\n",
    "\n",
    "$\\epsilon$ here just prevents from division by zero\n",
    "\n",
    "[arxiv (2011)](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    "\n",
    "### Adadelta\n",
    "The proposed in Adagrad decrease tends to be too radical. We would like to make it more gradual. \n",
    "So we modify Adagrad by by changing sum of accumulated gradients to sliding average of accumulated gradients.\n",
    "We will refer to this sliding average as $RMS[grad(\\theta_{t-1})]$\n",
    "\n",
    "$$\\theta_{t} = \\theta_{t-1} + \\frac{\\eta}{RMS[grad(\\theta_{t-1})]} \\cdot grad(\\theta_t)$$\n",
    "\n",
    "[arxiv (2012)](https://arxiv.org/abs/1212.5701)\n",
    "\n",
    "### RMSprop\n",
    "This method is almost the same as Adadelta\n",
    "$$\\theta_{t} = \\theta_{t-1} + \\frac{\\eta}{RMS[grad(\\theta_{t-1})]} \\cdot grad(\\theta_t)$$\n",
    "It stems from rprop\n",
    "\n",
    "[arxiv (2015)](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
    "\n",
    "### Adam\n",
    "Now we would like to add momentum to RMSProp like we did in earlier algorithms. So we add (exponential) momentum - accumulated gradient\n",
    "\n",
    "[arxiv(2014)](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "$$\\theta_{t} = \\theta_{t-1} + \\frac{\\eta}{RMS[grad(\\theta_{t-1})]} \\cdot (\\gamma \\cdot grad_{t-1} + (1-\\gamma) \\cdot grad_{t})$$\n",
    "\n",
    "### Adamax\n",
    "As a slight modification to Adam algorithm we allow usage of arbitrary $L^p$ metrics for gradient momentum.\n",
    "\n",
    "[arxiv (2014)](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "\n",
    "### Nadam\n",
    "\n",
    "Why not switching from momentum in Adam algorithm to accelerated Nesterov momentum\n",
    "\n",
    "[arxiv (2015)](http://cs229.stanford.edu/proj2015/054_report.pdf)\n",
    "\n",
    "## What  algorithm to use\n",
    "\n",
    "Currently (2021) Adam algorithm is considered the state-of-the-art approach and is recommended as a deafult algorithm for most deep learning applications.\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"optimization.png\" width = 350>\n",
    "\n",
    "Algorithm parameters:\n",
    "- $\\eta$ - initial learning rate\n",
    "- $\\beta_1$ - momentum retention rate\n",
    "- $\\beta_2$ - controls the level of $\\eta$ decay\n",
    "- $\\epsilon$ - prevents division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
