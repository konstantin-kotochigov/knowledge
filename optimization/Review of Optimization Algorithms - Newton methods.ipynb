{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton methods\n",
    "\n",
    "Newton-Raphson algorithm (1669) is an algorithm for finding roots of a function. \n",
    "\n",
    "On each iteration the estimate can be computed by setting the dervivate (tangent line) to zero:\n",
    "\n",
    "$$x_{k+1} = x_{k} - \\frac{f(x_k)}{f'(x_k)}$$\n",
    "\n",
    "But it's equally used in optimization problems. The necessary conditions for function optimum is that the derivative equals zero. So we apply the root-finding algorithm for the derviative and get the following expression:\n",
    "\n",
    "$$x_{k+1} = x_{k} - \\frac{f'(x_k)}{f''(x_k)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-dimensional Newton for optimization\n",
    "Let's decompose function $f(x)$ in its Taylor series (up to the second derviative)\n",
    "\n",
    "$$f(x+t) \\approx f(x) + f'(x) \\cdot t + \\frac{1}{2}f''(x) \\cdot t^2$$\n",
    "\n",
    "We approximated the function f(x) with a 2-degree polynomial.\n",
    "\n",
    "Now let's find the optimum of this polynomial - compute its derviative (with respect to **t**) and set it to zero:\n",
    "\n",
    "$$\\frac{\\partial f(x+t)}{\\partial t} = f'(x) + f''(x) = 0 $$\n",
    "\n",
    "The solution gives us a value for t that maximizes:\n",
    "\n",
    "$$t = -\\frac{f'(x)}{f''(x)} $$\n",
    "\n",
    "So, the update rule $x_{k+1} := x_k + t$ becomes the following:\n",
    "\n",
    "$$x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)} $$\n",
    "\n",
    "<img src=\"newton_scheme.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional Newton for optimization\n",
    "\n",
    "When $f(\\hat{x}) = f(x_1,x_2 ... x_n)$ is a multidimensional function:\n",
    "- Derviative $f'(x)$ becomes gradient $\\nabla f(x)$ - vector of first derviatives\n",
    "- Second derivative $f''(x)$ becomes Hessisan $\\nabla^2$ - matrix of partial derviatives\n",
    "\n",
    "$$x_{k+1} = x_k + \\nabla {f(x)} \\cdot [\\nabla^2 f(x)]^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quasi-Newton methods\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Quasi-newton methods are approximate Newton methods: \n",
    "\n",
    "1. They does not require exact computation of a Hessian matrix on each step. Instead they use B - approximation for matrix $\\nabla^2$ which is updated iteratively using current step information.\n",
    "\n",
    "\n",
    "2. Besides [Sherman-Morisson](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula) formulas allow efficient computation of B inverse => those methods are even more efficient.\n",
    "\n",
    "**NOTE** [Hessians are symmetric](https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives#Schwarz's_theorem) => B should be symmetric too\n",
    "\n",
    "\n",
    "\n",
    "Let's repeat the 2nd order Taylor expansion from Newton method\n",
    "\n",
    "$$f(x_k + \\Delta x) \\approx f(x_k) + \\nabla f(x_k)^T \\cdot \\Delta x + \\Delta x^{T} \\cdot B \\cdot \\Delta x$$\n",
    "\n",
    "The gradient of this approximation will give us\n",
    "\n",
    "$$\\frac{\\partial f(x_k + \\Delta x)}{\\partial \\Delta x} \\approx \\nabla f(x_k) + B \\cdot \\Delta x$$\n",
    "\n",
    "Setting this gradient to zero we get Newton update step\n",
    "$$\\Delta x = -B^{-1} \\nabla f(x_k)$$\n",
    "\n",
    "How to update B without calculating it?\n",
    "\n",
    "## Overview of popular implementations\n",
    "- DFP (1959)\n",
    "- BFGS (1970)\n",
    "- BHHH (1974)\n",
    "- L-BFGS (1989) = limited memory variant of BFGS\n",
    "- SR1 (1991)\n",
    "\n",
    "## DFP\n",
    "\n",
    "Davidon–Fletcher–Powell (1959)\n",
    "\n",
    "## BFGS\n",
    "\n",
    "Broyden–Fletcher–Goldfarb–Shanno (1970)\n",
    "\n",
    "Algorithm:\n",
    "0. Set initial $B_0$\n",
    "\n",
    "Loop until convergence\n",
    "\n",
    "\n",
    "1. Compute the optimal direction using the equation from Newton method: $B \\Delta x = - \\nabla f(x)$\n",
    "\n",
    "\n",
    "2. Choose $\\alpha$ the optimal step size in that direction\n",
    "\n",
    "    $\\alpha = argmin(f(x_k + \\alpha \\Delta x))$\n",
    "    \n",
    "    This is usually done via some basic line search algorithm\n",
    "\n",
    "\n",
    "3. Make a step\n",
    "\n",
    "    $x_{k-1} = x_k + \\alpha \\Delta x_k$\n",
    "\n",
    "\n",
    "4. Compute the change in gradient\n",
    "\n",
    "    $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_k)$\n",
    "\n",
    "\n",
    "5. Update Hessian using $y$ and $s$\n",
    "\n",
    "    $B_{k+1} = B_k + \\frac{yy^T}{yTs} + \\frac{Bss^TB^T}{s^TBs}$\n",
    "\n",
    "#### Complexity\n",
    "Does not require matrix inversion => it's complexity is $O(n^2)$ compared to $O(n^3)$ for Newton methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T22:09:41.335972Z",
     "start_time": "2021-01-10T22:09:41.295401Z"
    }
   },
   "source": [
    "### Newton vs Gradient methods\n",
    "Newton methods are <u>second order</u> mtehods - they require computation of second derivatives.\n",
    "Gradient methods require only first derviatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Adam and L-BGFS\n",
    "\n",
    "(from stackexchange)\n",
    "\n",
    "Those are two best methods in two different classes of approaches: gradient descent and quasi-netwon methods.\n",
    "\n",
    "An L-BFGS solver is a true quasi-Newton method in that it estimates the curvature of the parameter space via an approximation of the Hessian. So if your parameter space has plenty of long, nearly-flat valleys then L-BFGS would likely perform well. It has the downside of additional costs in performing a rank-two update to the (inverse) Hessian approximation at every step. While this is reasonably fast, it does begin to add up, particularly as the input space grows. This may account for the fact that ADAM outperforms L-BFGS for you as you get more data.\n",
    "\n",
    "ADAM is a first order method that attempts to compensate for the fact that it doesn't estimate the curvature by adapting the step-size in every dimension. In some sense, this is similar to constructing a diagonal Hessian at every step, but they do it cleverly by simply using past gradients. In this way it is still a first order method, though it has the benefit of acting as though it is second order. The estimate is cruder than that of the L-BFGS in that it is only along each dimension and doesn't account for what would be the off-diagonals in the Hessian. If your Hessian is nearly singular then these off-diagonals may play an important role in the curvature and ADAM is likely to underperform relative the BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scipy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
