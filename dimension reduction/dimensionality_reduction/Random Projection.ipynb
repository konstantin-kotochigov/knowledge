{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Projection (2001)\n",
    "\n",
    "The next method is called Random Projection and that's probabliy the simplest approach in the list. It's extremely useful when number of dimensions is large. In that case PCA is extremely hard to perform. And it turns out that we can define new dimensions simply as a random linear combination of other dimensions. And surprisingly, it works quite well. \n",
    "\n",
    "Cool feature about it - is that is you don't even need to have access to data to construct such projection. Which means it's extremely fast.\n",
    "\n",
    "There is also a little bit of a theoretical ground here. We actually can say how much we need.\n",
    "\n",
    "There is very similar algorithm called Locality Sensitive Hasing, which is quite popular in ML. It utilizes the same idea - we describe data with a number of random projections, in this case random hash functions. But it has slightly different application - it's used mainly for finding nearest neighbors in high-dimensional spaces, not dimensionality reduction (?), so I'm gonna spent much time on it in his video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
