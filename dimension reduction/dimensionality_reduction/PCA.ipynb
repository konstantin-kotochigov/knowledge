{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "PCA is one of the oldest methods. It was introduced in the early 1900s by Fisher (who is considered the founder of modern statistics). And what it actually does, it seeks for a linear map, that projects data onto some 2D hyperplane in such a way that the projected data retains its original structure as much as possible.\n",
    "\n",
    "So how this idea is formulated. It turns out that there are several equivalent settings:\n",
    "- First one \n",
    "\n",
    "  we seek for such a linear map, that the distance between orignal points and their projections is as low as possible\n",
    "\n",
    "\n",
    "- Second setting \n",
    "        \n",
    "  we seek for such a linear map, that variance of projected data is  retained as high as possible\n",
    "\n",
    "\n",
    "- And the third setting \n",
    "\n",
    "  we seek for such a liner map, that correlations between new axes is low as possible\n",
    "\n",
    "All three settings are equivalent and lead to the same result.\n",
    "\n",
    "Top-R отобранных собственных векторов - и есть новый базис, на который проецируется выборка.\n",
    "How do we solve it? Supose we need to project m-dimensional data to a new k-dimensional space. \n",
    "The most popular solution is based on eigendecomposition and can be described in following 3 steps:\n",
    "We describe our dataset by computing its covariance matrix\n",
    "We form a new basis - the basis of eigenvectors of this matrix\n",
    "We choose first k dimensions of this basis and describe dataset in terms  \n",
    "\n",
    "Let's dive a bit deeper. The concept of eigendecomposition is hugely used in machine learning and it’s definitely worth understanding.\n",
    "\n",
    "We need to give a bit more detail on what eigendecomposition actually is.\n",
    "Briefly, eigen vectors are the vetors that pertain their direction when being multiplied by the matrix. The number of such eigen vectors (coefficient) is the same as the dimensionality of the space and those vectors have a very important feature - besides they form an othogonal basis, they can be sorted in descending order by their eigen values, that is by the variance each new vector conveys to the dataset. This means that by choosing k - a number of vectors, we can actually get any desired precision of matrix approximation.\n",
    "\n",
    "It’s also important to note that dimensions of low-dimensional space are non-interpretable. They are just linear combinations of the whole bunch of original dimensions and in general, the lower the chosen dimensionaly, the less meaningful new axes would be. Although there are some ways to describe the axes, but the main purpose of PCA is to pertain the structure of the data, not to meaningfully describe it. It’s also important to understand.\n",
    "\n",
    "Another question - how do we choose the dimensionality of low-dimensional space? As always, it largely depends on a task, but there is a couple of ways, how we can actually do it.\n",
    "One way is to plot a data variance and to see how exactly it increases when we increase the number of dimensions. Or, alternatively, we can plot a marginal utility of each dimension and see what rate of variation each new dimension brings in. So, for example, since we determine what number of dimensions gives us 80% of original data variation, we can use this number to project the data onto this new space.\n",
    "\n",
    "What are drawback of using PCA? Well, whlie PCA performs pretty well when data is distributed according to normal distribution (when data is like blobs), it definitely poorly tracks a structure of more sophisticated manifolds, such as, for example, spirals. \n",
    "\n",
    "That is why there is a number of newer and more powerful methods, which we gonna talk about.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "\n",
    "## Sparse PCA\n",
    "PCA generates a new basis as linear combinations of original features and those combinations could be quite large (if we work in high-dimensional space). Most of them don't hold much information so we might want to define each component as a combination of just a few features. To do so we need to add some kind of regularizer to the task.\n",
    "\n",
    "\n",
    "\n",
    "Local methods work on the same principle. We somehow describe the local neighborhood (local geometry) of a point and look for such a projection that would retain this description in a new space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
