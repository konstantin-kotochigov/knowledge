{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (2013)\n",
    "\n",
    "Thomas Mikolov invented a convenient way to vectorize words in NLP using neural networks. It revolutionized NLP, hugely popularized the term \"embedding\" and motivated a large number of successor algorithms in both NLP and other domains (sentence2vec, graph2vec etc).\n",
    "\n",
    "\n",
    "\n",
    "### Problem setting\n",
    "\n",
    "Suppose we have some text data. We want to use those words in ML models => we need a way to convert each a word into some vector.\n",
    "\n",
    "Naive approach would be just to enumerate words using dummy encoding\n",
    "- (1,0,0)\n",
    "- (0,1,0)\n",
    "- (0,0,1)\n",
    "\n",
    "And for a long time it was a stadard approach.\n",
    "\n",
    "Why it is bad? Because it's very inefficient:\n",
    "- it requires huge amount of memory\n",
    "- it does not take into account semantical topology of those words\n",
    "\n",
    "We need a smaller feature space.\n",
    "\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "Embedding is a process of \"squeezing\" a data point into space of significantly smaller dimensionality so that \"similar\" in some sense data points would remain close to each other in a new space. In case of Word2Vec data point is some word and two words are similar if they have similar meaning (often used in the same context).\n",
    "\n",
    "The reduced space is often refered to as \"latent\" or \"semantic\".\n",
    "\n",
    "**Latent** becasue you can't observe it directly, you can only deduct it. \n",
    "\n",
    "**Semantic** because it's used to describe the content of the word, not its visual form - for example synonyms would have very similar descriptions.\n",
    "\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "In basic Word2vec implementation embeddings are \"global\" - each embedding depends only on the word itself. This for example limits Word2vec usability for omonimes - words that are spelled the same but have different meanings. Newer embedding models are more context dependent.\n",
    "\n",
    "Context of a word = neighboring words in a sentence (for example previous two words and next two words).\n",
    "\n",
    "Смысл алгоритма в том, чтобы каждое слово из словаре описать вектором небольшой размерности. Набор таких векторов с эмбедингами - и есть параметры модели.\n",
    "\n",
    "There are two alternative settings:\n",
    "- Continuous Bag-Of-Words: \n",
    "    \n",
    "        predict the word probabilities based on its context\n",
    "\n",
    "\n",
    "- Skip-gram: \n",
    "\n",
    "        predict context words probabilities based on the word\n",
    "\n",
    "In both cases feature space remains the same so they both do solve our problem.\n",
    "\n",
    "На выходе будем считать вероятность принадлжености анализируемого слова к каждому из слов словаря и считается она, как softmax от скалярного произведения эмбедингов анализируемого слова и слова контекста. То есть если эмбединги анализируемого слова и слова контекста находятся близко, softmax дожен давать высокую вероятность. Если далеко, вероятность будет близка к нулю.\n",
    "\n",
    "Оптимизировать будем логарфимическую функцию правдоподобия модели.\n",
    "\n",
    "Для получения обучающей выборки разделим тексты по окнам фиксированной длины. \n",
    "Для обучения классификатора помимо позитивного класса нужен негативный. Поэтому здесь используется механизм негативного сэмплирования.\n",
    "\n",
    "При негативном сжмпировании частоты слов возводятся в степень ¾, затем чтобы уиеньшить исаользование слишком часто употребляемых слов, и наоборот увеличить использование редких слов.\n",
    "\n",
    "Альтернативный способ обучения - иереархическая чего-то там."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
