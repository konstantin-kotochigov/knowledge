{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Eigenmaps (2003)\n",
    "---\n",
    "\n",
    "Similar to Isomap - we also represent dataset as a spanning graph. But here we reduce dimensionality by choosing eigenvectors that would minimize the Laplacian of that graph\n",
    "\n",
    "\n",
    "## Laplacian (Continuous)\n",
    "\n",
    "In calculus Laplacian is a functional operator that sums the second derivatives.\n",
    "<img src=\"img/laplacian.png\" width=200>\n",
    "\n",
    "Interpretation = measure of \"unsmoothness\" of a function at a point. The more steering on a track = the less smooth the path is\n",
    "\n",
    "Applied to field the output is another scalar field. If a function $f$ is chaotic or changes its direction a lot => it's Laplacian will have a large norm $||\\nabla^2 f||$\n",
    "\n",
    "\n",
    "## Laplacian (Discrete)\n",
    "\n",
    "We can approximate the second derivative with a __central finite difference__ approximation:<br> \n",
    "\n",
    "$\n",
    "f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}\n",
    "$\n",
    "\n",
    "Interpretation<br>\n",
    "if function value is lower than the average of its neighbors => the function is convex $f''(x) > 0$<br>\n",
    "\n",
    "\n",
    "<img src=\"img/deriv.png\" width=200>\n",
    "\n",
    "This interpretation makes it possible to use Laplacian to measure non-continuous functions (defined in grids, graphs etc)\n",
    "\n",
    "### Derivation of the Central finite difference\n",
    "\n",
    "Check the neighbors $f(x+h)$ and $f(x-h)$ expansion around $x$:\n",
    "\n",
    "$\n",
    "f(x+h) = f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\frac{h^3}{6} f'''(x) + \\frac{h^4}{24} f^{(4)}(x) + O(h^5)\n",
    "$\n",
    "\n",
    "$\n",
    "f(x-h) = f(x) - h f'(x) + \\frac{h^2}{2} f''(x) - \\frac{h^3}{6} f'''(x) + \\frac{h^4}{24} f^{(4)}(x) + O(h^5)\n",
    "$\n",
    "\n",
    "And express f''(x) from the neighbors:\n",
    "\n",
    "$\n",
    "f(x+h) + f(x-h) = 2 f(x) + h^2 f''(x) + \\frac{h^4}{12} f^{(4)}(x) + O(h^6)\n",
    "$\n",
    "\n",
    "$\n",
    "h^2 f''(x) = (f(x+h) + f(x-h)) - 2 f(x) + O(h^4)\n",
    "$\n",
    "\n",
    "Thus, the second derivative is approximated as:\n",
    "\n",
    "$\n",
    "f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "## Graph Laplacian\n",
    "\n",
    "At a graph level continuous analysis does not work => we replace it with a discrete approximation\n",
    "\n",
    "Laplacian matrix\n",
    "<img src=\"img/laplacian_matrix.png\" width=750>\n",
    "\n",
    "Node degree on the diagonal, edges on columns\n",
    "Row sum equals zero\n",
    "\n",
    "\n",
    "\n",
    "#### Observation 1\n",
    "The sum of squared descripancies between all pairs of nodes $\\sum_{i,j} (y_i - y_j)^2$ can be rewritten in the form of $y^TLy$\n",
    "\n",
    "\n",
    "#### Proof\n",
    "\n",
    "$\n",
    "f^T L f = f^T (D - A) f = f^T D f - f^T A f\n",
    "$\n",
    "\n",
    "Expanding each term:\n",
    "\n",
    "$\n",
    "f^T D f = \\sum_{i} d(i) f(i)^2\n",
    "$\n",
    "\n",
    "$\n",
    "f^T A f = \\sum_{i,j} A_{ij} f(i) f(j)\n",
    "$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$\n",
    "f^T L f = \\sum_{i} d(i) f(i)^2 - \\sum_{i,j} A_{ij} f(i) f(j)\n",
    "$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$\n",
    "f^T L f = \\frac{1}{2} \\sum_{i,j} A_{ij} (f(i) - f(j))^2\n",
    "$\n",
    "\n",
    "\n",
    "#### Observation 2\n",
    "Minimizng $y^TLy$ is equivalent to finding eigenvectors\n",
    "\n",
    "\n",
    "# Algorithm\n",
    "---\n",
    "1. Construct a spanning graph for a dataset<br>\n",
    "   two options how we can do it:\n",
    "    - connect each point to all points in its epsilon neighborhood\n",
    "    - connect each point to its k closest points<br><br>\n",
    "2. Choose the edge weights<br>two variants:<br>- binary $w \\in \\{0,1\\}$<br>- using Heat kernel (Gaussian): more neighbors => more weight<br><br>\n",
    "3. Construct the Laplacian of that graph <br>$L = D - A$<br><br>\n",
    "3. Find the eigenvectors of the Laplacian<br>$Le = \\lambda W e$<br><br>\n",
    "4. Select Top-k eigenvectors that will form a mapping to a new low-dimensional space<br>$\\vec{e}=\\{e_1, e_2 ... e_k, o, o ... o\\}$<br><br>\n",
    "5. Output = a point projected onto the first k eigenvectors<br>$y^i = [x^i \\,  e_1, \\,\\, x^i \\, e_2 ... x^i \\, e_k]$<br>\n",
    "\n",
    "\n",
    "There is an alternative variant that directly uses Hessians<br> Hessian Locally Liner Embeddings (2003)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
