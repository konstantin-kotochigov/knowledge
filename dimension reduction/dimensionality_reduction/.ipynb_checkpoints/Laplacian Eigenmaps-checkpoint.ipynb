{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Eigenmaps (2003)\n",
    "\n",
    "Laplacian eigenmaps is quite a similar approach to Isomap, as we also represent a dataset as a kNN graph. As you may know from graph theory, there is a conventional way of reducing dimensionality of a graph - its called eigendecomposition of graph. And we utilize this logic to transfer the approach from graphs to any datasets.\n",
    "\n",
    "in graph theory when we do decomposition we do it in the following steps:\n",
    "we compute a laplacian, which is degree matrix minus adjacency matrix\n",
    "we eigen vectors of this matrix\n",
    "\n",
    "So, the full algorithm would look like this:\n",
    "we construct a spanning graph of a dataset. The process is quite the same as earlier\n",
    "we compute a laplacian of that graph, which is an adjacency matrix of a graph minus degree diagonal matrix of a graph\n",
    "so now we have a matrix describing our dataset, and what we gonna do here - we gonna decompose it as usual using an eigendecomposition\n",
    "Top k eigenvectors form a mapping of point to a new low-dimensional space.\n",
    "\n",
    "\n",
    "\n",
    "Полностью алгоритм выглядит так:\n",
    "Строится граф, соединяющий ближайших соседей выборки (1-NN)\n",
    "Этот граф описывается стандартным для описания графов способом, то есть строится adjacency matrix (матрица смежности). \n",
    "Далее их этой матрицы вычитается degree matrix (матрицы степеней) - диагональная матрица, описывающая сколько ребер входит в каждую вершину. \n",
    "Разность дает нам третью матрицу - лапласиан графа, graph laplacian. Это и есть описание выборки в методе laplacian eigenmaps.\n",
    "Далее действуем стандартно, полученый лапласиан раскладываем на собственные вектора; выбираем нужную нам размерность R и делаем проекцию выборки на первые R собственных векторов\n",
    "Точка в новом eigen базисе (x_i * e_i) - и есть эмбединг\n",
    "\n",
    "Иногда еще лапласиан взвешивают, больше веса дается точкам, которые в исходном пространстве находятся близко.\n",
    "\n",
    "Теперь дам чуть больше деталей относительно математике процесса. Из матанализа мы помним, что если у нас есть некая многомерная функция R^n -> R, то для этой функции в каждой точке пространства R^n мы можем посчитать:\n",
    "Hessian (гессиан) - матрица вторых частных производных\n",
    "Laplacian (лапласиан) - это сумма диагнональных эдементов гессиана, то есть просто сумма вторых производных в точке.\n",
    "Лапласиан еще иногда описывают через операторы, а именно как дивергенцию градиента (L = div grad), где дивергенция - сумма первых производных, а градиент (grad) - соотвественно вектор первых производных\n",
    "\n",
    "\n",
    "\n",
    "На самом деле есть еще формулировка этого метода именно через гессианы - Hessian Locally Liner Embeddings (2003).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
