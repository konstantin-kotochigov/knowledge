{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDS\n",
    "\n",
    "Multidimensional scaling is a concept that appeared in statistics in 1950s. Its purpose was to <u>preserve pairwise distances</u> between points after mapping from high-dimensional to low-dimensional space. \n",
    "\n",
    "In statistics low-dimensional space is sometimes referred to as [ordination](https://en.wikipedia.org/wiki/Ordination_(statistics)) space. Most of the time it was used for data visualization, so the new dimensionality was often chosen to be 2 or 3.\n",
    "\n",
    "### Common problem statement\n",
    "\n",
    "We don't have original dataset coordinates, we only have some [dissimilarity](https://en.wikipedia.org/wiki/Distance_matrix) matrix\n",
    "\n",
    "#### TBD: change notation\n",
    "\n",
    "$$D^{old}_{ij} = dis(x_i,x_j)$$\n",
    "\n",
    "Suppose we did some mapping and came to configuration X. We then compute dissimilarities in a new space. Usually dissinilarities in a new space are standard euclidean distances (it does not make much sense to use more complex distances):\n",
    "\n",
    "$$D^{new}_{ij}(X) = {||x_i - x_j ||}^2 $$\n",
    "\n",
    "The task is to preserve distances => we need to find such configuration X that minimizes loss function:\n",
    "\n",
    "$$X^{opt} = \\underset{X}{\\operatorname{{arg\\,min}}} \\sum_{i \\ne j} {\\left( D^{new}_{ij}(X) - D^{old}_{ij}\\right)}^2$$\n",
    "\n",
    "The scheme of the process is described below:\n",
    "\n",
    "<img src = \"img/mds.png\" width=500>\n",
    "\n",
    "Classical MDS (assumption = D_old - euclidean distances):\n",
    "1. We compute a decentered distance matrix of all distances between all pairs of points in a dataset\n",
    "2. We do eigendecomposition of that matrix which gives us a set of eigenvectors of that matrix\n",
    "3. We project dataset onto the first K components\n",
    "\n",
    "### Proof\n",
    "\n",
    "1. [Centering matrix](https://en.wikipedia.org/wiki/Centering_matrix) is a concise way to subtract mean from matrix\n",
    "\n",
    "    If we denote $I_n$ an identity matrix; $J_n$ - matrix of all ones, then\n",
    "$$C_n =  I_n - \\tfrac{1}{n}J_n$$\n",
    "\n",
    "    For example:\n",
    "\n",
    "$$C_3 = \\left[ \\begin{array}{rrr}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{array} \\right] -  \\frac{1}{3}\\left[ \\begin{array}{rrr}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \n",
    "\\end{array} \\right]\n",
    " = \\left[ \\begin{array}{rrr}\n",
    "\\frac{2}{3} & -\\frac{1}{3} & -\\frac{1}{3} \\\\\n",
    "-\\frac{1}{3} & \\frac{2}{3} & -\\frac{1}{3} \\\\\n",
    "-\\frac{1}{3} & -\\frac{1}{3} & \\frac{2}{3} \n",
    "\\end{array} \\right]$$\n",
    "\n",
    "\n",
    "Right multiplication <u>subtracts row mean</u> from each row \n",
    "$$XC$$\n",
    "\n",
    "Left multiplication <u>subtracts column mean</u> from each column\n",
    "$$CX$$\n",
    "\n",
    "Double centering = applying demeaning matrices twice\n",
    "$$CXC$$\n",
    "\n",
    "In that case each row and column will have an average of zero.\n",
    "\n",
    "-----\n",
    "\n",
    "2. Let's describe distance matrix D through (unknown) coordinates matrix X\n",
    "\n",
    "$$D^X =  Z - 2X^TX + Z^T$$\n",
    "\n",
    "Here $Z$ and $Z^T$ contain squared coordinates\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "3. Let's apply double decentering to our distance matrix $D^X$\n",
    "\n",
    "    $Z$ and $Z^T$ are row/column constant => they vanish out\n",
    "\n",
    "    So we get:\n",
    "\n",
    "$$CD^XC = -2\\hat{X}^T\\hat{X}$$\n",
    "\n",
    "Note that $X^T$ and $X$ were also decentered and became $\\hat{X}^T$ and $\\hat{X}$ - decentered coordinate matrices.\n",
    "\n",
    "Let's denote this decentered distance matrix by $B^X$:\n",
    "\n",
    "$$B^X = -\\frac{1}{2}CD^XC = \\hat{X}^T\\hat{X}$$\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "4. Now we need to find best rank-k approximation of decentered distance matrix $\\hat{X}^T\\hat{X}$. \n",
    "\n",
    "$$B^y = \\underset{B^Y}{argmin} = ||B^X - B^Y||^2 = ||\\hat{X}^T\\hat{X} - Y^TY||^2$$\n",
    "\n",
    "Notice that Y's that we are seeking for, will also represent the decentered coordinates, not the original ones\n",
    "\n",
    "Notice that here we use Frobenius norm - it is the same as minimizaing mean squared error.\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "5. This optimization is done by SVD decomposition.\n",
    "\n",
    "    Suppose we deal with matrix A, then\n",
    "<img src=\"img/svd_theorem.png\" width=300>\n",
    "\n",
    "\n",
    "Those Y optimally approximate B^X:\n",
    "$$Y^TY = (UD^{1/2})(D^{1/2}U)$$\n",
    "\n",
    "### Repeat\n",
    "1. decentralize distance matrix\n",
    "2. decompose using SVD\n",
    "3. coordinates in singluar basis = new coordinates\n",
    "\n",
    "#### Relation to PCA\n",
    "\n",
    "MDS is similar to PCA in a way that it also uses the eigendecomposition to get reduced coordinates. It's even refered to as PCoA.\n",
    "\n",
    "But the formulation is a bit different\n",
    "- for PCA = to preserve data variation after mapping\n",
    "- for MDS = to preserve pairwise distances after mapping\n",
    "\n",
    "Also in MDS we work with distance matrix instead of covariance matrix. It makes this approach a bit more general.\n",
    "\n",
    "####  Flavours of MDS\n",
    "- Classic MDS\n",
    "- metric MDS\n",
    "- non-metruc MDS\n",
    "\n",
    "### References\n",
    "\n",
    "[SVD](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
