{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem setting\n",
    "\n",
    "In this Machine learning series we gonna talk about the dimensionality reduction. First we'll do some theory and then as usual try some practice. Today we'll cover a bunch of methods, they are on the slide.\n",
    "\n",
    "And let's start.\n",
    "\n",
    "As you might know, there are 3 main pillars of machine learning:\n",
    "- classification\n",
    "    \n",
    "        when we predict class of an object\n",
    "\n",
    "\n",
    "- regression\n",
    "    \n",
    "        when we predict some numeric value\n",
    "\n",
    "\n",
    "- and clusterization\n",
    "    \n",
    "        when we don't have a training set so we deduct the inner structure of the data\n",
    "\n",
    "\n",
    "Apart form those 3 major tasks, there are quite a few others. And one of those tasks is dimensionality reduction, which we gonna talk about today.\n",
    "\n",
    "In AI realm we work with signals. And the signal could be simple, like a record in a relational database, or it could extremely complex, like a 5-minute video stream. \n",
    "\n",
    "Now data tends to be more and more complex. For example if we want to classify an RGB image of 480 resolution, our model will have to deal with at least 1M dimensional frame.\n",
    " \n",
    "So we need compress it somehow:\n",
    "we need to reduce its size\n",
    "and we need to do it in a smart way - while removing insignificant noise, we should retain as much valuable information as possible.\n",
    "\n",
    "The first is usually achieved by simple **feature selection** (we are talking about feature selection methods in a separate video).\n",
    "And the second is achieved by construction of new features. This is the basis of deep learning models, when raw image is being converted to some compressed semantic representation that numerically encodes the content of the image.\n",
    "\n",
    "There is a closely related concept of manifold learning, when we assume that data points are distributied along some structured manifold. By learning the form of this manifold we can represent data in a much more compressed way.\n",
    "\n",
    "So, what is the value of dimension reduction?\n",
    "1. We can process more data. Because there is less strain on CPU or memory.\n",
    "2. In some ML tasks the compressed signal could be a result itself.\n",
    "For example in Topic Modeling we represent each document as a blend of topics\n",
    "3. We are able to plot data and analise it visually. We just need to project the data onto 2-dimensional space and plot it\n",
    "\n",
    "There are two large groups of methods:\n",
    "- methods from the first group work with global structure data\n",
    "- methods from the first group utilize the local structure of the data\n",
    "\n",
    "Timeline:\n",
    "- PCA\n",
    "1900\n",
    "- MDS\n",
    "1960\n",
    "- NMF\n",
    "1999\n",
    "- LLE\n",
    "2000\n",
    "- Isomap\n",
    "2000\n",
    "- Random Projection\n",
    "2001\n",
    "- LTSA\n",
    "2002\n",
    "- Eigenmaps\n",
    "2003\n",
    "- LDA (Andrew Ng)\n",
    "2003\n",
    "- t-SNE\n",
    "2008\n",
    "- Word2Vec\n",
    "2013\n",
    "- GloVe\n",
    "2014\n",
    "- largeVis\n",
    "2016\n",
    "- UMAP\n",
    "2018\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating\n",
    "\n",
    "Most dimensionality reduction algorithms are unsupervised. So the question comes - how can we assess its performance?\n",
    "And the answer is - we can kinda move it to semi-supervised setting. \n",
    "\n",
    "Let's take a look at 2 datasets that are frequently used to assess unsupervised methods \n",
    "- first one is MNIST, which is a dataset of handwritten digits from 0 to 9, \n",
    "- and the second one is MFashion - a collection of small images of different kinds of clothes.\n",
    "\n",
    "We mask target labels and treat our data as if it were unlabeled.\n",
    "Then we apply dimensionality reduction (for example, PCA) and see how actual labels are distributed in low-dimensional space.\n",
    "\n",
    "The easiest way is to use 2-dimensional target space so we can actually see on a plane the mapping of labels. Let's try to do it with digits dataset. We assign a separate color for each digit and take a look at how these colors are distributed in low-dimensinal space. Ideally, we want the points to form well-defined and separated clusters of the same color.\n",
    "\n",
    "Here is our result. As we see, PCA projection is far from being ideal, there is a large intersection of labels and cluster edges are not very well defined. We’ll see later that modern approaches tend to do the job way better.\n",
    "\n",
    "There is a couple of enhancemnts to standard approach. And here they are:\n",
    "Sparse PCA is PCA performed with an added constraint on number of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD\n",
    "\n",
    "## LargeVis (2016)\n",
    "References\n",
    "- https://arxiv.org/pdf/1602.00370.pdf\n",
    "- https://habr.com/ru/post/341208/\n",
    "\n",
    "LargeVis is an extended version of t-SNE. Major differences:\n",
    "- On the first step, when they are finding nearest neighbors  they use more effective Random Projection Trees instead of Quadtrees\n",
    "- Cost function is defined probabilistic\n",
    "- It is optimized using maximum likelihood\n",
    "- They use negative sampling to make optimization more efficient\n",
    "\n",
    "## ePCA (2017)\n",
    "\n",
    "## NMF (2001)\n",
    "\n",
    "\n",
    "\n",
    "## Glove (2014)\n",
    "\n",
    "It's an alternative model for word embedding, that appeared in Stanford a year later. Unlike Word2Vec it uses  Document-term matrices which are much easier to get. \n",
    "\n",
    "Используется 2 набора параметров вместо одного, поскольку так обучается лучше. Практика показывает, что результирующий эмбединг лучше всего задавать как X + Y.\n",
    "\n",
    "## LTSA (2002)\n",
    "Local Tangent Space Alighnment.\n",
    "\n",
    "## LDA\n",
    "In NLP there is one of the central tasks which is called \"Topic modeling\". There is a plethora of algorithms that solve this task. The modern ones are often quite involved, but the idea is very simple - we need to . In some sense it's very close to clusterization, but unlike clusterization it allows multiple topics to be present in one document.\n",
    "\n",
    "Instead of using a raw description of a text, (for example bag-of-encoded-words), we can describe it just as a mixture of several topics. This would be enough to compare different documents to each other, plot them on a single map and so on.\n",
    "\n",
    "You don't even have to describe the topics themselves. They could act as just uninterpretable numeric embeddings.\n",
    "\n",
    "As of today probably the most popular application - is latent dirichlet allocation (LDA).\n",
    "\n",
    "First it was used to track representation of different species in population analysis. But later it became the central method for exrtacting topics from texts. I'm not gonna spent much time on it. I'll make another video.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dimensionality Reduction и Deep Learning\n",
    "Как правило, на вход подается какой-то многомерный сигнал, на выходе имеем очень простой результат (если только это не генеративная или seq2seq модель). То есть сокращение размерности происходит в любом случае, просто где-то на этапе предобработки, а где-то в конце применения модели.\n",
    "\n",
    "До распространения глубокого обучепния работа со сложным сигналом была сильно завязана на ручную генерацию фичей. Действительно, при работе с изображениями, автор алгоритма сначала рассчитывал некий набор агрегатов, который был призван максимально точно передать информацию о сигнале, а затем уже строил предиктивную модель.\n",
    "\n",
    "В конце 80х Ян Лекун подумал, а зачем все это делать вручную, когда можно этот процесс параметризовать и сделать частью алгоритма. Так появились первые сверточные нейронные сети. Архитектурно, они состояли из продолжительного процесса свертки, то есть по сути нашего dimension reduction, и небольшой моделирующей части, отвечающей за обучение предиктивной модели. Параметры свертки добавились к параметрам сети и стали настраиваться уже при обучении модели.\n",
    "\n",
    "Затем в конце 90х появились рекуррентные сети, как аналог сверточных но уже для последовательностей, то есть главным образом текстов и различных временных рядов. Механизм обработки немного другой, но приницпы оставалсиь теми же. Перед подачей в сеть каждое слово переводилось в свой эмбединг - семантический вектор в пространстве меньшей размерности. Эмбединг слов также стал частью сети и настраивался на этапе обучения модели.\n",
    "\n",
    "До приблизительно 2010 года глубокие сети не имели большого распространения, в основном из-за высокой стоимости железа, требующегося для полноценных расчетов. Но после 2010 состоялся взрывной рост их популярности и появилось огромное количество новых алгоритмов.\n",
    "\n",
    "Автоэнкодеры - тип нейронных сетей, которые встраивают сигнал в простанство меньшей размерности. Они используюься как самостоятельно для получения сокращенного описания, но чаще как часть архитектуры большей сети.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
