{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important part of data preprocessing is feature scaling.\n",
    "\n",
    "#### Typical feature transformations\n",
    "- Normalization\n",
    "- Standardization\n",
    "- Exponentiation\n",
    "\n",
    "#### Implemented Sklearn transformations\n",
    "\n",
    "- MaxAbsScaler\n",
    "\n",
    "        normalize maximum absolute value\n",
    "\n",
    "\n",
    "- MinMaxScaler\n",
    "\n",
    "        map values onto predefined interval, default is [0,1]\n",
    "\n",
    "\n",
    "- StandardScaler\n",
    "\n",
    "        subtracts average and divides by standard deviation\n",
    "\n",
    "- RobustScaler\n",
    "\n",
    "        standard scaler that filters out 25% outliers\n",
    "\n",
    "- Normalizer\n",
    "\n",
    "        converts each data row so that it has unit norm (L1, L2 or Maximum)\n",
    "\n",
    "- PowerTransformer\n",
    "\n",
    "        applies box-cox / yeo-jensen transformation to decrease skewness of data\n",
    "        https://en.wikipedia.org/wiki/Power_transform\n",
    "\n",
    "\n",
    "#### Sklearn transformers functions\n",
    "\n",
    "- fit() \n",
    "\n",
    "        -- learns the required transformation\n",
    "- transform()\n",
    "        \n",
    "        -- applies transformations\n",
    "- fit_transform()\n",
    "\n",
    "        -- does both\n",
    "\n",
    "predict() data preprocessing (if used) must be applied to all (train/test/validation) parts of data.\n",
    "\n",
    "#### Where to fit the scalers\n",
    "To imitate the most realistic estimates of model performance, we should restrict fitting a scaler to Train part of data and apply to Test parts.\n",
    "\n",
    "#### Is it necessary?\n",
    "- If there is enough training data and train/test splitting retains feature distributions => No\n",
    "- If train/test splitting changes feature distribution => Yes\n",
    "\n",
    "#### Does data splitting retain distributions\n",
    "Let's try it on real datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T00:17:32.307671Z",
     "start_time": "2021-01-04T00:17:32.304488Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import ShuffleSplit, KFold\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "bunch = fetch_openml(name=\"abalone\", data_home=\"/Users/Konstantin/git/\", as_frame=True)\n",
    "X = bunch['data'].select_dtypes('number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try ShuffledSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T00:20:43.047312Z",
     "start_time": "2021-01-04T00:20:42.988316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale = 0.00057\n",
      "center = 0.00043\n"
     ]
    }
   ],
   "source": [
    "scaler_train = StandardScaler()\n",
    "scaler_test = StandardScaler()\n",
    "\n",
    "scale_result = []\n",
    "center_result = []\n",
    "\n",
    "for train_records, test_records in ShuffleSplit(n_splits=10, test_size=.25).split(X):\n",
    "    X_train = X.loc[train_records]\n",
    "    X_test = X.loc[test_records]\n",
    "    scaler_train.fit(X_train)\n",
    "    scaler_test.fit(X_test)\n",
    "    scale_stats = round(sum([round(pow(x[1]-x[0],2), 5) for x in zip(scaler_train.scale_, scaler_test.scale_)]), 5)\n",
    "    center_stats = round(sum([round(pow(x[1]-x[0],2), 5) for x in zip(scaler_train.mean_, scaler_test.mean_)]), 5)\n",
    "    scale_result.append(scale_stats)\n",
    "    center_result.append(center_stats)\n",
    "\n",
    "print(\"scale = {}\".format(round(np.mean(scale_result),5)))\n",
    "print(\"center = {}\".format(round(np.mean(center_result),5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T00:20:45.150215Z",
     "start_time": "2021-01-04T00:20:45.072373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale = 0.00076\n",
      "center = 0.00071\n"
     ]
    }
   ],
   "source": [
    "scaler_train = RobustScaler()\n",
    "scaler_test = RobustScaler()\n",
    "\n",
    "scale_result = []\n",
    "center_result = []\n",
    "\n",
    "for train_records, test_records in ShuffleSplit(n_splits=10, test_size=.25).split(X):\n",
    "    X_train = X.loc[train_records]\n",
    "    X_test = X.loc[test_records]\n",
    "    scaler_train.fit(X_train)\n",
    "    scaler_test.fit(X_test)\n",
    "    scale_stats = round(sum([round(pow(x[1]-x[0],2), 5) for x in zip(scaler_train.scale_, scaler_test.scale_)]), 5)\n",
    "    center_stats = round(sum([round(pow(x[1]-x[0],2), 5) for x in zip(scaler_train.center_, scaler_test.center_)]), 5)\n",
    "    scale_result.append(scale_stats)\n",
    "    center_result.append(center_stats)\n",
    "\n",
    "print(\"scale = {}\".format(round(np.mean(scale_result),5)))\n",
    "print(\"center = {}\".format(round(np.mean(center_result),5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T00:15:35.209152Z",
     "start_time": "2021-01-04T00:15:35.152083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale = 0.00354\n",
      "center = 0.01828\n"
     ]
    }
   ],
   "source": [
    "scaler_train = StandardScaler()\n",
    "scaler_test = StandardScaler()\n",
    "\n",
    "scale_result = []\n",
    "center_result = []\n",
    "\n",
    "for train_records, test_records in KFold(n_splits=10).split(X):\n",
    "    X_train = X.loc[train_records]\n",
    "    X_test = X.loc[test_records]\n",
    "    scaler_train.fit(X_train)\n",
    "    scaler_test.fit(X_test)\n",
    "    scale_stats = round(sum([round(pow(x[1]-x[0],2), 5) for x in zip(scaler_train.scale_, scaler_test.scale_)]), 5)\n",
    "    center_stats = round(sum([round(pow(x[1]-x[0],2), 5) for x in zip(scaler_train.mean_, scaler_test.mean_)]), 5)\n",
    "    scale_result.append(scale_stats)\n",
    "    center_result.append(center_stats)\n",
    "\n",
    "print(\"scale = {}\".format(round(np.mean(scale_result),5)))\n",
    "print(\"center = {}\".format(round(np.mean(center_result),5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add shuffling to KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T00:15:31.054012Z",
     "start_time": "2021-01-04T00:15:30.998164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale = 0.00034\n",
      "center = 0.00059\n"
     ]
    }
   ],
   "source": [
    "scaler_train = StandardScaler()\n",
    "scaler_test = StandardScaler()\n",
    "\n",
    "scale_result = []\n",
    "center_result = []\n",
    "\n",
    "X_shuffled = X.copy().sample(frac=1).reset_index(drop=True)\n",
    "X_shuffled\n",
    "\n",
    "for train_records, test_records in KFold(n_splits=10).split(X_shuffled):\n",
    "    X_train = X_shuffled.loc[train_records]\n",
    "    X_test = X_shuffled.loc[test_records]\n",
    "    scaler_train.fit(X_train)\n",
    "    scaler_test.fit(X_test)\n",
    "    scale_stats = round(sum([round(pow(x[1]-x[0],2), 5) for x in zip(scaler_train.scale_, scaler_test.scale_)]), 5)\n",
    "    center_stats = round(sum([round(pow(x[1]-x[0],2), 5) for x in zip(scaler_train.mean_, scaler_test.mean_)]), 5)\n",
    "    scale_result.append(scale_stats)\n",
    "    center_result.append(center_stats)\n",
    "\n",
    "print(\"scale = {}\".format(round(np.mean(scale_result),5)))\n",
    "print(\"center = {}\".format(round(np.mean(center_result),5)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
