{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Идея:__ апроксимируем рейтинг $R_{ij}$ как произведение двух эмбедингов - пользовательского и товарного \n",
    "\n",
    "Соотвественно, матрица рейтигнов апроксимируется как произведение двух матриц U и V\n",
    "\n",
    "Выход алгоритма: два вектора эмюедингов U и V\n",
    "\n",
    "Их скалярное произведение даёт прогноз рейтинга\n",
    "\n",
    "Плюсы:\n",
    "- очень быстро считается\n",
    "\n",
    "Модификации:\n",
    "- BlockALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минимизируем ошибку приближения\n",
    "<img src=\"img/als.png\" width=350>\n",
    "\n",
    "Функция не выпуклая => использовать всякие градиентные методы не получится\n",
    "\n",
    "Однако, если зафиксировать одну из двух переменных, задача упрощается и становится решаемой аналитически\n",
    "\n",
    "Алгоритм\n",
    "0. инициализируем эмбединги чем-то\n",
    "1. фиксируем пользовательский эмбединг и оптимизируемся по товарному\n",
    "2. фиксируем товарный эмбединг и оптимизируемся по пользовательскому\n",
    "3. повторяем до сходимости\n",
    "\n",
    "Ниже формальное описание\n",
    "\n",
    "<img src=\"img/als2.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод шага алгоритма для U\n",
    "\n",
    "Фиксируем матрицу V, оптимизируем коэффициенты матрицы U.\n",
    "\n",
    "Строки матрицы рейтингов независимы => оптимизируем построчно. Это удобно, можно запускать параллельно.\n",
    "\n",
    "У нас получается m задач линейной регрессии с $n$ кейсами в каждом - рейтинг будем прогнозировать как комбинацию k факторов, кодирующих пользователя\n",
    "\n",
    "Например возьмем пользователя $U_i$ и посчитаем ошибку приближения:\n",
    "\n",
    "$$J(U_i) = \\sum_{j \\in items} \\bigg(R_{ij} - U_{ij} V_j^T\\bigg)^2$$\n",
    "\n",
    "Здесь $R_i$ - строка с рейтингами пользователя, $U_i$ - эмбединг пользователя, $V^T$ - эмбединг товара\n",
    "\n",
    "Добавим $L_2$ регуляризацию\n",
    "\n",
    "$$J(U_i) = \\sum_j (R_{ij} - U_{ij} V^T)^2 + \\lambda \\sum_j || U_{ij} ||^2$$\n",
    "\n",
    "Сумму квадратов можно представить векторно, если в вектор то, по чему суммируем: набор $R_{ij}$ обозначим за $R_i$, а столбцы $V_j$ за матрицу $V$)\n",
    "\n",
    "$$J(U_i) = (R_i - U_i V^T) (R_i - U_i V^T)^T$$\n",
    "\n",
    "Чтобы привести к более классической форме записи формулы регрессии, перенесем транспонирование внутрь:\n",
    "\n",
    "$$J(U_i) = (R_i^T - V U_i^T)^T (R_i^T - V U_i^T)$$\n",
    "\n",
    "Вспоминаем постановку МНК-регрессии с регуляризацией\n",
    "\n",
    "$$J(U_i) = (y - X \\beta)^T (y - X \\beta) + \\lambda X^TX$$\n",
    "\n",
    "Видим, что это и есть наш функционал:\n",
    "\n",
    "$$R^T=y$$\n",
    "$$V=X$$\n",
    "$$U^T=\\beta$$\n",
    "\n",
    "Тогда вспоминаем формулу для Ridge регрессии (выводить не будем)\n",
    "\n",
    "$$\\beta = ((X^TX)^{-1} + \\lambda I)X^Ty$$\n",
    "\n",
    "И подставляем наши переменные\n",
    "\n",
    "$$U^T = ((V^TV)^{-1} + \\lambda I)V^TR^T$$\n",
    "\n",
    "Для получения U транспонируем\n",
    "\n",
    "$$U = (RV)((V^TV)^{-1} + \\lambda I)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод шага алгоритма для V\n",
    "\n",
    "Все аналогично U"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
