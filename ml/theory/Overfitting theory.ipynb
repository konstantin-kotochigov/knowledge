{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance tradeoff\n",
    "\n",
    "Предположим, в рамках нашей модели $y$ раскладывается на $f(x)$ и случайный шум $\\epsilon$\n",
    "\n",
    "$$y = f(x) + \\epsilon$$\n",
    "\n",
    "ML-модель строит оценку $\\hat{y}|X$ по формуле, зависящей от наблюдаемого сэмпла данных. $\\hat{y}$ зависит от реализации $x \\in X$ и является сама случайной величиной.\n",
    "\n",
    "Если берем модель линейной регрессии ($\\epsilon$ = неубираемый независимый шум), то там оценка имеет конкретный вид $\\hat{f} = E[y|x]$\n",
    "\n",
    "Нам бы неплохо получить характеристики этой случайной величины, в частности смещенность и разброс\n",
    "\n",
    "#### шаг 0\n",
    "\n",
    "исследуем, как ведет себя отклонение решения $\\hat{f}$ от реального значения $y$\n",
    "\n",
    "$$\\text{MSE} = E{[y-\\hat{f}]}^2$$\n",
    "\n",
    "по нашей модели $y$ содержит неубираемый (и независимый) шум $\\epsilon$, поэтому его легко выделить в общем разбросе и перейти к более содержательным величинам $\\hat{f}$ и $f$, это делается на шаге 1\n",
    "\n",
    "#### шаг 1\n",
    "избавляемся от y, в формуле будет фигурировать только $f$ и $\\hat{f}$:\n",
    "\n",
    "$$E{[y-\\hat{f}]}^2 = E{[f - \\hat{f} + \\epsilon]}^2 = \\underbrace{E[f-f]^2}_{\\text{Model Error}} + \\underbrace{2E[f-f]\\epsilon}_{\\text{Cov}=0} + \\underbrace{E{[\\epsilon]}^2}_{\\text{Noise } \\sigma^2}$$\n",
    "\n",
    "из-за независимости шума, дисперсия аддитивно раскладывается на \n",
    "- разброс предсказания $\\hat{f}$ относительно реального значения $f$ (не путать с дисперсией)\n",
    "- случайный шум модели $\\epsilon$\n",
    "\n",
    "#### шаг 2\n",
    "разброс предсказания неплохо бы разделить на составляющие части: \n",
    "- ошибку матожидания, от носительно которого рзбросаны предикты (Bias) \n",
    "- ошибку относительно матожидания (Variance)\n",
    "\n",
    "<img src=\"img/bias_variance.png\" width=300>\n",
    "\n",
    "для этого добавляем в равенство слагаемое $E\\hat{f}$ (средний предикт по всем возможным выборкам), относительно которого делим на 2 слагаемых. \n",
    "\n",
    "\n",
    "\n",
    "$$E[f-\\hat{f}]^2 = E{\\big[(f - E\\hat{f}) + (E\\hat{f} - \\hat{f})\\big]}^2 = \\underbrace{E[f - E\\hat{f}]^2}_{Bias^2} + \\underbrace{2E(f-Ef)(E\\hat{f}-\\hat{f})}_{Cov=0} + \\underbrace{E[E\\hat{f}-\\hat{f}]^2}_{Var(\\hat{f})}$$\n",
    "\n",
    "почти сразу получаем сумму разбросов:\n",
    "- Bias = размер систематического сдвига предсказания (после всех усреднений)$\\hat{f}$\n",
    "- Cov = 0\n",
    "- Var = разброс предсказаний относительно своего среднего\n",
    "\n",
    "почему $Bias^2$ - потому что есть устоявшаяся терминология: смещение оценки $\\hat{f}$: $f - E[\\hat{f}]$, а здесь она в квадрат вовзодится\n",
    "\n",
    "матожидание от константы = константа:\n",
    "$$E[f - E\\hat{f}]^2 = [f - E\\hat{f}]^2$$\n",
    "\n",
    "Итого:\n",
    "$$E\\big[ y - \\hat{f} \\big] = {Bias}^2 + Var + \\sigma$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Несколько комментариев\n",
    "\n",
    "по чему берутся все матожидания?\n",
    "по всем возможным реализациям X\n",
    "\n",
    "риск формулируется как\n",
    "\n",
    "$$E_{X}[ \\text{MSE}] = E_{X}{\\big[{y(x) - \\hat{f}(x)}\\big]}^2$$\n",
    "\n",
    "нам важнее не ошибаться там, где больше вероятность X, просто потому, что там больше точек\n",
    "\n",
    "напомним форму записи\n",
    "$$E_{X}[F] = \\sum_x p(x) \\cdot F(x)$$\n",
    "\n",
    "распределение может быть совместным:\n",
    "$$E_{X,\\epsilon} = \\sum_x p(x,\\epsilon) \\cdot F$$\n",
    "\n",
    "это как раз наш случай, у нас $\\epsilon$ тоже случайное\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "есть вот такая дурацкая картинка:\n",
    "<img src=\"img/aims.png\" width=350>\n",
    "\n",
    "как читать:\n",
    "- центр мишени = реальная функция $f$ \n",
    "- выстрелы = оценки функции $\\hat{f}$, полученные как результат применения ML модели над разными сэмплами данных\n",
    "- чем дальше от центра, тем хуже оценка (выше суммарная ошибка предсказания)\n",
    "\n",
    "ноль идей, почему она двумерная, логичнее изобразить одномерной - тогда точки можно трактовать либо как результат применения модели на примере одного кейса, либо не ограничивая общности изобразить точкой полный набор данных\n",
    "\n",
    "но будем считать, что в 2D просто красивее мишень смотрится, а точка - это типа предсказания на 2 кейсах\n",
    "\n",
    "главный вывод:\n",
    "- у любой ML-модели есть две важных нативных характеристики: смещенность и разброс относительно разнообразия выборок\n",
    "\n",
    "эти две характеристики однозначно мапятся на другое абстрактное понятие \"сложность\" модели\n",
    "\n",
    "<img src=\"img/tradeoff_lines.png\" width=550>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
