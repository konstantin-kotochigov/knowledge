{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VC Dimension = measure of a <u>binary</u> classifier \"complexity\".\n",
    "\n",
    "It measures how much data points classifier can always classify perfectly (without errors). Of course this makes sense given that points are different (no duplicate points with contradicting labels).\n",
    "\n",
    "The authors use the term \"shattering\" = to divide space around the set of points into classes, so that each point is classified correctly.\n",
    "\n",
    "For basic estimators it's easy to determine VC-dimensionality. For example:\n",
    "- VC(d-dimensional perceptron) = d + 1\n",
    "- VC(d-dimensional perceptron without bias) = d\n",
    "\n",
    "<img src=\"img/vc1.png\" width=350>\n",
    "\n",
    "For more sophisticated estimators  it's usually a non trivial task.\n",
    "\n",
    "For example there is an upper bound estimate for **boosting classifier**:\n",
    "\n",
    "$$T\\cdot (D+1) \\cdot (3\\log(T\\cdot (D+1)) + 2)$$\n",
    "\n",
    "where T - number of trees, D - vc-dimension of each weak classifier\n",
    "\n",
    "\n",
    "Or there is an upper bound for **neural networks**:\n",
    "\n",
    "$$O(|E|\\cdot \\log(|E|))$$ where E - number of edges = number of weights of a network\n",
    "\n",
    "### Statistical learning\n",
    "VC-dimension appeared to be of good use in risk assessment in classification theory.\n",
    "\n",
    "Let's denote: \n",
    "- $D$ = VC-dimension of classifier\n",
    "- $N$ = size of a train dataset\n",
    "- $1-\\eta$ = confidence level\n",
    "\n",
    "Then we can compute an interval (an upper bound) for a Test error rate:\n",
    "\n",
    "$$\\Pr \\left(\\text{test error} \\leqslant  \\text{training error} + \\sqrt{ \\frac{1}{N} \\left [ D \\left (\\log \\left (\\tfrac{2N}{D} \\right )+1 \\right )-\\log \\left (\\tfrac{\\eta}{4} \\right ) \\right]} \\, \\right )= 1 - \\eta$$\n",
    "\n",
    "In other words for any classifier we can get an estimate of how exactly Test error would be larger then Train error.\n",
    "\n",
    "<img src=\"img/vc2.png\" width=500>\n",
    "\n",
    "In the pictue above VC dimernsionality depends on model complexity. Higher complexity (for example, max_depth in decision tree) = higher VC dimensionality.\n",
    "\n",
    "If we look at this formula, it perfectly makes sense:\n",
    "- With $N \\rightarrow \\infty$ the Test error converges to Train error\n",
    "\n",
    "        since the difference between sample and population vanishes\n",
    "\n",
    "- With $D \\rightarrow \\infty$ the Test error get larger\n",
    "\n",
    "        estimator is complex => we are more prone to overfitting\n",
    "- With $\\eta \\rightarrow 1$ (we weaken the strictness of the estimate) interval is getting smaller"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
