{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как запоминать? Как рассказывать на лекции?\n",
    "\n",
    "1. Функции потерь помогают прицелиться в наблюдаемое значение параметра p\n",
    "2. Параметризуем p через x => лосс меняется, но не сильно - получаем логистическую регрессию\n",
    "3. Детальнее, как выглядит оптимизация лоса для логрега\n",
    "4. Обощение на произвольное распределение Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Loss\n",
    "\n",
    "Logistic Loss = Negative Loglikelihood = Binary cross-entropy\n",
    "\n",
    "$$Log{L(p)} = \\sum_{i=1}^{N} \\bigg[y_i \\cdot log{p_i} + (1-y_i) \\cdot \\log{(1-p_i)}\\bigg]$$\n",
    "\n",
    "#### Derivation\n",
    "Suppose we have a set of binary variables $\\overline{y} = \\{y_1, y_2 ... y_n\\}$, where $y_i \\sim B(p)$ - Bernoulli random variable with parameter $p$\n",
    "\n",
    "*The most typical example of Bernoulli trial is a coin flip (p=1/2)\n",
    "\n",
    "In most of the problems we do not know parameter $p$ but we can infer it. It is typically done by maximizing likelihoood of the observed data\n",
    "\n",
    "Recall the likelihood function $l(\\theta)$ = the probability of the data given the parameter $\\theta$: $P(x | \\theta)$\n",
    "<img src=\"img/logistic_mle1.png\" width=250>\n",
    "\n",
    "Maximium function value refers to the most probable parameters set $\\theta_{ML}$\n",
    "\n",
    "---\n",
    "\n",
    "The likelihood of one Bernoulli trial is\n",
    "$$L(p) = \\begin{cases}\n",
    "    p,& \\text{if y = 1}\\\\\n",
    "    1-p,& \\text{if y = 0}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Or in one row \n",
    "$$L(p) = p^{y} \\times (1-p)^{(1-y)}$$\n",
    " \n",
    "Likelihood function of one Bernoulli trial can be illustrated in 3-D where L = (x,p)<br> Here $x \\in \\{0,1\\}$ and $p \\in R$\n",
    "- Red line = P(0|p)\n",
    "- Blue line = P(1|p)\n",
    "<img src=\"img/logistic_mle2.png\" width=200>\n",
    "\n",
    "In this case MLE maximization is straitforward: just set p = y and it gonna be the most probable model\n",
    "\n",
    "---\n",
    "\n",
    "Now take the likelihood of a series of Bernoulli trials, say<br>[0,1,0,0,1,1,1,0,1,0,1 ...]<br>\n",
    "\n",
    "Here 3D illustration is harder, since it is not clear how to order $y$ combinations along the $x$ axis. But we can slice on this particular combination $\\overline{y}$=[0,1,0,0,1,1,1,0,1,0,1 ...] and intuitively the likelihood will be a  bell with its maximum equals the rate of \"ones\" $p=\\frac{|y|}{N}$\n",
    "\n",
    "More formally\n",
    "\n",
    "$$L(p) = \\prod_{i=1}^{N} P(y_i|p) = \\prod_{i=1}^{N} p^{y_i} \\cdot (1-p)^{1-y_i} \\rightarrow \\max_{p}$$\n",
    "\n",
    "Or, since trials are independent, we can group mutliplcations. Thus the probability of the series gonna be:\n",
    "\n",
    "$$L(p) = p^{|y|} \\cdot (1-p)^{N-|y|} \\rightarrow \\max_{p}$$\n",
    "\n",
    "*NOTE that if we multiply by $\\binom N {|y|}$ we'll get probability for the Binomial = P of the series with exact number of \"ones\"\n",
    "\n",
    "It is easier to optimize sums => apply logarithm and simplify to make a Log-Likelihood\n",
    "\n",
    "$$L(p) = \\log{\\bigg[\\prod_{i} p^{y_i} \\cdot (1-p)^{1-y_i}}\\bigg] = \\sum_{i=1}^{N} \\bigg[ \\log{p^{y_i}} + \\log{(1-p)^{1-y_i}}\\bigg] = \\sum_{i=1}^{N} \\bigg[y_i \\cdot log{p} + (1-y_i) \\cdot \\log{(1-p)}\\bigg]$$\n",
    "\n",
    "NOTE when we take logarithm, we make L negative. So by maximizing we make L less negative as possible.\n",
    "\n",
    "In Machine Learning it is more common to use loss functions rather than do P maximization, so we can alter the sign and solve a minimization problem\n",
    "\n",
    "$$V(y,p)= - \\sum_{i=1}^{N} \\bigg[y_i \\cdot log{p} + (1-y_i) \\cdot \\log{(1-p)}\\bigg] \\rightarrow \\min_{p}$$\n",
    "\n",
    "Hence,<br>\n",
    "we showed that finding a proper model for a binary variable is equivalent to maximizing its loglikelihood OR minimizing the logistic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood\n",
    "\n",
    "If $p$ is a single parameter in the model we can solve it analytically. Let's prove formally that $L(p)$ attains maximum at the Y's sample mean\n",
    "\n",
    "__Proof__\n",
    "\n",
    "Take the loglikelihood\n",
    "$$L(p) = \\sum_{i=1}^{N} \\bigg[y_i \\cdot log{p} + (1-y_i) \\cdot \\log{(1-p)}\\bigg]$$\n",
    "\n",
    "Get rid of the sums to make expression more consise\n",
    "\n",
    "$$L(p) = |y| \\cdot log{(p)} + (N - |y|) \\cdot log{(1-p)}$$\n",
    "\n",
    "\n",
    "Now let's write the first and second order conditions for the maximum. First derivative becomes\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{p}} = \\frac{1}{p} \\cdot |y| - \\frac{1}{1-p} \\cdot (n - |y|) = 0 \\\\\n",
    "$$\n",
    "\n",
    "Or if we rearrange\n",
    "$$\\frac{1-p}{p} = \\frac{n - |y|}{|y|}$$\n",
    "\n",
    "This equals to\n",
    "$$\\frac{1}{p} - 1  = \\frac{n}{|y|} - 1$$\n",
    "\n",
    "Thus necessary condition for maximum is that p equals the sample average of $y$\n",
    "\n",
    "$$p  = \\frac{|y|}{n} = \\frac{1}{n}\\sum_{i=1}^N y_i$$\n",
    "\n",
    "To prove that it will be the maximum (not saddle or minimum) let's check the Second order conditions which takes the form of\n",
    "\n",
    "$$\\frac{\\partial^2{L}}{\\partial{p^2}} = \\frac{1}{p^2} \\cdot \\sum y_i + \\frac{1}{(1-p)^2} \\cdot \\sum (1-y_i) < 0\n",
    "$$\n",
    "\n",
    "It's easy to see that this condition is always matched\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__NOTE__ Sort of similar logic is applied when training quantile regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In logistic regression we model $p$ as a sigmoid $\\sigma$ over the linear regression of the features $x$\n",
    "\n",
    "$$p = \\sigma(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-(w_0 + w_1 x_1 + w_2 x_2 ... + w_n x_n)}}$$\n",
    "\n",
    "The dependency chain becomes a little more complicated\n",
    "$X \\rightarrow z \\rightarrow P$\n",
    "\n",
    "Task: find parameters $w$ that make the logloss between p and y mimimal\n",
    "\n",
    "<img src=\"img/logistic_intro.png\" width=500>\n",
    "\n",
    "Unlike logloss and linear part which are convex, sigmoid is not convex\n",
    "\n",
    "\n",
    "If we plug in the expression for $P=\\sigma(f(w,x))$ we can rewrite the Logistic Loss for regression task\n",
    "\n",
    "$$L(y,z) = log{(1+e^{-y \\cdot z})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Loss\n",
    "\n",
    "Margin-based loss = symmetric variant of the loss function, where $z=X\\beta$ fo retiher class is plotted over the X axis\n",
    "\n",
    "Take the logistic loss:\n",
    "\n",
    "$$L=\\begin{cases}\n",
    "\\log{(p)}, \\text{ if } y=1 \\\\\n",
    "\\log{(1-p)}, \\text{ if } y=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Plug-in p as a sigmoid over $z = (w,x)$ to get:\n",
    "\n",
    "$$L=\\begin{cases}\n",
    "\\log{(p)} = \\log{\\big(\\frac{1}{1+e^{-z}}\\big)} = - \\log{\\big(1+e^{-z}\\big)} \\\\\n",
    "\\log{(1-p)} = \\log{\\big(\\frac{1}{1+e^{z}}\\big)} = - \\log{\\big(1+e^{z}\\big)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Observation = the loss is symmetric under new variable $z$\n",
    "\n",
    "Let's use $y \\in \\{-1,1\\}$ as a class indicator. Thus we can rewrite in one row as $$L=\\log{(1+e^{-y \\cdot z})}$$\n",
    "\n",
    "__Important__ class tag $y$ is used as indicator only => it does not affect the loss\n",
    "\n",
    "\n",
    "\n",
    "Hence,<br>\n",
    "we reformulate the logistic Regresson task as:\n",
    "$$w = \\underset{p}{\\mathrm{argmin}} \\bigg( \\sum_{i=1}^N \\log{\\big(1 + e^{-y x w}\\big)} \\bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with some other popular margin-based variants of losses\n",
    "\n",
    "__NOTE__ logistic loss is defined up to lograithm base. Here plotted for $log_2(x)$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/logistic_margin.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative formulations\n",
    "\n",
    "Sometimes logistic regression is set in \"Logit\" form\n",
    "$$X\\beta = \\log\\bigg(\\frac{p}{1-p}\\bigg)$$\n",
    "\n",
    "Here logit = inverse of the sigmoid - a function that links linear domain to the response domain\n",
    "\n",
    "It defines the same set of points but in slightly different notation\n",
    "\n",
    "$X \\rightarrow z \\leftarrow P$\n",
    "\n",
    "\n",
    "\n",
    "### Solution\n",
    "\n",
    "Let's define some notaton <br> $y \\in \\{0,1\\}$ - correct output <br> $\\hat{y} \\in (0,1)$ - predicted output <br> $z = \\sum_{i=1}^{N} w_i x_i$ - linear part of the regression\n",
    "\n",
    "Recall the logistic loss:\n",
    "\n",
    "$$L = - \\sum_{i=1}^{N} \\bigg[ y_i \\log(\\hat{y_i}) - (1-y_i) \\log{(1-\\hat{y_i})} \\bigg]$$\n",
    "\n",
    "Let's compute the gradient for a single loss instance by applying the chain rule:\n",
    "\n",
    "$$\\frac{\\delta L}{\\delta w_i} = \\frac{\\delta L}{\\delta \\hat{y}} \\cdot \\frac{\\delta \\hat{y}}{\\delta z} \\cdot \\frac{\\delta z}{\\delta w_i}$$\n",
    "\n",
    "Individual derivatives will be the folowing:\n",
    "\n",
    "1. Derivative of logistic loss: $\\frac{\\delta L}{\\delta \\hat{y}} = \\bigg( \\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}} \\bigg)$<br><br>\n",
    "\n",
    "2. Derivative of the sigmoid: $\\frac{\\delta \\hat{y}}{\\delta w} = (1-\\hat{y}) \\cdot \\hat{y}$<br><br>\n",
    "\n",
    "3. Derivative of the linear part $\\frac{\\delta z}{\\delta w_i} = x_i$<br><br>\n",
    "\n",
    "\n",
    "Now let's plug them in and simplify the expression\n",
    "\n",
    "$$\\frac{\\delta L}{\\delta w_i} = \\bigg(\\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}}\\bigg) \\cdot (1-\\hat{y}) \\cdot \\hat{y} \\cdot x = (y-\\hat{y}) \\cdot x$$\n",
    "\n",
    "\n",
    "So the gradient step gonna be\n",
    "$$w_i := w_i - \\eta \\cdot (y-\\hat{y}) \\cdot x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Loss and Cross-entropy\n",
    "\n",
    "Let's recall some definitions from information theory\n",
    "\n",
    "Entropy of a distribution $H(P) = - \\sum_{i=1}^N P_i \\cdot \\log{P_i}$\n",
    "\n",
    "Cross-entropy of two distributions is $H(P,Q) = - \\sum_{i=1}^N P_i \\cdot \\log{Q_i}$\n",
    "\n",
    "One can clearly see that when $N=2$ (distributions are binary) cross-entropy takes the same form as a logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{KL} = \\sum_{i} P_i \\cdot \\log{\\frac{P_i}{Q_i}} = \\sum_{i} P_i \\cdot \\log{P_i} - \\sum_{i} P_i \\cdot \\log{Q_i} = H(P) - H(P,Q)$$\n",
    "\n",
    "How do those distributions relate to binary distributions?<br>\n",
    "- $P = y$<br>real distribution - defined by \"true\" parameter p=P<br><br>\n",
    "- $Q = \\hat{y}$<br>predicted distribution - defined by candidate parameter p=Q<br>\n",
    "\n",
    "Here $H(p)$ is defined over observed data thus constant. To minimize $H(P,Q)$ means to minimize $D_{KL}$\n",
    "\n",
    "Hence,<br>\n",
    "to find proper distribution for $\\hat{y}$ = to minimize a gap between the \"real\" and \"predicted\" = to minimize $D_{KL}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convexity\n",
    "\n",
    "Logistic Loss is convex\n",
    "\n",
    "#### By $\\hat{y}$\n",
    "\n",
    "Logarithm is a concave transform<br>\n",
    "Logloss is a negative weighted sum of logaritms => convex\n",
    "\n",
    "\n",
    "\n",
    "#### By $w$\n",
    "\n",
    "Function is convex <=> its second derivative is postiive everywhere. Let's check\n",
    "\n",
    "$$L'(z) = \\frac{d}{dz} \\bigg[\\log(1+e^{−z}) \\bigg] = -\\frac{e^{-z}}{1+e^{-z}} = - \\frac{1}{1+ e^{z}} = −\\sigma(z)$$\n",
    "\n",
    "$$L''(z) = \\frac{d}{dz} (- \\sigma(z)) = -\\frac{e^{z}}{(1+e^{z})^2} \\ge 0$$\n",
    "\n",
    "NOTE Despite sigmoid is not convex the Logistic loss is convex => gradient methods converge to the local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit model\n",
    "\n",
    "Logit is the inverse of sigmoid function. It maps probability $p \\in [0,1]$ back to $\\mathbb{R}$\n",
    "\n",
    "\n",
    "Alternative way to pose a logistic regression problem is through logit function\n",
    "\n",
    "$$\\log{\\bigg(\\frac{p}{1-p}\\bigg)}=w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n$$\n",
    "\n",
    "$Logit = \\sigma^{-1}(p) = \\log{\\bigg(\\frac{p}{1-p}\\bigg)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Model\n",
    "\n",
    "We have linear regression and logistic regression. They share many common elelments.\n",
    "Let's define some universal regression model that would model\n",
    "\n",
    "GLM model is: $E[y|x] = \\mu = g^{-1}(X\\beta)$\n",
    "\n",
    "$E[y|x]$ = mean of the response variable<br>\n",
    "$g^{-1}$ = link function<br>\n",
    "$\\mu$ = mean function\n",
    "\n",
    "In general they don't have closed-form (aka analytical) solution => are fit with IRLS procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bishop\n",
    "\n",
    "Тут чуть-чут другой взгляд на модель, более общий вероятностный. В каждой точке пространства x есть латентная переменная: номер класса. И каждый из двух классов может производить ответ с какой-то вероятностью\n",
    "\n",
    "$$P(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)} = \\frac{1}{1+exp \\big[\\log \\frac{P(x|C_2)P(C_2)}{P(x|C1)P(C_1)}\\big]} = \\frac{1}{1+exp \\big[- \\log \\frac{P(x|C_1)P(C_1)}{P(x|C2)P(C_2)}\\big]} = \\frac{1}{1+e^{-\\alpha}}$$\n",
    "\n",
    "где $\\alpha=\\log{\\big(\\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_1)}\\big)}$ - это логарифм отношения правдоподобий классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
