{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ensemble learning\n",
    "\n",
    "base learner / weak learner\n",
    "\n",
    "#### bootstrapping\n",
    "Bootstrapping = многократное сэмплирование из доступной выборки с целью моделирования свойств всей популяции\n",
    "\n",
    "Для чего может быть полезно:\n",
    "- усреднение предикта (bootstrap aggregation)\n",
    "- кросс-валидация (OOB estimation - проверяем на том, что не попало в сэмпл)\n",
    "- оценка распределения шума генеративного процесса\n",
    "\n",
    "сэмплируется обычно с повторениями\n",
    "\n",
    "#### почему эффективно\n",
    "если base learners независимые, то ошибки тоже независимые / нескоррелированные => их вклад в bias-variance разброс становится меньше\n",
    "\n",
    "Три кита ML ансамблирования:\n",
    "- bagging\n",
    "- boosting\n",
    "- stacking\n",
    "\n",
    "<img src=\"img/ensembles1.jpg\" width=750>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bagging\n",
    "Bagging = boostrap aggregation\n",
    "\n",
    "#### random forest\n",
    "random forest = bagging + RSM (random subset model)\n",
    "\n",
    "base learners произвольно сложные => малый bias\n",
    "независимые и повторяются много раз => малый variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stacking\n",
    "навешивание мета-модели поверх выходов базовых моделей (часто разных)\n",
    "\n",
    "<img src=\"img/stacking.png\" width=500>\n",
    "\n",
    "важно обучать базовые и мета-модель на разных сетах (мету ok обучать на холдауте базовых)\n",
    "\n",
    "#### meta learning\n",
    "связанная область - подбор моделей\n",
    "\n",
    "#### majority voting\n",
    "для классификации предикт обычно выбирают по приницпу, за кого больше голосов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boosting\n",
    "\n",
    "основывается на концепте аддитивных ансамблей (adaptive learning), где предикт складывается из набора функций. в общей формулировке выглядит так:\n",
    "\n",
    "<img src=\"img/additive_model.png\" width=500>\n",
    "\n",
    "идея бустинга = итеративно добавляем классификаторы так, чтобы каждый новый уменьшал суммарную ошибку ансамбля. \n",
    "\n",
    "за счет эффекта накопления ошибок неправильно классифицируемые примеры имеют больший вес в функции потерь $L$ и за счет этого новые классификаторы учатся правильно предиктить именно эти примеры\n",
    "\n",
    "*отсюда термин бустинг - на каждой итерации мы повышаем вес неправильно классифицированных примеров\n",
    "\n",
    "__градиентный бустинг__ = итеративно добавляем классификаторы в ансамбль так, чтобы предикт каждого нового дерева совпадал с антиградиентом уже построенного ансамбля. тогда их комбинация будет наилучшим образом приближать точное решение\n",
    "\n",
    "*антиградиент именно по предикту, не по отдельным параметрам base learner-а, в этом фишка метода\n",
    "\n",
    "<img src=\"img/boosting.png\" width=750>\n",
    "\n",
    "__Adaboost__ = в качестве loss-функции берем Exponential\n",
    "\n",
    "за счет выбора конкретной функции, шаг оптимизации имеет аналитическое решение и его используют в качестве нового классификатора\n",
    "\n",
    "вес классификатора в ансамбле определяется после построения самого классификатора  \n",
    "\n",
    "We notice that $yh = 1$ for correct classifications and $yh = -1$ for incorrect classifications. So we rewrite:\n",
    "\n",
    "Выпишем итоговую функцию потерь:\n",
    "\n",
    "$$L = \\sum_{correct} w^m e^{-\\alpha_m} + \\sum_{incorrect} w^m e^{\\alpha_m}$$\n",
    "\n",
    "$$L = \\sum_{all} \\big( w^m e^{-\\alpha_m} \\big) + \\sum_{incorrect} {\\big(w^m e^{\\alpha_m} - w^m e^{-\\alpha_m}\\big)}$$\n",
    "\n",
    "Для минимизации приравниваем нулю производную $\\frac{\\partial{L}}{\\partial{\\alpha}} = 0$ и получаем выражение для $\\alpha$\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = \\frac{1}{2} \\log{ \\frac{\\sum_{correct}w^m}{\\sum_{incorrect}w^m}} = \\frac{1}{2} \\log{\\frac{1-\\epsilon}{\\epsilon}}\n",
    "$$\n",
    "\n",
    "__Gradient Boosting__ = общий алгоритм для произвольной Loss-функции и произвольных base learners $h(\\theta)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
