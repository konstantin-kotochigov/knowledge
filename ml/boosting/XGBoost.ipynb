{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data class\n",
    "\n",
    "In XGBoost the container class for datasets is called DMatrix. It contains both features and target variable.\n",
    "\n",
    "<img src=\"img/xgboost_data.png\" width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class\n",
    "\n",
    "Booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics\n",
    "\n",
    "XGBoost = eXtreme Gradient Boosting\n",
    "\n",
    "They introduce regularization:\n",
    "\n",
    "$$\\text{obj}(\\theta) = L(\\theta) + \\Omega(\\theta)$$\n",
    "\n",
    "n cases and t trees.\n",
    "\n",
    "$$\\text{obj} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t)}) + \\sum_{i=1}^t\\Omega(f_i)$$\n",
    "\n",
    "Let's assume regression trees and MSE as a loss function:\n",
    "\n",
    "$$\\text{obj}^{(t)} = \\sum_{i=1}^n (y_i - (\\hat{y}_i^{(t-1)} + f_t(x_i)))^2 + \\sum_{i=1}^t\\Omega(f_i) $$\n",
    "\n",
    "Rewrite the function:\n",
    "\n",
    "$$\\sum_{i=1}^n [2(\\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \\Omega(f_t) + \\mathrm{constant}$$\n",
    "\n",
    "For general Loss function.\n",
    "\n",
    "On each step we've got a model $y_{t-1}(x)$ and L calculated on this model $L(y_{t-1})$.\n",
    "\n",
    "New iteration corrects our model a bit: $y_{t-1}(x) + f_t(x)$.\n",
    "So we need to update L correspondingly.\n",
    "We can do it approximately using Taylor expansion with 2 order terms - gradient and hessian.\n",
    "New L value will be equal to the old value $L(y_{t-1}(x))$ plus second order approximation:\n",
    "\n",
    "$$\\text{obj}^{(t)} = \\sum_{i=1}^n [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)] + \\Omega(f_t) + \\mathrm{constant}$$\n",
    "\n",
    "Here we denoted:\n",
    "- h = hessian\n",
    "- g = gradient\n",
    "\n",
    "When defining custom loss functions in XGB, they must return gradient and hessian.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
