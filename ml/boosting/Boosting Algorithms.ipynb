{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is process of converting a set of weak classifiers to string classifier.\n",
    "\n",
    "The problem was first posed in PAC (probably approximately correct) learning.\n",
    "\n",
    "Shapire (AT&T) in 1989 came up with the first boosting algorithm. Later .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Stagewise Additive Modeling\n",
    "\n",
    "Boosting process is based on the [additive modeling](https://en.wikipedia.org/wiki/Additive_model#:~:text=In%20statistics%2C%20an%20additive%20model,class%20of%20nonparametric%20regression%20models) concept, described by Friedman in 1981.\n",
    "\n",
    "On each iteration a new (weak) estimator $b(x)$ is fitted to minimize the loss function for the whole function F(x).\n",
    "We optimize both for parameters of the classifier $b_m(x)$ and it's relative weight in the ensemble $\\beta_m$.\n",
    "\n",
    "Loss function is arbitrary so we don't define the optimization process yet.\n",
    "\n",
    "<img src=\"img/additive_model.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost described in 1995 by Schapire and Friedman.\n",
    "\n",
    "Here we assume \n",
    "- h(x) is a binary classifier with 2 classes: {-1,+1}\n",
    "- Loss function is exponential\n",
    "\n",
    "In that case the additive model above turns to this algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/adaboost.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T05:06:31.160457Z",
     "start_time": "2021-01-03T05:06:31.151702Z"
    }
   },
   "source": [
    "### In plain english\n",
    "\n",
    "0. Initialize weight for each case as $(\\frac{1}{n},\\frac{1}{n} \\cdots \\frac{1}{n})$\n",
    "\n",
    "While not convergence_criteria:\n",
    "1. Train a new \"weak\" classifier $h_{m}$ that minimizes the weighted loss function\n",
    "2. Update the weights in the dataset\n",
    "3. Renormalize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the decision function as an additive model:\n",
    "\n",
    "$$ F_m(x) = \\alpha_1 h_1(x) + \\alpha_2 h_2(x) + \\alpha_1 h_1(x) + \\cdots + \\alpha_{m-1} h_{m-1}(x) $$\n",
    "\n",
    "We can also rewrite it as an iterative process. Suppose we already have $F_{m-1}$:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\alpha_{m-1}h_{m-1}(x)$$\n",
    "\n",
    "The loss function for this decision function would be $$-y(x)F_m(x) = \n",
    "\\begin{cases}\n",
    "    -1,& \\text{when prediction is correct}\\\\\n",
    "    1,& \\text{when prediction is incorrect}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Loss function minimization = on each step we train a \"weak\" classifier $h_{m-1}(x)$ that minimizes the loss function.\n",
    "\n",
    "Let's rewrite the loss function as an exponent so that it would be easier to optimize\n",
    "\n",
    "$$L = \\sum e^{-y(x) F_m(x)}$$\n",
    "\n",
    "Plug in the decision function:\n",
    "$$L = \\sum e^{-yF_{m-1}(x) - \\alpha_{m-1}h_{m-1}}$$\n",
    "\n",
    "Let's factorize the exponent:\n",
    "$$L = \\sum e^{-yF_{m-1}(x)} e^{-y\\alpha_{m-1}h_{m-1}}$$\n",
    "\n",
    "Let's denote $e^{yF_{m-1}}$ as weight $w^m$;\n",
    "$$L = \\sum w^m e^{-y\\alpha_{m-1}h_{m-1}} $$\n",
    "\n",
    "Now we need to optimize for $(\\alpha, h(x|\\theta))$. \n",
    "\n",
    "Luckily we can do it separately - first for h(x), then for $\\alpha$. Notice the necessary condition for minimum of L is \n",
    "\n",
    "$$h(x) = arg min \\sum w_i I(h(x) != y_i)$$ regardless of $\\alpha$.\n",
    "\n",
    "This is equivalent to just fitting h(x) on a weighted dataset.\n",
    "\n",
    "After that we can optimize for $\\alpha$.\n",
    "\n",
    "We notice that $yh = 1$ for correct classifications and $yh = -1$ for incorrect classifications. So we rewrite:\n",
    "\n",
    "$$L = \\sum_{correct} w^m e^{-\\alpha_m} + \\sum_{incorrect} w^m e^{\\alpha_m}$$\n",
    "\n",
    "$$L = \\sum_{all} \\big( w^m e^{-\\alpha_m} \\big) + \\sum_{incorrect} {\\big(w^m e^{\\alpha_m} - w^m e^{-\\alpha_m}\\big)}$$\n",
    "\n",
    "The alpha that minimizes the loss function is:\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = \\frac{1}{2} \\log{ \\frac{\\sum_{correct}w^m}{\\sum_{incorrect}w^m}} = \\frac{1}{2} \\log{\\frac{1-\\epsilon}{\\epsilon}}\n",
    "$$\n",
    "\n",
    "So, the resulting prediction can be achieved using a Weighted Voting with weights $\\alpha=(\\alpha_1,\\alpha_2 \\cdots \\alpha_m)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Stump (Decision Stump) = 1-level decision Tree.\n",
    "\n",
    "## Mutli-class Adaboost\n",
    "\n",
    "### SAMME\n",
    "\n",
    "[paper](https://web.stanford.edu/~hastie/Papers/samme.pdf)\n",
    "\n",
    "SAMME = **S**tagewise **A**dditive **M**odeling with **M**ulticlass **E**xponential loss function\n",
    "\n",
    "SAMME is an extension of Adaboost for multiple classes.\n",
    "\n",
    "There are two flavours of the algorithm: \n",
    "- SAMME (for discrete estimators)\n",
    "- SAMME.R (for real estimators)\n",
    "\n",
    "SAMME\n",
    "\n",
    "\n",
    "\n",
    "When K = 2 algorithm is the same as Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T05:37:28.937967Z",
     "start_time": "2021-01-03T05:37:28.928740Z"
    }
   },
   "source": [
    "### Important algoritm parameters\n",
    "- num_estimators\n",
    "- learning rate ($\\eta$) - a decreasing multiplier that shrinks the contribution ($\\alpha$) of each new weak classifier\n",
    "\n",
    "High learning rate helps fight overfitting.\n",
    "\n",
    "https://stats.stackexchange.com/questions/82323/shrinkage-parameter-in-adaboost/355632#355632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn\n",
    "\n",
    "There are 3 versions:\n",
    "- AdaBoost\n",
    "- GradientBoosting\n",
    "- HistogramBoosting\n",
    "\n",
    "In Sklearn AdaBoost there is also Feature-importance calculation performed using permutation-based approach.\n",
    "\n",
    "### AdaBoost classifier\n",
    "This is the implementation of SAMME Adaboosting.\n",
    "\n",
    "Basic Parameters:\n",
    "- weak estimator (default = decision stump)\n",
    "- n_estimators\n",
    "- learning_rate\n",
    "\n",
    "Algorithm params:\n",
    "- SAMME\n",
    "- SAMME.R\n",
    "\n",
    "You can call <u>staged_predict_proba()</u> which returns probs for each of the estimators\n",
    "\n",
    "### GradientBoostingClassifier\n",
    "\n",
    "Basic params\n",
    "- estimator = Decision Tree\n",
    "- n_extimators\n",
    "- learning_rate\n",
    "\n",
    "\n",
    "- loss - loss function\n",
    "    - deviance = negative log-likelihood, for classification\n",
    "    - exponential, for binary (same as in AdaBoost)\n",
    "\n",
    "Stochastic params\n",
    "- subsample\n",
    "- max_features\n",
    "\n",
    "Tree params\n",
    "- max_depth\n",
    "- max_learff_nodes\n",
    "- min_impurity_split\n",
    "\n",
    "Process params\n",
    "- warm_start - allows to continue training\n",
    "\n",
    "### HistogramBoostingClassifier\n",
    "\n",
    "Inspired by lightGBM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM Regularization\n",
    "\n",
    "Ways to reduce deviance on Test:\n",
    "- set lower learning rate to enable shrinkage\n",
    "- use random subsample for training (bagging) - This is what they do in Stochastic Gradient Descent\n",
    "- use random feature selection (bagging)\n",
    "\n",
    "<img src=\"img/regularization.png\" width=500>\n",
    "\n",
    "[Code here](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightGBM\n",
    "\n",
    "2016\n",
    "\n",
    "Microsoft implementation of GBM\n",
    "\n",
    "https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost, Yandex\n",
    "\n",
    "2017\n",
    "\n",
    "Basic params:\n",
    "- n_estimators\n",
    "- learning rate\n",
    "- max_depth\n",
    "\n",
    "Metric params:\n",
    "- loss_function\n",
    "- eval_metric\n",
    "\n",
    "One of the main features of CatBoost is ability to use categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
