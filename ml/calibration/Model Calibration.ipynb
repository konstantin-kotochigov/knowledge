{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When you pose your classification task as a scoring problem, your classification model returns **Probability**, not a class label. Such probability is always an estimate.\n",
    "\n",
    "- Well-calibrarted models estimate class probabilities accurately\n",
    "- Poorly-calibrated models often miss. They could <u>overestimate</u> or <u>underestimate</u> probabilities\n",
    "\n",
    "**Model calibration** = the process of correcting probability estimates to make them more precise.\n",
    "\n",
    "### When probabilty calibration is NOT necessary\n",
    "\n",
    "- When you don't do scoring and return class labels immediately.\n",
    "- When you need probabilities only to rank the output\n",
    "\n",
    "### When probabilty calibration IS necessary\n",
    "\n",
    "- When you care about accurate absolute values\n",
    "    \n",
    "For example, there could be some kind of probability threshold - you might react only to high probability cases.\n",
    "\n",
    "### How to determine poor calibration\n",
    "\n",
    "You just apply your model to TEST dataset and assess how accurate the probabilities are using labeled data.\n",
    "\n",
    "In order to assess probability we need to aggregate it somehow => we usually distribute observations into 10 buckets according to their probabilities. Like this (bin1 = [0.0-0.1), bin2 = [0.1-0.2) ... etc)\n",
    "\n",
    "**Reliability (calibration) plots** show how exactly error depends on probability value\n",
    "- X = probability bucket\n",
    "- Y = precision (positive class rate in the corresponding bucket) \n",
    "\n",
    "If probability estimates are perfect, positive class rate should exactly match dotted diagonal line.\n",
    "\n",
    "<img src=\"img/calibration_curve.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What  metrics to use\n",
    "\n",
    "Let's denote\n",
    "\n",
    "- acc = precision in bucket $B_m$\n",
    "- conf = average probability in bucket $B_m$\n",
    "\n",
    "Then calibration error = |acc - conf|\n",
    "\n",
    "----------\n",
    "\n",
    "**Expected Calibration Error** is just an average calibration error weighted by number of examples in each bucket\n",
    "\n",
    "$$ ECE = \\sum_{m=1}^{10} \\frac{|B_m|}{10} |acc(B_m) - conf(B_n)| $$\n",
    "\n",
    "It's a sampled variant. Populational variant would look like this:\n",
    "\n",
    "$$ECE = E_p[ P(Y=y|P=p)] - p $$\n",
    "\n",
    "----------\n",
    "\n",
    "**Maximum calibration error** is a maximum calibration error among all bickets\n",
    "\n",
    "$$ MCE = {max}_{m=1..10} \\big( |acc(B_m) - conf(B_n)| \\big) $$\n",
    "\n",
    "----------\n",
    "\n",
    "**Brier Score** is a mean squared error of prediction\n",
    "\n",
    "Let's denote\n",
    "\n",
    "- $p_i$ = predicted probability of target class (interval [0.0,1.0])\n",
    "- $o_i$ = observed class label (0 for \"negative\" class and 1 for \"target\" class)\n",
    "\n",
    "$$BS = \\frac{1}{N}\\sum_{i=1}^N (p_i - o_i)^2$$\n",
    "\n",
    "Note that Brier score is a simpler alternative for the standard negative log-likelihood / logloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How different models behave\n",
    "\n",
    "\n",
    "\n",
    "Poorly calibrated models usually behave like sigmoids - they tend to \n",
    "- overestimate LOW probabilities \n",
    "- underestimate HIGH probabilities\n",
    "\n",
    "Though that's not the rule\n",
    "\n",
    "#### Research\n",
    "\n",
    "In 2005 there was a [research](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf) released dedicated to how well different models are calibrated. They tested a number of SOTA models on different datasets.\n",
    "\n",
    "<img src=\"img/calibration_models.png\" width=700>\n",
    "\n",
    "#### Poorly-calibrated examples\n",
    "\n",
    "**Boosted Trees** demonstrate obvious sigmoid.\n",
    "Suppose we have some observation that we are confident about. Since in boosting we deal with ensemble of models, ALL classifiers should return the highest score. Which is not achievable in practice.\n",
    "\n",
    "**SVM** is a margin maximization method => it focuses on HARD examples, the ones that have probabilies around 0.5. It does nor pay too much attention to easier examples.\n",
    "\n",
    "#### Well-calibrated examples\n",
    "\n",
    "Becasuse of their nature **Logitic Regression** models usually are perferctly calibrated.\n",
    "\n",
    "Older and simpler **neutral networks** were also perfectly calibrated. But with all of the advancements of Deep Learning they are not calibrated anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calibrate\n",
    "\n",
    "### Platt Scaling\n",
    "\n",
    "[paper](https://www.researchgate.net/publication/2594015_Probabilistic_Outputs_for_Support_Vector_Machines_and_Comparisons_to_Regularized_Likelihood_Methods), 1999, Microsoft\n",
    "\n",
    "Just fit a new logistic regression on the output of the estimator. It will learn how to correct the output using sigmoid.\n",
    "\n",
    "In this  he proposes the solution for bad behavior of SVMs. In those times SVMs were the biggest problem.\n",
    "\n",
    "### Isotonic regression\n",
    "\n",
    "Isotonic regression is a piecewise constant regression that is used for modeling monotonically increasing data.\n",
    "\n",
    "<img src=\"img/isotonic.png\" width=500>\n",
    "\n",
    "It is the same approach as in Platt, but isotonic regression does not assume sigmoid dependency => it is a more general aproach. The drawback - it is more succeptible to overfitting on small datasets.\n",
    "\n",
    "You define the number of intervals of the model and them optimize the loss function\n",
    "$$  $$\n",
    "\n",
    "### Temperature Scaling\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1706.04599.pdf), 2017\n",
    "\n",
    "Similar approach, but here it can be embedded right in the neural network.\n",
    "\n",
    "T (temperature) is a single parameter of the Net that linearly modifies the logit output **z** of the network (right before applying softmax).\n",
    "\n",
    "$$softmax = \\frac{e^{\\frac{z}{T}}}{\\sum_i e^{\\frac{z_i}{T}}}$$\n",
    "\n",
    "<img src=\"img/temperature_scaling.png\" width=500>\n",
    "\n",
    "\n",
    "It acts as a modifier (smoother or sharpener) for the softmax function. Perfectly smoothened softmax represents constant. Perfectly sharpened softmax is just a regular argmax (indicator function). Somewhere in between there is a normalizer that approximates output probabilities in a best way.  \n",
    "\n",
    "The optimal T value is achieved by stadard optimization process of the log-loss function during network training.\n",
    "\n",
    "### Probability Calibration Trees\n",
    "\n",
    "2017\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
