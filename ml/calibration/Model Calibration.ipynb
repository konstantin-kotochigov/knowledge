{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In classification tasks Propensity models return **probability**, not a class label.\n",
    "\n",
    "Predicting probability is a more complex task as the model works on a continuous scale. It might not be enough to focus on decision boundaries - you have to make the output precise everywhere along the scale. \n",
    "\n",
    "- Well-calibrarted models estimate class probabilities accurately\n",
    "- Poorly-calibrated models often miss. They could <u>overestimate</u> or <u>underestimate</u> probabilities\n",
    "\n",
    "**Model calibration** = the process of correcting probability estimates to make them more precise.\n",
    "\n",
    "When probabilty calibration is NOT necessary\n",
    "\n",
    "- When you don't do scoring and return class labels immediately.\n",
    "- When you need probabilities only to rank the output\n",
    "\n",
    "When it is\n",
    "\n",
    "- When you care about accurate absolute values\n",
    "    \n",
    "For example, there could be some kind of probability threshold - you might react only to high probability cases.\n",
    "\n",
    "#### How to test calibration quality\n",
    "\n",
    "By applying trained model to validation dataset and comparing predicted and real outputs. If model is well calibrated they must correlate.\n",
    "\n",
    "First predicted probabilities are aggregated into buckets. Then for each bucket we computey average real probability.\n",
    "\n",
    "**Reliability (calibration) plots** show how exactly error depends on probability value\n",
    "- X = probability bucket\n",
    "- Y = precision (positive class rate in the corresponding bucket) \n",
    "\n",
    "If probability estimates are perfect, positive class rate should exactly match dotted diagonal line.\n",
    "\n",
    "<img src=\"img/calibration_curve.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration metrics\n",
    "\n",
    "Let's denote\n",
    "\n",
    "- acc = precision in bucket $B_m$\n",
    "- conf = average probability in bucket $B_m$\n",
    "\n",
    "Then calibration error = |acc - conf|\n",
    "\n",
    "----------\n",
    "\n",
    "**Expected Calibration Error** is just an average calibration error weighted by number of examples in each bucket\n",
    "\n",
    "$$ ECE = \\sum_{m=1}^{10} \\frac{|B_m|}{10} |acc(B_m) - conf(B_n)| $$\n",
    "\n",
    "It's a sampled variant. Populational variant would look like this:\n",
    "\n",
    "$$ECE = E_p[ P(Y=y|P=p)] - p $$\n",
    "\n",
    "----------\n",
    "\n",
    "**Maximum calibration error** is a maximum calibration error among all bickets\n",
    "\n",
    "$$ MCE = {max}_{m=1..10} \\big( |acc(B_m) - conf(B_n)| \\big) $$\n",
    "\n",
    "----------\n",
    "\n",
    "**Brier Score** is a mean squared error of prediction\n",
    "\n",
    "Let's denote\n",
    "\n",
    "- $p_i$ = predicted probability of target class (interval [0.0,1.0])\n",
    "- $o_i$ = observed class label (0 for \"negative\" class and 1 for \"target\" class)\n",
    "\n",
    "$$BS = \\frac{1}{N}\\sum_{i=1}^N (p_i - o_i)^2$$\n",
    "\n",
    "Note that Brier score is a simpler alternative for the standard negative log-likelihood / logloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How different models behave\n",
    "\n",
    "\n",
    "\n",
    "Poorly calibrated models usually behave like sigmoids - they tend to \n",
    "- overestimate LOW probabilities \n",
    "- underestimate HIGH probabilities\n",
    "\n",
    "Though that's not the rule\n",
    "\n",
    "#### Research\n",
    "\n",
    "In 2005 there was a [research](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf) released dedicated to how well different models are calibrated. They tested a number of SOTA models on different datasets.\n",
    "\n",
    "<img src=\"img/calibration_models.png\" width=700>\n",
    "\n",
    "#### Poorly-calibrated examples\n",
    "\n",
    "**Boosted Trees** demonstrate obvious sigmoid.\n",
    "Suppose we have some observation that we are confident about. Since in boosting we deal with ensemble of models, ALL classifiers should return the highest score. Which is not achievable in practice.\n",
    "\n",
    "**SVM** is a margin maximization method => it focuses on HARD examples, the ones that have probabilies around 0.5. It does nor pay too much attention to easier examples.\n",
    "\n",
    "#### Well-calibrated examples\n",
    "\n",
    "Becasuse of their nature **Logitic Regression** models usually are perferctly calibrated.\n",
    "\n",
    "Older and simpler **neutral networks** were also perfectly calibrated. But with all of the advancements of Deep Learning they are not calibrated anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calibrate\n",
    "\n",
    "### Platt Scaling\n",
    "\n",
    "[paper](https://www.researchgate.net/publication/2594015_Probabilistic_Outputs_for_Support_Vector_Machines_and_Comparisons_to_Regularized_Likelihood_Methods), 1999, Microsoft\n",
    "\n",
    "__Idea:__ train and apply auxiliairy output correcting model (specifically 1-d logistic regression) which will map outputs to [0,1] and make any output more logistic-like\n",
    "\n",
    "$$f(x) \\rightarrow \\frac {1}{1+e^{a f(x)+b}}$$\n",
    "\n",
    "The process is the same as applying logistic function to a logit (linear output of a model) in logistic regression. \n",
    "\n",
    "$$t=\\beta _{0}+\\beta _{1}x \\rightarrow \\frac {1}{1+e^{-f(x)}}$$\n",
    "\n",
    "The only difference is there are also scaling parameters $a$ and $b$.\n",
    "\n",
    "Sigmoid gives perfect calibration curve => it will correct any flaws in model output\n",
    "\n",
    "Motivation was poor performance of SVM outputs (models were popular at the time).\n",
    "\n",
    "```\n",
    "gnb = GaussianNB()\n",
    "```\n",
    "\n",
    "### Isotonic regression\n",
    "\n",
    "Isotonic regression is a piecewise constant regression that is used for modeling monotonically increasing data.\n",
    "\n",
    "<img src=\"img/isotonic.png\" width=500>\n",
    "\n",
    "It is the same approach as in Platt, but isotonic regression does not assume sigmoid dependency => it is a more general aproach. The drawback - it is more succeptible to overfitting on small datasets.\n",
    "\n",
    "You define the number of intervals of the model and them optimize the loss function\n",
    "$$  $$\n",
    "\n",
    "### Temperature Scaling\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1706.04599.pdf), 2017\n",
    "\n",
    "Similar approach, but here it can be embedded right in the neural network.\n",
    "\n",
    "T (temperature) is a single parameter of the Net that linearly modifies the logit output **z** of the network (right before applying softmax).\n",
    "\n",
    "$$softmax = \\frac{e^{\\frac{z}{T}}}{\\sum_i e^{\\frac{z_i}{T}}}$$\n",
    "\n",
    "<img src=\"img/temperature_scaling.png\" width=500>\n",
    "\n",
    "\n",
    "It acts as a modifier (smoother or sharpener) for the softmax function. Perfectly smoothened softmax represents constant. Perfectly sharpened softmax is just a regular argmax (indicator function). Somewhere in between there is a normalizer that approximates output probabilities in a best way.  \n",
    "\n",
    "The optimal T value is achieved by stadard optimization process of the log-loss function during network training.\n",
    "\n",
    "### Probability Calibration Trees\n",
    "\n",
    "2017\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
