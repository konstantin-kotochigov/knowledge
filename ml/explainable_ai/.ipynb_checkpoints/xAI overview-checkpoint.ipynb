{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI\n",
    "---\n",
    "\n",
    "Explainable AI (XAI) refers to techniques, methods, and frameworks that make the predictions, decisions, and behaviors of AI systems understandable to humans. As AI becomes more complex (e.g., deep learning, ensemble models), understanding \"why\" a model made a certain decision is crucial for trust, accountability, safety, and fairness\n",
    "\n",
    "Purpose: User Trust, Debugging, Ethics, Regulatorics, Safety\n",
    "\n",
    "__Categories__\n",
    "1. Model-Specific vs Model-Agnostic\n",
    "    - Model-specific<br>require access to internal structure (e.g., decision trees, attention maps)\n",
    "    - Model-agnostic<br>work with any black-box model via inputs/outputs (e.g., LIME, SHAP, ALE)<br><br>\n",
    "2. Global vs Local Explanations\n",
    "    - Global: Explain the overall model behavior (e.g., feature importance, PDP, ALE)\n",
    "    - Local: Explain a single prediction (e.g., LIME, SHAP, counterfactuals, ICE)<br><br>\n",
    "3. Post-Hoc vs Intrinsic\n",
    "    - Post-hoc: Explanations are generated after model training\n",
    "    - Intrinsic: Models that are interpretable by design (e.g., linear models, decision trees)<br><br>\n",
    "\n",
    "\n",
    "\n",
    "|Technique| Description|Type|\n",
    "| :--------------------------------- |:--------------------------------------------------------------------------------- |:-------------- |\n",
    "| **Feature Importance**            | Ranks how much each feature contributes to predictions                            | Global         |\n",
    "| **Partial Dependence Plot (PDP)** | Shows average effect of a feature                                                 | Global         |\n",
    "| **ICE & d-ICE**                   | Individual Conditional Expectation — shows how prediction changes per observation | Local          |\n",
    "| **ALE**                           | Accumulated Local Effects — unbiased average effect in data-supported regions     | Global         |\n",
    "| **LIME**                          | Local interpretable model-agnostic explanations via surrogate models              | Local          |\n",
    "| **SHAP**                          | Shapley values for consistent, theoretically grounded feature attribution         | Local + Global |\n",
    "| **Anchors**                       | IF-THEN rules that \"anchor\" predictions with high precision                       | Local          |\n",
    "| **Counterfactuals**               | \"What would need to change for a different prediction?\"                           | Local          |\n",
    "| **Saliency maps, Attention**      | Visualization for interpreting deep learning models                               | Model-specific |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Prediction Anchors (2018)\n",
    "---\n",
    "[[paper]](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)<br>\n",
    "\n",
    "Idea: approximate decision boundaries by simple decision rules\n",
    "\n",
    "Anchor or Rule = a single partition of the feature space (for example, node of a decision tree) which is intened to be simple - with as few features as possible\n",
    "\n",
    "For a given observation:\n",
    "1. Find a set of candidate rules where target is stable<br>aka class distribution should be as non-uniform as possible\n",
    "2. Generate random changes to the observation by perturbing features not included in the rule \n",
    "3. Evaluate how stable is the answer<br>if target won't change for most cases - a rule is a good explanation\n",
    "\n",
    "Metrics<br>\n",
    "Precision = rate of observations where answer did not change<br>\n",
    "Coverage = rate of observations where rule applies\n",
    "\n",
    "# Counterfactual explanations\n",
    "---\n",
    "Counterfactual observations = synthetic data (usually close to the original) that exemplifies the shortest path to obtain the desired target<br>\n",
    "\n",
    "Example: your credit application was rejected. GDPR gives you right to  appeal or at least understand: \"What should I do to be approved?\"\n",
    "\n",
    "<img src=\"img/counterfactual.png\" width=500>\n",
    "\n",
    "# Watcher (2017)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/1711.00399)<br>A method to find the best counterfactual explanation: for a given observation $x$ find a minimal change $x' = x+\\delta$ that achieves the desired target. We find such data point by solving a minimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{x'} \\ \\lambda \\cdot \\text{loss}(f(x'), y_{\\text{target}}) + d(x, x')\n",
    "$$\n",
    "\n",
    "Here loss function = distance to the desired class $y_{target}$, for binary problem could be $\\{0,1\\}$ <br>d = distance from the data point $x$ <br>$\\lambda$ is a balance coefficient between proximity and validity\n",
    "\n",
    "# DiCE (2019)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/1905.07697)<br>\n",
    "Diverse Counterfactual Explanations: for a given observations find a set of minimal changes that would help obtain the desired target. Unlike giving one shortest change here we output a diverse list of possible changes\n",
    "\n",
    "We find such observations through solving optimization problem - they must be as close to the original as possible & a set must be as diverse as possible. And of course not going beyond the real distributions\n",
    "\n",
    "$$\n",
    "\\min_{\\{x_i'\\}} \\sum_{i=1}^{k} \\left[ \\lambda_1 \\cdot \\text{loss}(f(x_i'), y_{\\text{target}}) + \\lambda_2 \\cdot d(x_i', x) \\right] - \\lambda_3 \\cdot \\text{diversity}(\\{x_i'\\})\n",
    "$$\n",
    "\n",
    "# Contrastive Explanations Method (2018)\n",
    "[[paper]](https://arxiv.org/abs/1802.07623)<br>\n",
    "CEM is a method that seeks for two feature sets that for a given observation explain its target value most. First set = \"Pertinent Positives\", features that must be present to justify the model’s prediction. Second set = \"Pertinent Negatives\", \n",
    "features that must be absent to justify the same prediction\n",
    "\n",
    "For the PN\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\delta}} \\ c \\cdot \\max \\left\\{ 0, \\max_{i \\neq t} f(\\mathbf{x} + \\boldsymbol{\\delta})_i - f(\\mathbf{x} + \\boldsymbol{\\delta})_t + \\kappa \\right\\} + \\lambda_1 \\cdot \\|\\boldsymbol{\\delta}\\|_1 + \\lambda_2 \\cdot \\|\\boldsymbol{\\delta}\\|_2^2 + \\lambda_3 \\cdot \\|\\mathbf{x} + \\boldsymbol{\\delta} - AE(\\mathbf{x} + \\boldsymbol{\\delta})\\|_2^2\n",
    "$$\n",
    "\n",
    "Our perturbation $\\delta$ must change the current class $t$. So we model the distance to the closest other class as ($k$ plays a role of threshold):\n",
    "$$\\max \\left\\{ 0, f(\\mathbf{x} + \\boldsymbol{\\delta})_t - \\max_{i \\neq t} f(\\mathbf{x} + \\boldsymbol{\\delta})_i + \\kappa \\right\\}$$\n",
    "\n",
    "Next we add two elasticnet regularizers\n",
    "$$\\lambda_1 \\cdot \\|\\boldsymbol{\\delta}\\|_1 + \\lambda_2 \\cdot \\|\\boldsymbol{\\delta}\\|_2^2$$\n",
    "\n",
    "And add an Autoencoder to ensure that the new observation $x+\\delta$ is \"realistic\" enough. Idea is if it is \"realistic\" autoencoder will have no problem decoding it\n",
    "\n",
    "These two optimizations are typically solved via projected gradient descent with additional constraints (e.g., feature ranges, categorical masks)\n",
    "\n",
    "Finally the problem takes the form of:\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\delta}} \\ c \\cdot \\max \\left\\{ 0, f(\\mathbf{x} \\odot \\boldsymbol{\\delta})_t - \\max_{i \\neq t} f(\\mathbf{x} \\odot \\boldsymbol{\\delta})_i + \\kappa \\right\\} + \\lambda_1 \\cdot \\|\\boldsymbol{\\delta}\\|_1 + \\lambda_2 \\cdot \\|\\boldsymbol{\\delta}\\|_2^2 + \\lambda_3 \\cdot \\|\\mathbf{x} \\odot \\boldsymbol{\\delta} - AE(\\mathbf{x} \\odot \\boldsymbol{\\delta})\\|_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "# Individual Conditional Expectation (2014)\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/1309.6392)<br>\n",
    "ICE = Individual Conditional Expectation. For a selected observation $x$ and feature $f_i$ this plot depicts how target depends on possbile values of $f_i$ given that all other features are freezed (ceteri paribas). In other words we slice prediction function at some selected feature\n",
    "\n",
    "ICE is a model agnostic method, it works with Black-box models. We just need to know how to call it\n",
    "\n",
    "The name deconstruction:\n",
    "- \"Individual\" because we deal with a single feature at a time\n",
    "- \"Expectation\" because we investigate model predictions\n",
    "- \"Conditional\" because we fix all other features and possible feature value\n",
    "\n",
    "On most production plots many lines are plotted - each corresponds to a single observation in a dataset.\n",
    "The expectation of these lines (a bold line) gives a Partial Dependency plot - it represents the \"averaged\" (isolated) effect of the feature\n",
    "\n",
    "<img src=\"img/ice2.png\" width=1000>\n",
    "\n",
    "ICE shows how a single isolated feature affects the target. It helps to detect:  \n",
    "- effect magnitude<br>high variablity means high feature importance for a chosen target\n",
    "- shape of the effect<br>linear / non-linear\n",
    "- heterogeneity<br>feature affects target differently along other feature => there is some interaction between them\n",
    "\n",
    "<img src=\"img/ice_vs_pdp.png\" width=500>\n",
    "\n",
    "__Variants__\n",
    "- c-ICE = centered ICE<br>instead of ploting $f(x)$ plot a difference with original observartion<br><br>\n",
    "- d-ICE = derivative ICE<br>Instead of plotting $f$ plot its derivative $\\frac{\\partial{f}}{\\partial{x}}$ over the feature. It helps highlight areas of rapid prediction change. If it's not constant => there is some interaction with other features\n",
    "\n",
    "<img src=\"img/d-ice.png\" width=500>\n",
    "\n",
    "# Partial Dependency Plot (2005)\n",
    "---\n",
    "[[paper]]()<br>\n",
    "PDP = Partial Dependency Plot - a plot that shows how <u>an average</u> prediction depends on possible feature values $x_i$ given that other features are fixed\n",
    "\n",
    "PDP is just the ICE lines averaged along the analyzed feature\n",
    "\n",
    "<img src=\"img/pdp.png\" width=500>\n",
    "\n",
    "# ALE\n",
    "---\n",
    "[[paper]](https://arxiv.org/abs/1309.6392)<br>\n",
    "ALE = Accumulated Local Effect. It's a plot that shows prediction's local change: how it increases or decreases - given that all other features remain fixed, averaged over all available observations and depending on feature value $x$\n",
    "\n",
    "Averaged and Discretized derivative-ICE gives ALE\n",
    "\n",
    "Algorithm:\n",
    "- select a feature to analize\n",
    "- split feature values into $k$ discrete bins\n",
    "- calculate prediction change inside each bin $\\Delta = f(x_{left}) - f(x_{right})$\n",
    "- get the averaged difference over across all observations in the dataset\n",
    "- accumulate those deltas into a continuous plot (integrate from left to right)\n",
    "\n",
    "<img src=\"img/ale.png\" width=500>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
