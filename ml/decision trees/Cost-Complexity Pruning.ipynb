{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 ways to regularize Decision Trees:\n",
    "1. Pre-pruning: setting some stopping criteria before model fitting\n",
    "2. Post-pruning: build a full tree, analyze its structure and prune it to some reduced version\n",
    "\n",
    "Cost-complexity pruning (CCP) is a process of simplifying tree strcuture by optimizing the cost function enhanced by complexity regularizer.\n",
    "\n",
    "Let's denote\n",
    "- $T_n$ = subtree with a root in node n\n",
    "- $T - T_n$ = pruned tree (n becomes leaf node)\n",
    "- $R(T)$ = cost function of a tree\n",
    "- $|T|$ = number of leaves in a tree \n",
    "- $R(n)$= cost function of a node\n",
    "- $w(n)$ = size of a partition (node weight)\n",
    "- $p(n)$ = impurity measure in the node\n",
    "\n",
    "Cost function for a tree is calculated on its leaves:\n",
    "$$R(T) = \\sum_{n \\in leaves} R(n) = \\sum_{n \\in leaves} w(n) \\cdot p(n) $$\n",
    "\n",
    "If we add some complexity regularizer to cost function:\n",
    "$$R_\\alpha(T) = \\sum_{n \\in leaves} R(n) + \\alpha \\cdot |T|$$\n",
    "\n",
    "How does impurity change without $\\alpha$?\n",
    "- When we parition a node impurity goes down\n",
    "- When we prune a node impurity goes up\n",
    "\n",
    "If we add regularizer $\\alpha$ to impurity, this is not longer the case\n",
    "- if we use small $\\alpha$, the cost will still be higher after pruning  \n",
    "- if we use large $\\alpha$, the cost after pruning will be lower\n",
    "\n",
    "Let's compute the effect of pruning tree $T$ by removing a subtree $T_n$:\n",
    "\n",
    "$$R_\\Delta(n) = R_{\\alpha}(T-T_n) - R_{\\alpha}(T) $$\n",
    "\n",
    "Also plug-in complexity-regularizers so:\n",
    "$$R_\\Delta(n) = \\bigg( R(T-T_n) + \\alpha \\cdot |T-T_n| \\bigg) - \\bigg(R(T) - \\alpha \\cdot |T|\\bigg) $$\n",
    "\n",
    "Let's simplify this expression a bit.\n",
    "\n",
    "We removed $|T_n|$ leaves of a subtree $T_n$ and replaced it with 1 leaf node. So the change in <u>complexity</u> is gonna be $-(1 + |T_n|)$\n",
    "\n",
    "Using the same logic, change in <u>impurity</u> gonna be $R(T_n) - R(n)$.\n",
    "\n",
    "<img src=\"img/ccp1.png\" width=500>\n",
    "\n",
    "So the cost delta becomes:\n",
    "$$R_\\Delta(n) = R(T_n) - R(n) - \\alpha \\cdot (1 + |T_n|)$$\n",
    "\n",
    "The choice of $\\alpha$ defines whether the regulairzed cost increases or decreases for a candidate node.\n",
    "\n",
    "$$\\frac{R(T_n) - R(n)}{1 - |T|} \\vee 0$$\n",
    "\n",
    "We don't need to set $\\alpha$ beforehand. We can first build a sequence of pruned trees and then deduct optimal value.\n",
    "\n",
    "Pruning process:\n",
    "- for each candidate node compute a cost gap\n",
    "    $$g(n) = \\frac{R(T_n) - R(n)}{1 - |T|}$$\n",
    "- select a node with a lowest gap\n",
    "    $$n = argmin \\big( g(n_1), g(n_2) \\cdots g(n_t) \\big)$$\n",
    "- prune the tree up to that node and contuniue to iterate\n",
    "    $$T := T - T_n$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn [DecisionTreeClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn-tree-decisiontreeclassifier) class has some of the functionality described above\n",
    "- __ccp_alpha__ - to set the regularization strenght\n",
    "- __cost_complexity_pruning_path()__ to return impurities for each alpha in a sequence\n",
    "\n",
    "Note that alpha path here is computed on Train data, not Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "(alphas, impurities) = clf.cost_complexity_pruning_path(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this alpha path which gives us some understanding of the alpha scale:\n",
    "<img src=\"img/ccp2.png\" width=500>\n",
    "\n",
    "In order to evaluate pruned trees on Test data, we need to construct this sequence manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alphas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db52dc993a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alphas' is not defined"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "for alpha in alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we did that, we can evaluate those trees any way we want. Including finding optimal $\\alpha$ in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ccp3.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCP for Random Forests\n",
    "\n",
    "[paper](https://www.researchgate.net/publication/315116126_Cost-Complexity_Pruning_of_Random_Forests), 2017\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
