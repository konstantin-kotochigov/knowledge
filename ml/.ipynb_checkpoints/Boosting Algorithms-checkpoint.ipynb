{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ideas of PAC (probably approximately correct) learning (),  Shapire (AT&T) in 1989 came up with the first boosting algorithm. Later .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Stagewise Additive Modeling\n",
    "\n",
    "<img src=\"img/additive_model.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost described in 1995 by Schapire and Friedman. Being a meta algorithm, it can work with any type of \"weak\" classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T05:06:31.160457Z",
     "start_time": "2021-01-03T05:06:31.151702Z"
    }
   },
   "source": [
    "#### Algorithm in plain english\n",
    "\n",
    "0. Initialize weight for each case as $(\\frac{1}{n},\\frac{1}{n} \\cdots \\frac{1}{n})$\n",
    "\n",
    "While not convergence_criteria:\n",
    "1. Train a new \"weak\" classifier $h_{m}$ that minimizes the weighted loss function\n",
    "2. Update the weights in the dataset\n",
    "3. Renormalize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T05:37:28.937967Z",
     "start_time": "2021-01-03T05:37:28.928740Z"
    }
   },
   "source": [
    "### Important algoritm parameters\n",
    "- num_estimators\n",
    "- learning rate ($\\eta$) - a decreasing multiplier that shrinks the contribution ($\\alpha$) of each new weak classifier\n",
    "\n",
    "High learning rate helps fight overfitting.\n",
    "\n",
    "https://stats.stackexchange.com/questions/82323/shrinkage-parameter-in-adaboost/355632#355632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the decision function as an additive model:\n",
    "\n",
    "$$ F_m(x) = \\alpha_1 h_1(x) + \\alpha_2 h_2(x) + \\alpha_1 h_1(x) + \\cdots + \\alpha_{m-1} h_{m-1}(x) $$\n",
    "\n",
    "We can also rewrite it as an iterative process. Suppose we already have $F_{m-1}$:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\alpha_{m-1}h_{m-1}(x)$$\n",
    "\n",
    "The loss function for this decision function would be $$-y(x)F_m(x) = \n",
    "\\begin{cases}\n",
    "    -1,& \\text{when prediction is correct}\\\\\n",
    "    1,& \\text{when prediction is incorrect}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Loss function minimization = on each step we train a \"weak\" classifier $h_{m-1}(x)$ that minimizes the loss function.\n",
    "\n",
    "Let's rewrite the loss function as an exponent so that it would be easier to optimize\n",
    "\n",
    "$$L = \\sum e^{-y(x) F_m(x)}$$\n",
    "\n",
    "Plug in the decision function:\n",
    "$$L = \\sum e^{-yF_{m-1}(x) - \\alpha_{m-1}h_{m-1}}$$\n",
    "\n",
    "Let's factorize the exponent:\n",
    "$$L = \\sum e^{-yF_{m-1}(x)} e^{-y\\alpha_{m-1}h_{m-1}}$$\n",
    "\n",
    "Let's denote $e^{yF_{m-1}}$ as weight $w^m$;\n",
    "$$L = \\sum w^m e^{-y\\alpha_{m-1}h_{m-1}} $$ \n",
    "\n",
    "We notice that $yh = 1$ for correct classifications and $yh = -1$ for incorrect classifications. So we rewrite:\n",
    "\n",
    "$$L = \\sum_{correct} w^m e^{-\\alpha_m} + \\sum_{incorrect} w^m e^{\\alpha_m}$$\n",
    "\n",
    "$$L = \\sum_{all} \\big( w^m e^{-\\alpha_m} \\big) + \\sum_{incorrect} {\\big(w^m e^{\\alpha_m} - w^m e^{-\\alpha_m}\\big)}$$\n",
    "\n",
    "The alpha that minimizes the loss function is:\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = \\frac{1}{2} \\log{ \\frac{\\sum_{correct}w^m}{\\sum_{incorrect}w^m}} = \\frac{1}{2} \\log{\\frac{1-\\epsilon}{\\epsilon}}\n",
    "$$\n",
    "\n",
    "So, the resulting prediction can be achieved using a Weighted Voting with weights $\\alpha=(\\alpha_1,\\alpha_2 \\cdots \\alpha_m)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Stump (Decision Stump) = 1-level decision Tree.\n",
    "\n",
    "#### SAMME (Mutli-class Adaboost)\n",
    "\n",
    "[paper](https://web.stanford.edu/~hastie/Papers/samme.pdf)\n",
    "\n",
    "SAMME = **S**tagewise **A**dditive **M**odeling with **M**ulticlass **E**xponential loss function\n",
    "\n",
    "SAMME is an extension of Adaboost for multiple classes.\n",
    "\n",
    "There are two flavours of the algorithm: SAMME (for discrete estimators) and SAMME.R (for real estimators)\n",
    "\n",
    "SAMME\n",
    "\n",
    "\n",
    "\n",
    "When K = 2 algorithm is the same as Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn\n",
    "\n",
    "There are 3 versions:\n",
    "- AdaBoost\n",
    "- GradientBoosting\n",
    "- HistogramBoosting\n",
    "\n",
    "In Sklearn AdaBoost there is also Feature-importance calculation performed using permutation-based approach.\n",
    "\n",
    "### AdaBoost classifier\n",
    "Allows to use arbitrary classifier. Default = DecisionTreeClassifier with max_depth=1.\n",
    "\n",
    "Algorithm Parameters:\n",
    "- weak estimator (default = decision stump)\n",
    "- n_estimators\n",
    "- learning_rate\n",
    "\n",
    "You can call <u>staged_predict_proba()</u> which returns probs for each of the estimators\n",
    "\n",
    "### GradientBoostoingClassifier\n",
    "\n",
    "### HistogramBoostingClassifier\n",
    "\n",
    "Inspired by lightGBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightGBM\n",
    "\n",
    "Microsoft implementation of GBM\n",
    "\n",
    "https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
