{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном туториале рассмотрены подходы к определению аномалий в данных. Аномалии - достаточно широкое понятие, в простейшем случае мы будем говорить про точки, значение которых отличется от распределения значений большинства точек. Такие точки называются __outliers__ (выбросы).\n",
    "\n",
    "Иногда используеется термин __novelty detection__, например, в задачах детектирования и прогнозирования поломок (predictive maintenance). Он указывает на то, что анализируемое наблюдение не вписывается в типовой паттерн поведения системы и описывает некое \"новое\" состояние.\n",
    "\n",
    "Ипользуются в задачах, где важно детектировать некое редкое событие (например, поломку механизма или мошенническую транзакцию), но статистики для supervised подходов не достаточно.\n",
    "\n",
    "Итого, зачем может понадобиться:\n",
    "1. Data quality assurance<br>починить кривые данные, здесь аномалия = плохо\n",
    "2. Rare class detection<br>детектировать поломку в системе, здесь аномалия = хорошо\n",
    "\n",
    "#### Сложности\n",
    "- нет единого критерия аномальности, он выбирается каждый раз под задачу\n",
    "- не всегда анализируемый сигнал представлен в виде точек, часто это более сложные паттерны наблюдений<br> (напирмер графы, изображения или другой распространенный пример - временные ряды)\n",
    "- кажущиеся аномальными наблюдения могут легко объясняться неучитываемыми факторами<br> (например, сезонность часто вносит свой вклад)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate outlier detection\n",
    "\n",
    "По данным строим распределение каждой фичи и смотрим, что там на хвостах этого распределения. \n",
    "\n",
    "Например, видим, что 99% значений признака находится в интервале [0,10]. Всё, что вне - кандидаты на аномальность.\n",
    "\n",
    "Минусы подхода\n",
    "- аномальное значение может быть следствием наблюдения другого признака. По-хорошему, надо учитывать все остальные признаки тоже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гаусовы эллипсы\n",
    "\n",
    "Базовый многомерный подход - описать данные Гаусовым распределением и найти точки из его хвостов.\n",
    "\n",
    "Пусть данные имеют <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\">Гауссово распределение</a> с неизвестными нам вектором средних $\\mu$ и матрицей ковариаций $\\Sigma$\n",
    "<img src=\"img/gaussian1.png\" width=500>\n",
    "\n",
    "Шаги:\n",
    "\n",
    "1. По доступной нам выборке делаем эмпирическую оценку параметров $\\mu$ (координаты центра) и $\\Sigma$ (ковариации признаков)<br>Для того используем <a href=\"https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BF%D1%80%D0%B0%D0%B2%D0%B4%D0%BE%D0%BF%D0%BE%D0%B4%D0%BE%D0%B1%D0%B8%D1%8F\">метод максимальногоо правдоподобия</a>\n",
    "    \n",
    "    * суть метода максимального правдоподобия = подбираем параметры распределения так, чтобы наблюдаемое облако точек лучше всего укладывадлось в это распределение (то есть вероятность наблюдать эти точки при условии выбора данного распредления, была маскимальной)\n",
    "\n",
    "\n",
    "2. Рассчитываем Махалонобисово расстояние от центра $\\mu$ до всех точек \n",
    "\n",
    "    $d_{(\\mu,\\Sigma)}(x_i)^2 = (x_i - \\mu)'\\Sigma^{-1}(x_i - \\mu)$\n",
    "    \n",
    "    * <a href=\"https://en.wikipedia.org/wiki/Mahalanobis_distance\">Махаланобисово расстояние</a> - это метрика, обобщающая классическое Евклидово расстояние меджду двумя точками, так чтобы учитывались ковариации между признаками. \n",
    "    \n",
    "    На интуитивном уровне - при наличии корреляций пространство как бы сжимается пропорционально линиям уровня распределения (см картинку выше) и малое расстояние между точками может быть просто следствием такого сжатия. В этом случае чтобы посчитать честное расстояние, оно компенсируется пропорционально тому как именно пространство было сжато. Такое компенсированное расстояние и называется расстоянием Махалонобиса.\n",
    "\n",
    "\n",
    "3. Отсекаем все, что больше порогового квантиля (например в 97,5%) - это и есть выбросы\n",
    "\n",
    "Множество точек с махаланобисовым расстоянием равным пороговому значению - эллипс, отсюда название метода. Всё что за пределами границы считаем выбросами (outliers), все что внутри - нормальными точками (inliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что не так с этим подходом? \n",
    "\n",
    "1) Во-первых, редко когда данные можно описать одним гауссовым эллипсом. В этом случае луше использовать GMM (он описываемый далее в этом докуенмте).\n",
    "\n",
    "2) Во-вторых, выбросы сильно влияют на оценку $\\mu$  и $\\Sigma$. На картинке ниже сравнение двух эллипсов (эмпирического - синий и реального - красный). \n",
    "\n",
    "Видим, что синий эллипс сильно \"притянут\" выбросами. В результате пропускается очень много аутлайеров.\n",
    "\n",
    "<img src=\"img/elliptic_empiric.png\" width=400>\n",
    "\n",
    "### Робастные эллипсы\n",
    "\n",
    "В математике **робастной** называют оценку какого-либо параметра, не подверженную влиянию случайных выбросов.\n",
    "\n",
    "Существует более робастная оценка матрицы ковариаций: MinCovDet (Min Covariance Determinant).\n",
    "\n",
    "\n",
    "Принцип построения: \n",
    "1. выбираем параметр h\n",
    "2. ищем подмножество точек объемом h, на котором определитель матрицы ковариаций минмален\n",
    "3. ищем на этом подмножестве оценки параметров $\\mu$ и $\\Sigma$ методом маскимального правдоподобия - это и есть наши робастные оценки\n",
    "\n",
    "Ниже сравнение ситуации с обычной оценкой ковариации (слева) и робастной (справа). По оси X отложены точки выборки, по оси Y - их махалонобисовы расстояния. Красная линия  - 97.5-процентный уровень эллипса.\n",
    "<img src=\"img/elliptic_comparison.png\" width=600>\n",
    "\n",
    "Видим, что при использовании робастной оценки, махаланобисовы расстояния выбросов стали больше => выбросы лучше отделяются.\n",
    "\n",
    "Для реализации нужен полный перебор всех подмножеств: $\\binom{n}{h}$ => считается долго. Но в 1999 году появился feasible вариант алгоритма (fastMCD) и его стали использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "#### Постановка\n",
    "\n",
    "GMM (gaussian mixture model) - вариация на тему предыдущего подхода с моделированием выборки нормальным распредеднием, только здесь данные моделируются сразу комбинацией нескольких гауссовых распределений, которые называются компонентами смеси.\n",
    "\n",
    "<img src=\"img/gmm.png\" width=400>\n",
    "\n",
    "Такая модель позволяет уйти от требуемой симметричности данных и моделировать более сложные выборки.\n",
    "\n",
    "#### Параметры\n",
    "\n",
    "Количество компонентов в смеси (k) является гиперпараметром метода и задается заранее.\n",
    "\n",
    "Внутренними параметрами модели являются пары location-параметров нормального распределения (среднее $\\mu$ и стандартное отклонение $\\sigma$) для каждого из k элементов смеси:\n",
    "\n",
    "$$ \\Theta = \\big\\{ (\\mu_1,\\sigma_1), (\\mu_2,\\sigma_2) \\cdots (\\mu_k,\\sigma_k) \\big\\}$$\n",
    "\n",
    "#### Обучение\n",
    "\n",
    "Параметры подбираются путем максимизации функции правдоподобия:\n",
    "\n",
    "$$\\hat{\\Theta}_{opt} = argmin \\big(log(L)\\big) = argmin \\big(log(P(X|\\Theta))\\big)$$\n",
    "\n",
    "Аналитического решения у задачи не существует, поэтому применяется типичный для подобного рода задач итеративный Expectation-Maximization алгоритм для поиска максимума функции аправдоподобия. Описание EM-алгоритма выходит за рамки данного туториала.\n",
    "\n",
    "Также существует еще один альтернативный подход к определению параметров модели - использование баесовской логики (variational bayesian inference), когда максимизируется не правдоподоие выборки (likelihood), а ее апостреиорная веротяность (см. теоркму Байеса). В этом случае необходимо также задать априорное распределение компонентов смеси, модель будет его учитывать.\n",
    "\n",
    "#### Минусы\n",
    "Несмотря на бОльшую гибкость, чем моделирование одной гауссианой, GMM может эффективно моделировать только данные, имеющие гауссову природу, то есть когда основная масса сосредоточена в центре выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Outlier Factor\n",
    "\n",
    "Алгоритм LOF (2000) относится к классу плотностных (density based) методов опрдедления аномалий.\n",
    "\n",
    "**Идея**: если \"плотность\" выборки в районе точки небольшая, то скорее всего это выброс. \n",
    "\n",
    "Что значит \"плотность\"?\n",
    "    \n",
    "    Плотность считается как 1 / среднее расстояние до k-ближайших соседей. \n",
    "\n",
    "Что значит \"небольшая\"? \n",
    "\n",
    "    Значит, что плотность точки сильно меньше, чем плотность её соседей. \n",
    "    \n",
    "Показатель LOF = отношение средней плотности соседей к плотности точки. Пример значений LOF для разных точек ниже.\n",
    "\n",
    "<img src=\"img/lof.png\" width=400>\n",
    "\n",
    "\n",
    "\n",
    "Небольшой нюанс в том, что в расстояния считается не Евклидово, а Reachibility Distance - оно дополнительно ограничивается снизу. Иллюстраиця ниже - расстояние от C до A равно расстоянию от B до A, а вот расстояние от D до A больше.\n",
    "<img src=\"img/lof_dist.png\" width=150>\n",
    "\n",
    "Сделано (как я понял) для устойчивости расчета LOF (чтобы много маленьких расстояний не влияли на среднее значение).\n",
    "\n",
    "Можно заметить, что в этом методе исп тот же концепт, что и в плотностном алгоритме кластеризации DBSCAN.\n",
    "\n",
    "Объяснение для чайников:\n",
    "https://blog.stealthbits.com/local-outlier-factor-part-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class SVM\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest\n",
    "\n",
    "[[paper]](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf), 2008\n",
    "\n",
    "Алгоритм выглядит так:\n",
    "1. Строится ансамбль \"случайных\" деревьев. Что значит \"случайное\" дерево:\n",
    "    - на каждом шаге построения дерева признак, по которому делается разделение, выбирается случайно\n",
    "    - значение, по которому делается разбиение также выбирается случайно\n",
    "2. По каждой точке определяем, в какой лист дерева эта точка попадает\n",
    "3. Глубина листа усредняется по всем деревьям и получаем метрику \"нормальности\"\n",
    "\n",
    "Чем короче в среднем путь до разбиения, в которое попадает точка, тем с большей вероятностью это выброс\n",
    "\n",
    "<img src=\"img/if.png\" width=600>\n",
    "\n",
    "__References__\n",
    "- деревья = https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "- ансамбли = <a href=\"https://scikit-learn.org/stable/modules/ensemble.html\">https://scikit-learn.org/stable/modules/ensemble.html</a>\n",
    "- isolation forest = https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angle-based Outlier Detection\n",
    "\n",
    "[[paper]](https://www.dbs.ifi.lmu.de/~zimek/publications/KDD2008/KDD08-ABOD.pdf), KDD, 2008\n",
    "\n",
    "### Idea \n",
    "The point is probably an outlier if the spread of angles between paths to all other points is relatively small\n",
    "\n",
    "<img src=\"img/abod_algorithm.png\" width=550>\n",
    "\n",
    "- Angle is coded with a modified cosine distance between two vectors\n",
    "- \"relatively small\" = <u>variance</u> of angles is low\n",
    "\n",
    "If we plot the variances (the spreads) for all points, the points with low variance (close to 0.0) will denote outliers\n",
    "<img src=\"img/abod_variance.png\" width=300>\n",
    "\n",
    "\n",
    "### More formally\n",
    "\n",
    "The outlier measure is Variance of angles:\n",
    "\n",
    "$$ABOD(x_i) = Var(A_i) = E[A_i^2] - (E[A_i])^2$$\n",
    "\n",
    "\n",
    "Here E = average, weighted by the inverse distance\n",
    "\n",
    "$$E[A_i] = \\frac{\\sum_{i != j} w_{ij}A_{i,j}}{\\sum_{i != j} w_{ij}}$$\n",
    "    \n",
    "where\n",
    "    \n",
    "$$w_{ij} = \\frac{1}{\\left\\lVert AB \\right\\rVert \\cdot \\left\\lVert AC \\right\\rVert}$$\n",
    "\n",
    "Thus, more weight is given to closer points. This is done to make sure that the <u>local density</u> of the dataset is taken into account not just its global structure.\n",
    "\n",
    "Proxy for an angle is cosine similarity normalized by squared distance\n",
    "\n",
    "$$A = \\frac{(\\vec{AB} \\cdot \\vec{AC})}{{\\left\\lVert AB \\right\\rVert}^ 2 \\cdot {\\left\\lVert AC \\right\\rVert}^2}$$\n",
    "\n",
    "\n",
    "Let's write down the full expression:\n",
    "    \n",
    "<img src=\"img/abod.png\" width=500>\n",
    "\n",
    "### Highlights\n",
    "- algorithm always labels the points located on the edge of the dataset as outliers\n",
    "- takes local desity into account by weighting the angle with distance\n",
    "- but fails to detect outliers in symmteric environment\n",
    "- computationaly expensive, requires $O(n^3)$ operations\n",
    "\n",
    "### Variations\n",
    "- __FastABOD__ - most of the points will yield zero weights because of the high distance and thus high denominator, so it makes sense to take only nearest neighbors into calculation when computing variances\n",
    "- __LB-ABOD__ - they propose a multi-step process for finding top-l outliers\n",
    "    1. for each data point compute its variance lower bound using approximate approach (FastABOD)\n",
    "    2. store all data points in ascending order of their variance\n",
    "    3. loop over those data points:\n",
    "        \n",
    "        2.1 compute precise variance\n",
    "        \n",
    "        2.2 when we encounter a new minumum update the result set\n",
    "        \n",
    "        2.3 new minumums will become more and more rare, since LD is increasing\n",
    "    \n",
    "    3. Stop when we encounter LB larger then largest variance in the result set\n",
    "- __Kernel-ABOD__ - since method uses scalar product, it's straightforward to plug-in different non-linear kernels\n",
    "\n",
    "### Applications\n",
    "[[Yandex Video]](https://www.youtube.com/watch?v=aBckDgtG0Zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение подходов\n",
    "\n",
    "<img src=\"img/outlier_comparison.png\" width=500></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GasussianMixture\n",
    "gmm = GaussianMixture(n_components = 3, covariance_type='full')\n",
    "gmm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы оценить удаленность точки от центра своего распределения, можем посчитать вер-ть методом score_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_dist = gmm.score_samples(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ellipse Envelope in scikit-learn\n",
    "\n",
    "В sklearn обычная оценка матрицы ковариаций считается через covariance.EmpiricalCovariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EmpiricalCovariance\n",
    "cov = EmpiricalCovariance().fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Робастная оценка считается covariance.MinCovDet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import MinCovDet\n",
    "mcd = MinCovDet().fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этих же классах есть метод для расчета Махаланобисова расстояния при раасчитанных ($\\mu$,$\\Sigma$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.mahalanobis(X_test)\n",
    "mcd.mahalanobis(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.error_norm(another_cov) - норма разницы с другой матрицей ковариаций\n",
    "(норма Фробениуса - сумма квадратов элементов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Outlier Factor in scikit-learn\n",
    "\n",
    "В Sklearn для LOF есть класс LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "clf = LocalOutlierFactor(n_neighbors=20)\n",
    "y_pred = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы:\n",
    "- fit - считается \"нестандартность\" каждой точки (у каждого алгоритма метрика соотвественно своя)\n",
    "- predict - с помощью порогового значения каждая точка помечаются как выброс или нет\n",
    "\n",
    "Метод predict() возвращает значение :\n",
    "- 1, если точка не считается выбросом\n",
    "- -1, если точка прогнозируется выбросом\n",
    "\n",
    "Параметры обучения:\n",
    "- contamination_factor - верхний квантиль расстояния, в котором точка считается выбросом (обычно 10%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors=20, n_jobs=16)\n",
    "y_pred = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Материалы\n",
    "\n",
    "LOF:\n",
    "https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf\n",
    "\n",
    "\n",
    "Isolation forest:\n",
    "https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest\n",
    "\n",
    "One-class SVM:\n",
    "http://www.jmlr.org/papers/volume2/manevitz01a/manevitz01a.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
