{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RankNet\n",
    "\n",
    "List is perfectly ranked if for any pair of documents the score of the more relevant is larger. We will try to force our ML model to make correct comparisons for any given pair of documents.\n",
    "\n",
    "Feed arbitrary pairs (all combinations or a subset) to model, get scores, compute loss function and update it using loss function gradients.\n",
    "\n",
    "We model comparison outcome probability as a sigmoid which depends on two scores\n",
    "\n",
    "We use cross-entropy as a loss function for our P predictions.\n",
    "\n",
    "Itteratively update weights using calculated gradients\n",
    "\n",
    "### Maths\n",
    "\n",
    "Derivatives\n",
    "\n",
    "Let's consider a loss function of one comparison between $u_i$ and $u_j$. Cost is a function of two variables\n",
    "\n",
    "$$C=C(s(i),s(j))$$\n",
    "\n",
    "Hence we can unroll its derivative as:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_k} = \\frac{\\partial C}{\\partial s_i} \\cdot  \\frac{\\partial s_i}{\\partial w_k} + \\frac{\\partial C}{\\partial s_j} \\cdot \\frac{\\partial s_j}{\\partial w_k}$$\n",
    "\n",
    "<img src=\"img/first_derivative.png\">\n",
    "\n",
    "\n",
    "\n",
    "References:\n",
    "- <a href=\"https://www.researchgate.net/publication/221345726_Learning_to_Rank_using_Gradient_Descent\">Learning_to_Rank_using_Gradient_Descent</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
